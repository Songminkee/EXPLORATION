{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, precision_recall_curve, average_precision_score\n",
    "from scipy.interpolate import interp1d\n",
    "from inspect import signature\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from scipy.stats import norm\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "train_data = np.float32((train_data - 127.5) / 127.5)\n",
    "test_data = np.float32((test_data - 127.5) / 127.5)\n",
    "\n",
    "from PIL import Image\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(train_data[i])\n",
    "    plt.title(f'index: {i}')\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_labels(labels):\n",
    "    new_t_labels = []\n",
    "    for old_label in labels:\n",
    "        if old_label == 6:   \n",
    "            new_t_labels.append([0])  \n",
    "        else:\n",
    "            new_t_labels.append([1]) \n",
    "             \n",
    "    return np.array(new_t_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bol_train_labels = set_labels(train_labels)\n",
    "bol_test_labels = set_labels(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = []\n",
    "normal_labels = []\n",
    "anomaly_data = []\n",
    "anomaly_labels = []\n",
    "for data, label in zip(train_data, bol_train_labels):\n",
    "    if label == 0:\n",
    "        anomaly_data.append(data)\n",
    "        anomaly_labels.append(label)\n",
    "    else:\n",
    "        normal_data.append(data)\n",
    "        normal_labels.append(label)\n",
    "        \n",
    "normal_data = np.array(normal_data)\n",
    "normal_labels = np.array(normal_labels)\n",
    "anomaly_data = np.array(anomaly_data)\n",
    "anomaly_labels = np.array(anomaly_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 3) (45000, 1)\n",
      "(5000, 32, 32, 3) (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(normal_data.shape, normal_labels.shape)\n",
    "print(anomaly_data.shape, anomaly_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = normal_data\n",
    "bol_train_labels = normal_labels\n",
    "test_data = tf.concat([test_data, anomaly_data], 0)\n",
    "bol_test_labels = tf.concat([bol_test_labels, anomaly_labels], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 3)\n",
      "(15000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 1)\n",
      "(15000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(bol_train_labels.shape)\n",
    "print(bol_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "for label in bol_train_labels:\n",
    "    if label == 0:\n",
    "        print(label)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, bol_train_labels))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, bol_test_labels))\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]], shape=(8, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for data, label in train_dataset.take(1):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]], shape=(8, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for data, label in test_dataset.take(1):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.conv_layer = tf.keras.Sequential([\n",
    "            layers.Conv2D(num_filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                          kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        outputs = self.conv_layer(inputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_T_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_T_block, self).__init__()\n",
    "        self.conv_T_layer = tf.keras.Sequential([\n",
    "            layers.Conv2DTranspose(num_filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                                   kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, concat, training=False):\n",
    "        upsample = self.conv_T_layer(inputs)\n",
    "        outputs = tf.concat([upsample, concat], -1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, num_output_channel=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder_1 = Conv_block(64) # 16\n",
    "        self.encoder_2 = Conv_block(128) # 8\n",
    "        self.encoder_3 = Conv_block(256) # 4\n",
    "        self.encoder_4 = Conv_block(512) # 2\n",
    "        \n",
    "        self.center = Conv_block(512) # 1\n",
    "        \n",
    "        self.decoder_4 = Conv_T_block(512) # 2\n",
    "        self.decoder_3 = Conv_T_block(256) # 4\n",
    "        self.decoder_2 = Conv_T_block(128) # 8\n",
    "        self.decoder_1 = Conv_T_block(64) # 16\n",
    "        \n",
    "        self.output_layer = layers.Conv2DTranspose(num_output_channel, 1, strides=2, padding='same', use_bias=False, # 32\n",
    "                                                   kernel_initializer=tf.random_normal_initializer(0., 0.02))\n",
    "                \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 = self.encoder_1(inputs) # gen\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        de_4 = self.decoder_4(center, en_4)\n",
    "        de_3 = self.decoder_3(de_4, en_3)\n",
    "        de_2 = self.decoder_2(de_3, en_2)\n",
    "        de_1 = self.decoder_1(de_2, en_1)\n",
    "        \n",
    "        outputs = self.output_layer(de_1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.encoder_1 = Conv_block(64) # 16\n",
    "        self.encoder_2 = Conv_block(128) # 8\n",
    "        self.encoder_3 = Conv_block(256) # 4\n",
    "        self.encoder_4 = Conv_block(512) # 2\n",
    "        \n",
    "        self.center = Conv_block(100) # 1\n",
    "        \n",
    "        self.outputs = layers.Conv2D(1, 3, strides=1, padding='same',\n",
    "                                          use_bias=False, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 = self.encoder_1(inputs) # dis\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        outputs = self.outputs(center)\n",
    "        \n",
    "        return outputs, center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(num_output_channel=3)  # Generator가 32X32X1 짜리 이미지를 생성해야 합니다. \n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_loss = tf.keras.losses.MeanSquaredError()\n",
    "l1_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(pred_real, pred_fake):\n",
    "    real_loss = cross_entropy(tf.ones_like(pred_real), pred_real)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(pred_fake), pred_fake)\n",
    "    \n",
    "    total_dis_loss = (real_loss + fake_loss) * 0.5\n",
    "    \n",
    "    return total_dis_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(real_output, fake_output, input_data, gen_data, latent_first, latent_sec):\n",
    "    w_adv = 1.\n",
    "    w_context = 40.\n",
    "    w_encoder = 1.\n",
    "    \n",
    "    adv_loss = cross_entropy(real_output, fake_output)\n",
    "    context_loss = l1_loss(input_data, gen_data)\n",
    "    encoder_loss = l2_loss(latent_first, latent_sec)\n",
    "    \n",
    "    total_gen_loss = w_adv * adv_loss + \\\n",
    "                     w_context * context_loss + \\\n",
    "                     w_encoder * encoder_loss\n",
    "    \n",
    "    return total_gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer 설정\n",
    "generator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(images, training=True)\n",
    "        \n",
    "        pred_real, feat_real = discriminator(images, training=True)\n",
    "        pred_fake, feat_fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(pred_real, pred_fake,\n",
    "                                  images, generated_images,\n",
    "                                  feat_real, feat_fake)\n",
    "\n",
    "        disc_loss = discriminator_loss(pred_real, pred_fake)        \n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(os.getenv('HOME'),'aiffel/ganomaly_skip_no_norm/ckpt')\n",
    "\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 100, \t Total Gen Loss : 13.746088981628418, \t Total Dis Loss : 0.6942284107208252\n",
      "Steps : 200, \t Total Gen Loss : 15.437246322631836, \t Total Dis Loss : 0.606779932975769\n",
      "Steps : 300, \t Total Gen Loss : 14.291644096374512, \t Total Dis Loss : 0.4224868714809418\n",
      "Steps : 400, \t Total Gen Loss : 18.559110641479492, \t Total Dis Loss : 0.3569354712963104\n",
      "Steps : 500, \t Total Gen Loss : 17.265804290771484, \t Total Dis Loss : 0.37915724515914917\n",
      "Steps : 600, \t Total Gen Loss : 20.16524314880371, \t Total Dis Loss : 0.04737061262130737\n",
      "Steps : 700, \t Total Gen Loss : 17.652210235595703, \t Total Dis Loss : 0.0325196273624897\n",
      "Steps : 800, \t Total Gen Loss : 18.10719871520996, \t Total Dis Loss : 0.40460991859436035\n",
      "Steps : 900, \t Total Gen Loss : 17.344730377197266, \t Total Dis Loss : 0.050973616540431976\n",
      "Steps : 1000, \t Total Gen Loss : 22.611167907714844, \t Total Dis Loss : 0.15796604752540588\n",
      "Steps : 1100, \t Total Gen Loss : 19.7985782623291, \t Total Dis Loss : 0.0850161612033844\n",
      "Steps : 1200, \t Total Gen Loss : 20.05817222595215, \t Total Dis Loss : 0.034059882164001465\n",
      "Steps : 1300, \t Total Gen Loss : 19.665966033935547, \t Total Dis Loss : 0.02006792649626732\n",
      "Steps : 1400, \t Total Gen Loss : 17.207351684570312, \t Total Dis Loss : 0.27840784192085266\n",
      "Steps : 1500, \t Total Gen Loss : 21.955322265625, \t Total Dis Loss : 0.034764766693115234\n",
      "Steps : 1600, \t Total Gen Loss : 20.363386154174805, \t Total Dis Loss : 0.023747410625219345\n",
      "Steps : 1700, \t Total Gen Loss : 21.350547790527344, \t Total Dis Loss : 0.01854376122355461\n",
      "Steps : 1800, \t Total Gen Loss : 21.536436080932617, \t Total Dis Loss : 0.02722594328224659\n",
      "Steps : 1900, \t Total Gen Loss : 20.505992889404297, \t Total Dis Loss : 0.0290986318141222\n",
      "Steps : 2000, \t Total Gen Loss : 18.843639373779297, \t Total Dis Loss : 0.022020108997821808\n",
      "Steps : 2100, \t Total Gen Loss : 20.091028213500977, \t Total Dis Loss : 0.009476814419031143\n",
      "Steps : 2200, \t Total Gen Loss : 20.1654052734375, \t Total Dis Loss : 0.03822259232401848\n",
      "Steps : 2300, \t Total Gen Loss : 24.41233253479004, \t Total Dis Loss : 0.011045997962355614\n",
      "Steps : 2400, \t Total Gen Loss : 22.57549285888672, \t Total Dis Loss : 0.005695770028978586\n",
      "Steps : 2500, \t Total Gen Loss : 18.853879928588867, \t Total Dis Loss : 0.02779865264892578\n",
      "Steps : 2600, \t Total Gen Loss : 20.50697898864746, \t Total Dis Loss : 0.00519180204719305\n",
      "Steps : 2700, \t Total Gen Loss : 19.34628677368164, \t Total Dis Loss : 0.03420273959636688\n",
      "Steps : 2800, \t Total Gen Loss : 19.92403221130371, \t Total Dis Loss : 0.026408588513731956\n",
      "Steps : 2900, \t Total Gen Loss : 23.051280975341797, \t Total Dis Loss : 0.07263446599245071\n",
      "Steps : 3000, \t Total Gen Loss : 20.199134826660156, \t Total Dis Loss : 0.02627553790807724\n",
      "Steps : 3100, \t Total Gen Loss : 20.73442268371582, \t Total Dis Loss : 0.011672385036945343\n",
      "Steps : 3200, \t Total Gen Loss : 22.402498245239258, \t Total Dis Loss : 0.004652698524296284\n",
      "Steps : 3300, \t Total Gen Loss : 20.669628143310547, \t Total Dis Loss : 0.012582758441567421\n",
      "Steps : 3400, \t Total Gen Loss : 24.580169677734375, \t Total Dis Loss : 0.02054436132311821\n",
      "Steps : 3500, \t Total Gen Loss : 19.483983993530273, \t Total Dis Loss : 0.009192589670419693\n",
      "Steps : 3600, \t Total Gen Loss : 19.596773147583008, \t Total Dis Loss : 0.01479516550898552\n",
      "Steps : 3700, \t Total Gen Loss : 16.205514907836914, \t Total Dis Loss : 0.042528629302978516\n",
      "Steps : 3800, \t Total Gen Loss : 19.318395614624023, \t Total Dis Loss : 0.0694032534956932\n",
      "Steps : 3900, \t Total Gen Loss : 23.372228622436523, \t Total Dis Loss : 0.016341276466846466\n",
      "Steps : 4000, \t Total Gen Loss : 19.739782333374023, \t Total Dis Loss : 0.01258299220353365\n",
      "Steps : 4100, \t Total Gen Loss : 22.590770721435547, \t Total Dis Loss : 0.005154579412192106\n",
      "Steps : 4200, \t Total Gen Loss : 19.566225051879883, \t Total Dis Loss : 0.035304874181747437\n",
      "Steps : 4300, \t Total Gen Loss : 22.80967140197754, \t Total Dis Loss : 0.0034824847243726254\n",
      "Steps : 4400, \t Total Gen Loss : 26.130565643310547, \t Total Dis Loss : 0.0021870662458240986\n",
      "Steps : 4500, \t Total Gen Loss : 21.397993087768555, \t Total Dis Loss : 0.1081867516040802\n",
      "Steps : 4600, \t Total Gen Loss : 22.904451370239258, \t Total Dis Loss : 0.004251701757311821\n",
      "Steps : 4700, \t Total Gen Loss : 21.727907180786133, \t Total Dis Loss : 0.0029612842481583357\n",
      "Steps : 4800, \t Total Gen Loss : 20.396282196044922, \t Total Dis Loss : 0.004927563481032848\n",
      "Steps : 4900, \t Total Gen Loss : 26.110918045043945, \t Total Dis Loss : 0.0032507223077118397\n",
      "Steps : 5000, \t Total Gen Loss : 21.734912872314453, \t Total Dis Loss : 0.022270139306783676\n",
      "Steps : 5100, \t Total Gen Loss : 23.1527156829834, \t Total Dis Loss : 0.005562818609178066\n",
      "Steps : 5200, \t Total Gen Loss : 23.854955673217773, \t Total Dis Loss : 0.02783665992319584\n",
      "Steps : 5300, \t Total Gen Loss : 22.607276916503906, \t Total Dis Loss : 0.009451078251004219\n",
      "Steps : 5400, \t Total Gen Loss : 23.9821720123291, \t Total Dis Loss : 0.009432935155928135\n",
      "Steps : 5500, \t Total Gen Loss : 20.734363555908203, \t Total Dis Loss : 0.010970978997647762\n",
      "Steps : 5600, \t Total Gen Loss : 21.588565826416016, \t Total Dis Loss : 0.005337805021554232\n",
      "Time for epoch 1 is 359.5138123035431 sec\n",
      "Steps : 5700, \t Total Gen Loss : 23.881513595581055, \t Total Dis Loss : 0.0015094756381586194\n",
      "Steps : 5800, \t Total Gen Loss : 22.19840431213379, \t Total Dis Loss : 0.0025871337857097387\n",
      "Steps : 5900, \t Total Gen Loss : 23.708520889282227, \t Total Dis Loss : 0.015341036021709442\n",
      "Steps : 6000, \t Total Gen Loss : 23.258174896240234, \t Total Dis Loss : 0.0027010736521333456\n",
      "Steps : 6100, \t Total Gen Loss : 23.811677932739258, \t Total Dis Loss : 0.0047429827973246574\n",
      "Steps : 6200, \t Total Gen Loss : 18.42047691345215, \t Total Dis Loss : 0.20710739493370056\n",
      "Steps : 6300, \t Total Gen Loss : 20.38186264038086, \t Total Dis Loss : 0.005574731156229973\n",
      "Steps : 6400, \t Total Gen Loss : 18.640365600585938, \t Total Dis Loss : 0.17279352247714996\n",
      "Steps : 6500, \t Total Gen Loss : 25.092958450317383, \t Total Dis Loss : 0.0023748199455440044\n",
      "Steps : 6600, \t Total Gen Loss : 22.303056716918945, \t Total Dis Loss : 0.00450044684112072\n",
      "Steps : 6700, \t Total Gen Loss : 21.683719635009766, \t Total Dis Loss : 0.012321283109486103\n",
      "Steps : 6800, \t Total Gen Loss : 24.490068435668945, \t Total Dis Loss : 0.0014587813057005405\n",
      "Steps : 6900, \t Total Gen Loss : 22.76777458190918, \t Total Dis Loss : 0.003356102854013443\n",
      "Steps : 7000, \t Total Gen Loss : 23.214832305908203, \t Total Dis Loss : 0.004469318315386772\n",
      "Steps : 7100, \t Total Gen Loss : 22.28030014038086, \t Total Dis Loss : 0.002763153985142708\n",
      "Steps : 7200, \t Total Gen Loss : 27.20416259765625, \t Total Dis Loss : 0.004314967431128025\n",
      "Steps : 7300, \t Total Gen Loss : 20.499004364013672, \t Total Dis Loss : 0.07287213951349258\n",
      "Steps : 7400, \t Total Gen Loss : 21.977418899536133, \t Total Dis Loss : 0.001206380082294345\n",
      "Steps : 7500, \t Total Gen Loss : 22.669172286987305, \t Total Dis Loss : 0.007786645088344812\n",
      "Steps : 7600, \t Total Gen Loss : 23.61869239807129, \t Total Dis Loss : 0.005744044668972492\n",
      "Steps : 7700, \t Total Gen Loss : 25.15543556213379, \t Total Dis Loss : 0.0013406705111265182\n",
      "Steps : 7800, \t Total Gen Loss : 19.995702743530273, \t Total Dis Loss : 0.0019034089054912329\n",
      "Steps : 7900, \t Total Gen Loss : 26.839923858642578, \t Total Dis Loss : 0.001152058131992817\n",
      "Steps : 8000, \t Total Gen Loss : 23.537534713745117, \t Total Dis Loss : 0.001295538735575974\n",
      "Steps : 8100, \t Total Gen Loss : 23.988040924072266, \t Total Dis Loss : 0.0011250138049945235\n",
      "Steps : 8200, \t Total Gen Loss : 26.017318725585938, \t Total Dis Loss : 0.003140445798635483\n",
      "Steps : 8300, \t Total Gen Loss : 21.794132232666016, \t Total Dis Loss : 0.008748047985136509\n",
      "Steps : 8400, \t Total Gen Loss : 22.61237335205078, \t Total Dis Loss : 0.007818469777703285\n",
      "Steps : 8500, \t Total Gen Loss : 21.114906311035156, \t Total Dis Loss : 0.0015676310285925865\n",
      "Steps : 8600, \t Total Gen Loss : 22.45010757446289, \t Total Dis Loss : 0.009199528954923153\n",
      "Steps : 8700, \t Total Gen Loss : 23.639074325561523, \t Total Dis Loss : 0.0025356735568493605\n",
      "Steps : 8800, \t Total Gen Loss : 23.20527458190918, \t Total Dis Loss : 0.0016328705241903663\n",
      "Steps : 8900, \t Total Gen Loss : 22.866031646728516, \t Total Dis Loss : 0.007296436466276646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 9000, \t Total Gen Loss : 20.558277130126953, \t Total Dis Loss : 0.006378630641847849\n",
      "Steps : 9100, \t Total Gen Loss : 23.461219787597656, \t Total Dis Loss : 0.004712411668151617\n",
      "Steps : 9200, \t Total Gen Loss : 22.250539779663086, \t Total Dis Loss : 0.007655436173081398\n",
      "Steps : 9300, \t Total Gen Loss : 23.267866134643555, \t Total Dis Loss : 0.0016494439914822578\n",
      "Steps : 9400, \t Total Gen Loss : 23.07807159423828, \t Total Dis Loss : 0.017534814774990082\n",
      "Steps : 9500, \t Total Gen Loss : 23.04981803894043, \t Total Dis Loss : 0.009268688037991524\n",
      "Steps : 9600, \t Total Gen Loss : 27.117456436157227, \t Total Dis Loss : 0.0025168824940919876\n",
      "Steps : 9700, \t Total Gen Loss : 21.942214965820312, \t Total Dis Loss : 0.0054158903658390045\n",
      "Steps : 9800, \t Total Gen Loss : 24.465974807739258, \t Total Dis Loss : 0.005236579570919275\n",
      "Steps : 9900, \t Total Gen Loss : 23.179306030273438, \t Total Dis Loss : 0.0051703439094126225\n",
      "Steps : 10000, \t Total Gen Loss : 25.44211196899414, \t Total Dis Loss : 0.002963114297017455\n",
      "Steps : 10100, \t Total Gen Loss : 22.209774017333984, \t Total Dis Loss : 0.004632126539945602\n",
      "Steps : 10200, \t Total Gen Loss : 25.8723087310791, \t Total Dis Loss : 0.00191440898925066\n",
      "Steps : 10300, \t Total Gen Loss : 21.469600677490234, \t Total Dis Loss : 0.01711304299533367\n",
      "Steps : 10400, \t Total Gen Loss : 24.89942169189453, \t Total Dis Loss : 0.0019109200220555067\n",
      "Steps : 10500, \t Total Gen Loss : 21.62849235534668, \t Total Dis Loss : 0.002792719751596451\n",
      "Steps : 10600, \t Total Gen Loss : 25.32496452331543, \t Total Dis Loss : 0.0012664077803492546\n",
      "Steps : 10700, \t Total Gen Loss : 23.376590728759766, \t Total Dis Loss : 0.0016677684616297483\n",
      "Steps : 10800, \t Total Gen Loss : 24.07247543334961, \t Total Dis Loss : 0.0009242451633326709\n",
      "Steps : 10900, \t Total Gen Loss : 22.399707794189453, \t Total Dis Loss : 0.0007783258333802223\n",
      "Steps : 11000, \t Total Gen Loss : 22.523052215576172, \t Total Dis Loss : 0.006109853740781546\n",
      "Steps : 11100, \t Total Gen Loss : 20.189022064208984, \t Total Dis Loss : 0.028304576873779297\n",
      "Steps : 11200, \t Total Gen Loss : 23.358972549438477, \t Total Dis Loss : 0.008047359995543957\n",
      "Time for epoch 2 is 334.47332191467285 sec\n",
      "Steps : 11300, \t Total Gen Loss : 22.53256607055664, \t Total Dis Loss : 0.0009545000502839684\n",
      "Steps : 11400, \t Total Gen Loss : 25.313867568969727, \t Total Dis Loss : 0.0018298669019713998\n",
      "Steps : 11500, \t Total Gen Loss : 25.23602294921875, \t Total Dis Loss : 0.000926814042031765\n",
      "Steps : 11600, \t Total Gen Loss : 26.004592895507812, \t Total Dis Loss : 0.0016644236166030169\n",
      "Steps : 11700, \t Total Gen Loss : 23.617053985595703, \t Total Dis Loss : 0.005153344944119453\n",
      "Steps : 11800, \t Total Gen Loss : 24.270374298095703, \t Total Dis Loss : 0.0009258773643523455\n",
      "Steps : 11900, \t Total Gen Loss : 22.122772216796875, \t Total Dis Loss : 0.01385781355202198\n",
      "Steps : 12000, \t Total Gen Loss : 25.27225685119629, \t Total Dis Loss : 0.0009541638428345323\n",
      "Steps : 12100, \t Total Gen Loss : 23.21579933166504, \t Total Dis Loss : 0.0028899319004267454\n",
      "Steps : 12200, \t Total Gen Loss : 22.923738479614258, \t Total Dis Loss : 0.0019339643185958266\n",
      "Steps : 12300, \t Total Gen Loss : 23.629117965698242, \t Total Dis Loss : 0.0005394358304329216\n",
      "Steps : 12400, \t Total Gen Loss : 21.546478271484375, \t Total Dis Loss : 0.001893146545626223\n",
      "Steps : 12500, \t Total Gen Loss : 22.574636459350586, \t Total Dis Loss : 0.0013634628849104047\n",
      "Steps : 12600, \t Total Gen Loss : 21.84125518798828, \t Total Dis Loss : 0.0008510848274454474\n",
      "Steps : 12700, \t Total Gen Loss : 21.611661911010742, \t Total Dis Loss : 0.0029103129636496305\n",
      "Steps : 12800, \t Total Gen Loss : 24.254058837890625, \t Total Dis Loss : 0.0010250814957544208\n",
      "Steps : 12900, \t Total Gen Loss : 23.75027847290039, \t Total Dis Loss : 0.009694005362689495\n",
      "Steps : 13000, \t Total Gen Loss : 24.32042694091797, \t Total Dis Loss : 0.00488396966829896\n",
      "Steps : 13100, \t Total Gen Loss : 22.42669677734375, \t Total Dis Loss : 0.0004862490459345281\n",
      "Steps : 13200, \t Total Gen Loss : 24.38294792175293, \t Total Dis Loss : 0.0006540163885802031\n",
      "Steps : 13300, \t Total Gen Loss : 22.848953247070312, \t Total Dis Loss : 0.0012387894093990326\n",
      "Steps : 13400, \t Total Gen Loss : 25.280118942260742, \t Total Dis Loss : 0.00430800998583436\n",
      "Steps : 13500, \t Total Gen Loss : 26.10110855102539, \t Total Dis Loss : 0.0025272476486861706\n",
      "Steps : 13600, \t Total Gen Loss : 25.226552963256836, \t Total Dis Loss : 0.0026675958652049303\n",
      "Steps : 13700, \t Total Gen Loss : 22.383563995361328, \t Total Dis Loss : 0.00241392501629889\n",
      "Steps : 13800, \t Total Gen Loss : 23.089237213134766, \t Total Dis Loss : 0.0008833033498376608\n",
      "Steps : 13900, \t Total Gen Loss : 23.829391479492188, \t Total Dis Loss : 0.017112435773015022\n",
      "Steps : 14000, \t Total Gen Loss : 25.85982322692871, \t Total Dis Loss : 0.001005277968943119\n",
      "Steps : 14100, \t Total Gen Loss : 31.027807235717773, \t Total Dis Loss : 0.0005340927746146917\n",
      "Steps : 14200, \t Total Gen Loss : 26.323287963867188, \t Total Dis Loss : 0.0016169367590919137\n",
      "Steps : 14300, \t Total Gen Loss : 26.436410903930664, \t Total Dis Loss : 0.0034165417309850454\n",
      "Steps : 14400, \t Total Gen Loss : 22.68706512451172, \t Total Dis Loss : 0.0021910674404352903\n",
      "Steps : 14500, \t Total Gen Loss : 23.66489028930664, \t Total Dis Loss : 0.0017413562163710594\n",
      "Steps : 14600, \t Total Gen Loss : 23.783294677734375, \t Total Dis Loss : 0.00039801205275580287\n",
      "Steps : 14700, \t Total Gen Loss : 25.025745391845703, \t Total Dis Loss : 0.0015313610201701522\n",
      "Steps : 14800, \t Total Gen Loss : 23.3247013092041, \t Total Dis Loss : 0.001293341745622456\n",
      "Steps : 14900, \t Total Gen Loss : 25.373645782470703, \t Total Dis Loss : 0.0003031085943803191\n",
      "Steps : 15000, \t Total Gen Loss : 24.973793029785156, \t Total Dis Loss : 0.004144958686083555\n",
      "Steps : 15100, \t Total Gen Loss : 26.464622497558594, \t Total Dis Loss : 0.0005674191052094102\n",
      "Steps : 15200, \t Total Gen Loss : 23.279991149902344, \t Total Dis Loss : 0.000814891594927758\n",
      "Steps : 15300, \t Total Gen Loss : 22.668903350830078, \t Total Dis Loss : 0.0005141346482560039\n",
      "Steps : 15400, \t Total Gen Loss : 26.719867706298828, \t Total Dis Loss : 0.0033747663255780935\n",
      "Steps : 15500, \t Total Gen Loss : 25.644485473632812, \t Total Dis Loss : 0.0021208543330430984\n",
      "Steps : 15600, \t Total Gen Loss : 26.534841537475586, \t Total Dis Loss : 0.0007726176409050822\n",
      "Steps : 15700, \t Total Gen Loss : 25.210317611694336, \t Total Dis Loss : 0.0006657785270363092\n",
      "Steps : 15800, \t Total Gen Loss : 21.8779296875, \t Total Dis Loss : 0.00886792503297329\n",
      "Steps : 15900, \t Total Gen Loss : 25.109848022460938, \t Total Dis Loss : 0.0012463354505598545\n",
      "Steps : 16000, \t Total Gen Loss : 26.662595748901367, \t Total Dis Loss : 0.0006459438009187579\n",
      "Steps : 16100, \t Total Gen Loss : 28.417572021484375, \t Total Dis Loss : 0.0006622640648856759\n",
      "Steps : 16200, \t Total Gen Loss : 26.0697078704834, \t Total Dis Loss : 0.0010376116260886192\n",
      "Steps : 16300, \t Total Gen Loss : 25.855133056640625, \t Total Dis Loss : 0.005058675073087215\n",
      "Steps : 16400, \t Total Gen Loss : 21.587039947509766, \t Total Dis Loss : 0.007258136756718159\n",
      "Steps : 16500, \t Total Gen Loss : 25.808141708374023, \t Total Dis Loss : 0.0006562067428603768\n",
      "Steps : 16600, \t Total Gen Loss : 27.104324340820312, \t Total Dis Loss : 0.0008940023835748434\n",
      "Steps : 16700, \t Total Gen Loss : 23.76504898071289, \t Total Dis Loss : 0.000874705146998167\n",
      "Steps : 16800, \t Total Gen Loss : 23.152225494384766, \t Total Dis Loss : 0.000687716412357986\n",
      "Time for epoch 3 is 336.97765135765076 sec\n",
      "Steps : 16900, \t Total Gen Loss : 22.86708641052246, \t Total Dis Loss : 0.006635538302361965\n",
      "Steps : 17000, \t Total Gen Loss : 25.53614616394043, \t Total Dis Loss : 0.0009521671454422176\n",
      "Steps : 17100, \t Total Gen Loss : 21.319501876831055, \t Total Dis Loss : 0.03606650233268738\n",
      "Steps : 17200, \t Total Gen Loss : 23.004898071289062, \t Total Dis Loss : 0.0005908723687753081\n",
      "Steps : 17300, \t Total Gen Loss : 25.047504425048828, \t Total Dis Loss : 0.0006477385177277029\n",
      "Steps : 17400, \t Total Gen Loss : 25.19044303894043, \t Total Dis Loss : 0.001981486799195409\n",
      "Steps : 17500, \t Total Gen Loss : 23.94781494140625, \t Total Dis Loss : 0.002378379926085472\n",
      "Steps : 17600, \t Total Gen Loss : 23.33456039428711, \t Total Dis Loss : 0.0004811749386135489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 17700, \t Total Gen Loss : 24.428205490112305, \t Total Dis Loss : 0.0005231931572780013\n",
      "Steps : 17800, \t Total Gen Loss : 24.0782527923584, \t Total Dis Loss : 0.00019512567087076604\n",
      "Steps : 17900, \t Total Gen Loss : 25.609130859375, \t Total Dis Loss : 0.0006717729847878218\n",
      "Steps : 18000, \t Total Gen Loss : 23.30072784423828, \t Total Dis Loss : 0.0009864564053714275\n",
      "Steps : 18100, \t Total Gen Loss : 23.038890838623047, \t Total Dis Loss : 0.0006299753440544009\n",
      "Steps : 18200, \t Total Gen Loss : 25.773956298828125, \t Total Dis Loss : 0.0009026775369420648\n",
      "Steps : 18300, \t Total Gen Loss : 27.178234100341797, \t Total Dis Loss : 0.0005426228162832558\n",
      "Steps : 18400, \t Total Gen Loss : 24.579864501953125, \t Total Dis Loss : 0.0005084158619865775\n",
      "Steps : 18500, \t Total Gen Loss : 24.412979125976562, \t Total Dis Loss : 0.0014656669227406383\n",
      "Steps : 18600, \t Total Gen Loss : 25.453413009643555, \t Total Dis Loss : 0.0012642114888876677\n",
      "Steps : 18700, \t Total Gen Loss : 27.34876251220703, \t Total Dis Loss : 0.0006590824923478067\n",
      "Steps : 18800, \t Total Gen Loss : 21.95797348022461, \t Total Dis Loss : 0.0012534947600215673\n",
      "Steps : 18900, \t Total Gen Loss : 13.57537841796875, \t Total Dis Loss : 2.2891273498535156\n",
      "Steps : 19000, \t Total Gen Loss : 23.58867073059082, \t Total Dis Loss : 0.0010298627894371748\n",
      "Steps : 19100, \t Total Gen Loss : 23.24346160888672, \t Total Dis Loss : 0.0011684289202094078\n",
      "Steps : 19200, \t Total Gen Loss : 25.752172470092773, \t Total Dis Loss : 0.0006739873206242919\n",
      "Steps : 19300, \t Total Gen Loss : 26.336748123168945, \t Total Dis Loss : 0.03689517080783844\n",
      "Steps : 19400, \t Total Gen Loss : 23.2020320892334, \t Total Dis Loss : 0.0009830426424741745\n",
      "Steps : 19500, \t Total Gen Loss : 25.857908248901367, \t Total Dis Loss : 0.001634513959288597\n",
      "Steps : 19600, \t Total Gen Loss : 27.996784210205078, \t Total Dis Loss : 0.0002716624003369361\n",
      "Steps : 19700, \t Total Gen Loss : 24.122316360473633, \t Total Dis Loss : 0.0005343015654943883\n",
      "Steps : 19800, \t Total Gen Loss : 24.92098617553711, \t Total Dis Loss : 0.0003490595263428986\n",
      "Steps : 19900, \t Total Gen Loss : 22.015289306640625, \t Total Dis Loss : 0.0029667436610907316\n",
      "Steps : 20000, \t Total Gen Loss : 22.136199951171875, \t Total Dis Loss : 0.002687059808522463\n",
      "Steps : 20100, \t Total Gen Loss : 24.312122344970703, \t Total Dis Loss : 0.001370598329231143\n",
      "Steps : 20200, \t Total Gen Loss : 25.533512115478516, \t Total Dis Loss : 0.0005677610170096159\n",
      "Steps : 20300, \t Total Gen Loss : 27.77143669128418, \t Total Dis Loss : 0.00024044528254307806\n",
      "Steps : 20400, \t Total Gen Loss : 21.271472930908203, \t Total Dis Loss : 0.004013954196125269\n",
      "Steps : 20500, \t Total Gen Loss : 25.060226440429688, \t Total Dis Loss : 0.0011842905078083277\n",
      "Steps : 20600, \t Total Gen Loss : 23.277101516723633, \t Total Dis Loss : 0.002434910973533988\n",
      "Steps : 20700, \t Total Gen Loss : 25.027816772460938, \t Total Dis Loss : 0.0008511421619914472\n",
      "Steps : 20800, \t Total Gen Loss : 25.345829010009766, \t Total Dis Loss : 0.004964867606759071\n",
      "Steps : 20900, \t Total Gen Loss : 26.8521785736084, \t Total Dis Loss : 0.0073501840233802795\n",
      "Steps : 21000, \t Total Gen Loss : 27.43838882446289, \t Total Dis Loss : 0.02026279829442501\n",
      "Steps : 21100, \t Total Gen Loss : 25.650609970092773, \t Total Dis Loss : 0.0023517697118222713\n",
      "Steps : 21200, \t Total Gen Loss : 26.60541343688965, \t Total Dis Loss : 0.0004321602755226195\n",
      "Steps : 21300, \t Total Gen Loss : 27.26854133605957, \t Total Dis Loss : 0.00033119472209364176\n",
      "Steps : 21400, \t Total Gen Loss : 28.27178955078125, \t Total Dis Loss : 0.0007337893475778401\n",
      "Steps : 21500, \t Total Gen Loss : 25.883615493774414, \t Total Dis Loss : 0.0009955537971109152\n",
      "Steps : 21600, \t Total Gen Loss : 22.730192184448242, \t Total Dis Loss : 0.0074106305837631226\n",
      "Steps : 21700, \t Total Gen Loss : 24.316625595092773, \t Total Dis Loss : 0.0009630926069803536\n",
      "Steps : 21800, \t Total Gen Loss : 25.824935913085938, \t Total Dis Loss : 0.0007661416311748326\n",
      "Steps : 21900, \t Total Gen Loss : 23.7741756439209, \t Total Dis Loss : 0.001818430726416409\n",
      "Steps : 22000, \t Total Gen Loss : 20.610319137573242, \t Total Dis Loss : 0.0010611392790451646\n",
      "Steps : 22100, \t Total Gen Loss : 26.541881561279297, \t Total Dis Loss : 0.001226452412083745\n",
      "Steps : 22200, \t Total Gen Loss : 22.855934143066406, \t Total Dis Loss : 0.0007686697645112872\n",
      "Steps : 22300, \t Total Gen Loss : 25.40950584411621, \t Total Dis Loss : 0.0011866677086800337\n",
      "Steps : 22400, \t Total Gen Loss : 23.379501342773438, \t Total Dis Loss : 0.0013834589626640081\n",
      "Steps : 22500, \t Total Gen Loss : 25.499168395996094, \t Total Dis Loss : 0.0005505714216269553\n",
      "Time for epoch 4 is 336.84621715545654 sec\n",
      "Steps : 22600, \t Total Gen Loss : 26.472898483276367, \t Total Dis Loss : 0.008882456459105015\n",
      "Steps : 22700, \t Total Gen Loss : 26.793964385986328, \t Total Dis Loss : 0.00101012724917382\n",
      "Steps : 22800, \t Total Gen Loss : 22.739675521850586, \t Total Dis Loss : 0.008305483497679234\n",
      "Steps : 22900, \t Total Gen Loss : 22.04527473449707, \t Total Dis Loss : 0.0013247568858787417\n",
      "Steps : 23000, \t Total Gen Loss : 25.93771743774414, \t Total Dis Loss : 0.0010508783161640167\n",
      "Steps : 23100, \t Total Gen Loss : 25.983402252197266, \t Total Dis Loss : 0.0004973182221874595\n",
      "Steps : 23200, \t Total Gen Loss : 24.55048179626465, \t Total Dis Loss : 0.0019071615533903241\n",
      "Steps : 23300, \t Total Gen Loss : 28.254634857177734, \t Total Dis Loss : 0.0012698525097221136\n",
      "Steps : 23400, \t Total Gen Loss : 28.575733184814453, \t Total Dis Loss : 0.00045102284639142454\n",
      "Steps : 23500, \t Total Gen Loss : 28.856903076171875, \t Total Dis Loss : 0.0002904881548602134\n",
      "Steps : 23600, \t Total Gen Loss : 29.186138153076172, \t Total Dis Loss : 0.00024219755141530186\n",
      "Steps : 23700, \t Total Gen Loss : 18.159099578857422, \t Total Dis Loss : 1.6035598516464233\n",
      "Steps : 23800, \t Total Gen Loss : 23.80367088317871, \t Total Dis Loss : 0.0008130557253025472\n",
      "Steps : 23900, \t Total Gen Loss : 24.430721282958984, \t Total Dis Loss : 0.0026492809411138296\n",
      "Steps : 24000, \t Total Gen Loss : 26.328556060791016, \t Total Dis Loss : 0.0008555701351724565\n",
      "Steps : 24100, \t Total Gen Loss : 21.994436264038086, \t Total Dis Loss : 0.0013955571921542287\n",
      "Steps : 24200, \t Total Gen Loss : 23.584552764892578, \t Total Dis Loss : 0.002069771522656083\n",
      "Steps : 24300, \t Total Gen Loss : 21.64386558532715, \t Total Dis Loss : 0.0009731864556670189\n",
      "Steps : 24400, \t Total Gen Loss : 23.972318649291992, \t Total Dis Loss : 0.006641693413257599\n",
      "Steps : 24500, \t Total Gen Loss : 24.59613800048828, \t Total Dis Loss : 0.0006260529044084251\n",
      "Steps : 24600, \t Total Gen Loss : 21.64092445373535, \t Total Dis Loss : 0.001403138623572886\n",
      "Steps : 24700, \t Total Gen Loss : 26.085960388183594, \t Total Dis Loss : 0.002362573053687811\n",
      "Steps : 24800, \t Total Gen Loss : 22.842832565307617, \t Total Dis Loss : 0.00099365902133286\n",
      "Steps : 24900, \t Total Gen Loss : 22.59849739074707, \t Total Dis Loss : 0.0026224760804325342\n",
      "Steps : 25000, \t Total Gen Loss : 24.767318725585938, \t Total Dis Loss : 0.000508491531945765\n",
      "Steps : 25100, \t Total Gen Loss : 24.823163986206055, \t Total Dis Loss : 0.001375317806378007\n",
      "Steps : 25200, \t Total Gen Loss : 23.63837242126465, \t Total Dis Loss : 0.0007850740221329033\n",
      "Steps : 25300, \t Total Gen Loss : 22.79613494873047, \t Total Dis Loss : 0.0003111790865659714\n",
      "Steps : 25400, \t Total Gen Loss : 28.19658660888672, \t Total Dis Loss : 0.00024030666099861264\n",
      "Steps : 25500, \t Total Gen Loss : 25.854461669921875, \t Total Dis Loss : 0.00042783169192261994\n",
      "Steps : 25600, \t Total Gen Loss : 24.675121307373047, \t Total Dis Loss : 0.0014635155675932765\n",
      "Steps : 25700, \t Total Gen Loss : 25.499927520751953, \t Total Dis Loss : 0.000864383764564991\n",
      "Steps : 25800, \t Total Gen Loss : 26.698482513427734, \t Total Dis Loss : 0.00031277380185201764\n",
      "Steps : 25900, \t Total Gen Loss : 24.081283569335938, \t Total Dis Loss : 0.00041980016976594925\n",
      "Steps : 26000, \t Total Gen Loss : 22.417551040649414, \t Total Dis Loss : 0.0008911837940104306\n",
      "Steps : 26100, \t Total Gen Loss : 25.737285614013672, \t Total Dis Loss : 0.00041404779767617583\n",
      "Steps : 26200, \t Total Gen Loss : 24.065196990966797, \t Total Dis Loss : 0.001208010595291853\n",
      "Steps : 26300, \t Total Gen Loss : 27.981369018554688, \t Total Dis Loss : 0.0022001410834491253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 26400, \t Total Gen Loss : 23.069135665893555, \t Total Dis Loss : 0.0004154187045060098\n",
      "Steps : 26500, \t Total Gen Loss : 24.660154342651367, \t Total Dis Loss : 0.0004098587669432163\n",
      "Steps : 26600, \t Total Gen Loss : 26.574722290039062, \t Total Dis Loss : 0.00036978587741032243\n",
      "Steps : 26700, \t Total Gen Loss : 28.221683502197266, \t Total Dis Loss : 0.001193790347315371\n",
      "Steps : 26800, \t Total Gen Loss : 24.54084587097168, \t Total Dis Loss : 0.0014888443984091282\n",
      "Steps : 26900, \t Total Gen Loss : 20.75133514404297, \t Total Dis Loss : 0.0059155672788619995\n",
      "Steps : 27000, \t Total Gen Loss : 22.94601058959961, \t Total Dis Loss : 0.0007675890810787678\n",
      "Steps : 27100, \t Total Gen Loss : 22.64274024963379, \t Total Dis Loss : 0.0012250072322785854\n",
      "Steps : 27200, \t Total Gen Loss : 22.53842544555664, \t Total Dis Loss : 0.00033109437208622694\n",
      "Steps : 27300, \t Total Gen Loss : 22.115615844726562, \t Total Dis Loss : 0.00029091699980199337\n",
      "Steps : 27400, \t Total Gen Loss : 22.66779136657715, \t Total Dis Loss : 0.0006301840767264366\n",
      "Steps : 27500, \t Total Gen Loss : 25.15394401550293, \t Total Dis Loss : 0.0038548929151147604\n",
      "Steps : 27600, \t Total Gen Loss : 26.14370346069336, \t Total Dis Loss : 0.0005860827513970435\n",
      "Steps : 27700, \t Total Gen Loss : 22.084575653076172, \t Total Dis Loss : 0.0003208127454854548\n",
      "Steps : 27800, \t Total Gen Loss : 24.148399353027344, \t Total Dis Loss : 0.000338820944307372\n",
      "Steps : 27900, \t Total Gen Loss : 22.141639709472656, \t Total Dis Loss : 0.0005545086460188031\n",
      "Steps : 28000, \t Total Gen Loss : 25.37599754333496, \t Total Dis Loss : 0.0005626180209219456\n",
      "Steps : 28100, \t Total Gen Loss : 21.230466842651367, \t Total Dis Loss : 0.001237846678122878\n",
      "Time for epoch 5 is 338.2919056415558 sec\n",
      "Steps : 28200, \t Total Gen Loss : 23.61285400390625, \t Total Dis Loss : 0.0001606331206858158\n",
      "Steps : 28300, \t Total Gen Loss : 25.914512634277344, \t Total Dis Loss : 0.00023006604169495404\n",
      "Steps : 28400, \t Total Gen Loss : 25.721160888671875, \t Total Dis Loss : 0.00016243172285612673\n",
      "Steps : 28500, \t Total Gen Loss : 25.468961715698242, \t Total Dis Loss : 0.00010957465565297753\n",
      "Steps : 28600, \t Total Gen Loss : 25.14775848388672, \t Total Dis Loss : 0.0007223371067084372\n",
      "Steps : 28700, \t Total Gen Loss : 22.728944778442383, \t Total Dis Loss : 0.0005295334849506617\n",
      "Steps : 28800, \t Total Gen Loss : 20.98653793334961, \t Total Dis Loss : 0.02085604891180992\n",
      "Steps : 28900, \t Total Gen Loss : 24.420970916748047, \t Total Dis Loss : 0.00035634945379570127\n",
      "Steps : 29000, \t Total Gen Loss : 20.724388122558594, \t Total Dis Loss : 0.003420604392886162\n",
      "Steps : 29100, \t Total Gen Loss : 24.684040069580078, \t Total Dis Loss : 0.0010691490024328232\n",
      "Steps : 29200, \t Total Gen Loss : 22.427797317504883, \t Total Dis Loss : 0.0007372049149125814\n",
      "Steps : 29300, \t Total Gen Loss : 29.731245040893555, \t Total Dis Loss : 0.00036450737388804555\n",
      "Steps : 29400, \t Total Gen Loss : 26.054546356201172, \t Total Dis Loss : 0.0027014140505343676\n",
      "Steps : 29500, \t Total Gen Loss : 28.279720306396484, \t Total Dis Loss : 0.00017037359066307545\n",
      "Steps : 29600, \t Total Gen Loss : 28.562973022460938, \t Total Dis Loss : 0.004831254947930574\n",
      "Steps : 29700, \t Total Gen Loss : 21.74211883544922, \t Total Dis Loss : 0.024277882650494576\n",
      "Steps : 29800, \t Total Gen Loss : 23.773386001586914, \t Total Dis Loss : 0.0005083231953904033\n",
      "Steps : 29900, \t Total Gen Loss : 26.300582885742188, \t Total Dis Loss : 0.0006005221512168646\n",
      "Steps : 30000, \t Total Gen Loss : 25.602951049804688, \t Total Dis Loss : 0.0017850490985438228\n",
      "Steps : 30100, \t Total Gen Loss : 30.44751739501953, \t Total Dis Loss : 0.0001387530064675957\n",
      "Steps : 30200, \t Total Gen Loss : 28.40433692932129, \t Total Dis Loss : 0.00035579747054725885\n",
      "Steps : 30300, \t Total Gen Loss : 30.615528106689453, \t Total Dis Loss : 0.020001692697405815\n",
      "Steps : 30400, \t Total Gen Loss : 27.111663818359375, \t Total Dis Loss : 0.000293488847091794\n",
      "Steps : 30500, \t Total Gen Loss : 26.385900497436523, \t Total Dis Loss : 0.0005969927296973765\n",
      "Steps : 30600, \t Total Gen Loss : 24.885812759399414, \t Total Dis Loss : 0.024320511147379875\n",
      "Steps : 30700, \t Total Gen Loss : 29.687837600708008, \t Total Dis Loss : 0.0002239598543383181\n",
      "Steps : 30800, \t Total Gen Loss : 23.06974220275879, \t Total Dis Loss : 0.003253459231927991\n",
      "Steps : 30900, \t Total Gen Loss : 25.74789810180664, \t Total Dis Loss : 0.0005859495140612125\n",
      "Steps : 31000, \t Total Gen Loss : 25.417705535888672, \t Total Dis Loss : 0.00018344139971304685\n",
      "Steps : 31100, \t Total Gen Loss : 23.894954681396484, \t Total Dis Loss : 0.16793344914913177\n",
      "Steps : 31200, \t Total Gen Loss : 27.163673400878906, \t Total Dis Loss : 0.00019279356638435274\n",
      "Steps : 31300, \t Total Gen Loss : 26.142807006835938, \t Total Dis Loss : 0.0005523349391296506\n",
      "Steps : 31400, \t Total Gen Loss : 25.715002059936523, \t Total Dis Loss : 0.0005541383870877326\n",
      "Steps : 31500, \t Total Gen Loss : 25.075395584106445, \t Total Dis Loss : 0.002014667261391878\n",
      "Steps : 31600, \t Total Gen Loss : 28.323200225830078, \t Total Dis Loss : 0.001361329574137926\n",
      "Steps : 31700, \t Total Gen Loss : 20.557355880737305, \t Total Dis Loss : 0.756830096244812\n",
      "Steps : 31800, \t Total Gen Loss : 22.32115936279297, \t Total Dis Loss : 0.006017173640429974\n",
      "Steps : 31900, \t Total Gen Loss : 22.68089485168457, \t Total Dis Loss : 0.013037244789302349\n",
      "Steps : 32000, \t Total Gen Loss : 21.53725814819336, \t Total Dis Loss : 0.0029567833989858627\n",
      "Steps : 32100, \t Total Gen Loss : 24.30392074584961, \t Total Dis Loss : 0.0019879054743796587\n",
      "Steps : 32200, \t Total Gen Loss : 21.425519943237305, \t Total Dis Loss : 0.005764598958194256\n",
      "Steps : 32300, \t Total Gen Loss : 23.448564529418945, \t Total Dis Loss : 0.0017040255479514599\n",
      "Steps : 32400, \t Total Gen Loss : 21.497386932373047, \t Total Dis Loss : 0.0023159366101026535\n",
      "Steps : 32500, \t Total Gen Loss : 25.04009437561035, \t Total Dis Loss : 0.0006975019350647926\n",
      "Steps : 32600, \t Total Gen Loss : 22.987211227416992, \t Total Dis Loss : 0.0007267956389114261\n",
      "Steps : 32700, \t Total Gen Loss : 27.778369903564453, \t Total Dis Loss : 0.0005494208307936788\n",
      "Steps : 32800, \t Total Gen Loss : 22.279420852661133, \t Total Dis Loss : 0.0006960784085094929\n",
      "Steps : 32900, \t Total Gen Loss : 23.492341995239258, \t Total Dis Loss : 0.0012323572300374508\n",
      "Steps : 33000, \t Total Gen Loss : 23.35874366760254, \t Total Dis Loss : 0.0017225954215973616\n",
      "Steps : 33100, \t Total Gen Loss : 26.437562942504883, \t Total Dis Loss : 0.0003016436821781099\n",
      "Steps : 33200, \t Total Gen Loss : 24.126935958862305, \t Total Dis Loss : 0.0006003153976053\n",
      "Steps : 33300, \t Total Gen Loss : 27.11861801147461, \t Total Dis Loss : 0.0004975852207280695\n",
      "Steps : 33400, \t Total Gen Loss : 27.798519134521484, \t Total Dis Loss : 0.0003907152567990124\n",
      "Steps : 33500, \t Total Gen Loss : 25.869585037231445, \t Total Dis Loss : 0.001960687804967165\n",
      "Steps : 33600, \t Total Gen Loss : 26.22162437438965, \t Total Dis Loss : 0.0007896412280388176\n",
      "Steps : 33700, \t Total Gen Loss : 27.739233016967773, \t Total Dis Loss : 0.00036871677730232477\n",
      "Time for epoch 6 is 350.40117359161377 sec\n",
      "Steps : 33800, \t Total Gen Loss : 23.781129837036133, \t Total Dis Loss : 0.00034312502248212695\n",
      "Steps : 33900, \t Total Gen Loss : 25.20696258544922, \t Total Dis Loss : 0.0006715617491863668\n",
      "Steps : 34000, \t Total Gen Loss : 26.981658935546875, \t Total Dis Loss : 0.00036985217593610287\n",
      "Steps : 34100, \t Total Gen Loss : 23.211509704589844, \t Total Dis Loss : 0.00010209668835159391\n",
      "Steps : 34200, \t Total Gen Loss : 26.749835968017578, \t Total Dis Loss : 0.00045268057147040963\n",
      "Steps : 34300, \t Total Gen Loss : 26.943960189819336, \t Total Dis Loss : 0.0003350766492076218\n",
      "Steps : 34400, \t Total Gen Loss : 22.912588119506836, \t Total Dis Loss : 0.0009055619593709707\n",
      "Steps : 34500, \t Total Gen Loss : 24.97062873840332, \t Total Dis Loss : 0.0010072054574266076\n",
      "Steps : 34600, \t Total Gen Loss : 26.97636604309082, \t Total Dis Loss : 0.00029499639640562236\n",
      "Steps : 34700, \t Total Gen Loss : 24.284847259521484, \t Total Dis Loss : 0.0005642855539917946\n",
      "Steps : 34800, \t Total Gen Loss : 25.957731246948242, \t Total Dis Loss : 0.0008591539226472378\n",
      "Steps : 34900, \t Total Gen Loss : 28.873672485351562, \t Total Dis Loss : 0.00029312330298125744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 35000, \t Total Gen Loss : 26.34208106994629, \t Total Dis Loss : 0.0014917835360392928\n",
      "Steps : 35100, \t Total Gen Loss : 24.947813034057617, \t Total Dis Loss : 0.0004899410414509475\n",
      "Steps : 35200, \t Total Gen Loss : 27.8383731842041, \t Total Dis Loss : 0.0003534152056090534\n",
      "Steps : 35300, \t Total Gen Loss : 27.022891998291016, \t Total Dis Loss : 0.0003405017778277397\n",
      "Steps : 35400, \t Total Gen Loss : 23.20978546142578, \t Total Dis Loss : 0.00034916435834020376\n",
      "Steps : 35500, \t Total Gen Loss : 25.49427032470703, \t Total Dis Loss : 0.00023587647592648864\n",
      "Steps : 35600, \t Total Gen Loss : 27.680450439453125, \t Total Dis Loss : 0.00020693294936791062\n",
      "Steps : 35700, \t Total Gen Loss : 28.55379295349121, \t Total Dis Loss : 0.00014276852016337216\n",
      "Steps : 35800, \t Total Gen Loss : 27.010494232177734, \t Total Dis Loss : 0.00013310190115589648\n",
      "Steps : 35900, \t Total Gen Loss : 28.183895111083984, \t Total Dis Loss : 5.660526949213818e-05\n",
      "Steps : 36000, \t Total Gen Loss : 30.657474517822266, \t Total Dis Loss : 0.00014886945427861065\n",
      "Steps : 36100, \t Total Gen Loss : 25.846635818481445, \t Total Dis Loss : 5.242162660579197e-05\n",
      "Steps : 36200, \t Total Gen Loss : 28.837554931640625, \t Total Dis Loss : 4.545442425296642e-05\n",
      "Steps : 36300, \t Total Gen Loss : 25.746292114257812, \t Total Dis Loss : 6.036292325006798e-05\n",
      "Steps : 36400, \t Total Gen Loss : 29.19359588623047, \t Total Dis Loss : 0.00042809199658222497\n",
      "Steps : 36500, \t Total Gen Loss : 21.89882469177246, \t Total Dis Loss : 0.0034587783738970757\n",
      "Steps : 36600, \t Total Gen Loss : 26.776853561401367, \t Total Dis Loss : 0.0016794742550700903\n",
      "Steps : 36700, \t Total Gen Loss : 27.685781478881836, \t Total Dis Loss : 0.00032092456240206957\n",
      "Steps : 36800, \t Total Gen Loss : 25.709415435791016, \t Total Dis Loss : 0.00023431688896380365\n",
      "Steps : 36900, \t Total Gen Loss : 26.001014709472656, \t Total Dis Loss : 0.0003072096442338079\n",
      "Steps : 37000, \t Total Gen Loss : 24.424699783325195, \t Total Dis Loss : 0.003824958810582757\n",
      "Steps : 37100, \t Total Gen Loss : 23.885299682617188, \t Total Dis Loss : 0.00025588436983525753\n",
      "Steps : 37200, \t Total Gen Loss : 24.74399757385254, \t Total Dis Loss : 0.001215961528941989\n",
      "Steps : 37300, \t Total Gen Loss : 27.81665802001953, \t Total Dis Loss : 0.0001691189972916618\n",
      "Steps : 37400, \t Total Gen Loss : 24.81493377685547, \t Total Dis Loss : 0.00012678178609348834\n",
      "Steps : 37500, \t Total Gen Loss : 25.000621795654297, \t Total Dis Loss : 0.00022770499344915152\n",
      "Steps : 37600, \t Total Gen Loss : 25.344005584716797, \t Total Dis Loss : 0.00015825749142095447\n",
      "Steps : 37700, \t Total Gen Loss : 26.42479705810547, \t Total Dis Loss : 0.006143036298453808\n",
      "Steps : 37800, \t Total Gen Loss : 24.432884216308594, \t Total Dis Loss : 0.00010942911467282102\n",
      "Steps : 37900, \t Total Gen Loss : 25.134714126586914, \t Total Dis Loss : 7.315444963751361e-05\n",
      "Steps : 38000, \t Total Gen Loss : 25.817310333251953, \t Total Dis Loss : 0.0003014264802914113\n",
      "Steps : 38100, \t Total Gen Loss : 25.942535400390625, \t Total Dis Loss : 0.0001413678692188114\n",
      "Steps : 38200, \t Total Gen Loss : 24.37957763671875, \t Total Dis Loss : 0.00013591599417850375\n",
      "Steps : 38300, \t Total Gen Loss : 24.911865234375, \t Total Dis Loss : 8.969988266471773e-05\n",
      "Steps : 38400, \t Total Gen Loss : 24.28635597229004, \t Total Dis Loss : 6.963939813431352e-05\n",
      "Steps : 38500, \t Total Gen Loss : 26.113386154174805, \t Total Dis Loss : 8.494969370076433e-05\n",
      "Steps : 38600, \t Total Gen Loss : 27.38355827331543, \t Total Dis Loss : 3.9974544051801786e-05\n",
      "Steps : 38700, \t Total Gen Loss : 27.31879234313965, \t Total Dis Loss : 4.139763041166589e-05\n",
      "Steps : 38800, \t Total Gen Loss : 27.469818115234375, \t Total Dis Loss : 3.529121750034392e-05\n",
      "Steps : 38900, \t Total Gen Loss : 26.73481559753418, \t Total Dis Loss : 3.858085619867779e-05\n",
      "Steps : 39000, \t Total Gen Loss : 25.07150650024414, \t Total Dis Loss : 4.0067272493615746e-05\n",
      "Steps : 39100, \t Total Gen Loss : 24.55507469177246, \t Total Dis Loss : 9.966411744244397e-05\n",
      "Steps : 39200, \t Total Gen Loss : 29.627304077148438, \t Total Dis Loss : 0.008702123537659645\n",
      "Steps : 39300, \t Total Gen Loss : 23.078996658325195, \t Total Dis Loss : 0.0009663042728789151\n",
      "Time for epoch 7 is 351.30626106262207 sec\n",
      "Steps : 39400, \t Total Gen Loss : 26.737157821655273, \t Total Dis Loss : 0.001109287841245532\n",
      "Steps : 39500, \t Total Gen Loss : 25.19620132446289, \t Total Dis Loss : 0.0001590433530509472\n",
      "Steps : 39600, \t Total Gen Loss : 32.109683990478516, \t Total Dis Loss : 0.0002432324836263433\n",
      "Steps : 39700, \t Total Gen Loss : 32.35554885864258, \t Total Dis Loss : 0.00903038214892149\n",
      "Steps : 39800, \t Total Gen Loss : 26.160194396972656, \t Total Dis Loss : 0.0003193574957549572\n",
      "Steps : 39900, \t Total Gen Loss : 28.38172149658203, \t Total Dis Loss : 0.0030628745444118977\n",
      "Steps : 40000, \t Total Gen Loss : 29.493059158325195, \t Total Dis Loss : 0.0016999738290905952\n",
      "Steps : 40100, \t Total Gen Loss : 27.385330200195312, \t Total Dis Loss : 0.000453901884611696\n",
      "Steps : 40200, \t Total Gen Loss : 27.443828582763672, \t Total Dis Loss : 0.0004164413549005985\n",
      "Steps : 40300, \t Total Gen Loss : 25.532573699951172, \t Total Dis Loss : 0.0007862981874495745\n",
      "Steps : 40400, \t Total Gen Loss : 26.941980361938477, \t Total Dis Loss : 0.0005210977396927774\n",
      "Steps : 40500, \t Total Gen Loss : 26.340776443481445, \t Total Dis Loss : 0.001515375217422843\n",
      "Steps : 40600, \t Total Gen Loss : 25.48323631286621, \t Total Dis Loss : 0.0007444260991178453\n",
      "Steps : 40700, \t Total Gen Loss : 28.292156219482422, \t Total Dis Loss : 3.1010487873572856e-05\n",
      "Steps : 40800, \t Total Gen Loss : 26.828092575073242, \t Total Dis Loss : 0.0025911228731274605\n",
      "Steps : 40900, \t Total Gen Loss : 27.736066818237305, \t Total Dis Loss : 0.00042733363807201385\n",
      "Steps : 41000, \t Total Gen Loss : 27.157379150390625, \t Total Dis Loss : 0.00021202545030973852\n",
      "Steps : 41100, \t Total Gen Loss : 28.1787052154541, \t Total Dis Loss : 0.00021449224732350558\n",
      "Steps : 41200, \t Total Gen Loss : 24.629615783691406, \t Total Dis Loss : 0.001086444011889398\n",
      "Steps : 41300, \t Total Gen Loss : 29.598302841186523, \t Total Dis Loss : 0.000341871491400525\n",
      "Steps : 41400, \t Total Gen Loss : 28.578405380249023, \t Total Dis Loss : 0.007856408134102821\n",
      "Steps : 41500, \t Total Gen Loss : 30.51495933532715, \t Total Dis Loss : 5.676663568010554e-05\n",
      "Steps : 41600, \t Total Gen Loss : 32.17694091796875, \t Total Dis Loss : 0.0005219051381573081\n",
      "Steps : 41700, \t Total Gen Loss : 28.249284744262695, \t Total Dis Loss : 0.0012978609884157777\n",
      "Steps : 41800, \t Total Gen Loss : 25.480850219726562, \t Total Dis Loss : 0.002401398029178381\n",
      "Steps : 41900, \t Total Gen Loss : 25.420312881469727, \t Total Dis Loss : 0.0018517739372327924\n",
      "Steps : 42000, \t Total Gen Loss : 25.54486656188965, \t Total Dis Loss : 0.0008587514748796821\n",
      "Steps : 42100, \t Total Gen Loss : 23.585620880126953, \t Total Dis Loss : 0.002918446436524391\n",
      "Steps : 42200, \t Total Gen Loss : 30.4012393951416, \t Total Dis Loss : 0.004535979125648737\n",
      "Steps : 42300, \t Total Gen Loss : 28.170867919921875, \t Total Dis Loss : 0.013528278097510338\n",
      "Steps : 42400, \t Total Gen Loss : 28.110177993774414, \t Total Dis Loss : 0.0006935087731108069\n",
      "Steps : 42500, \t Total Gen Loss : 31.90198516845703, \t Total Dis Loss : 0.0022136818151921034\n",
      "Steps : 42600, \t Total Gen Loss : 29.558971405029297, \t Total Dis Loss : 0.021135905757546425\n",
      "Steps : 42700, \t Total Gen Loss : 31.210063934326172, \t Total Dis Loss : 0.0009006051695905626\n",
      "Steps : 42800, \t Total Gen Loss : 29.536312103271484, \t Total Dis Loss : 0.0014219216536730528\n",
      "Steps : 42900, \t Total Gen Loss : 27.017955780029297, \t Total Dis Loss : 0.0008778289775364101\n",
      "Steps : 43000, \t Total Gen Loss : 28.10309600830078, \t Total Dis Loss : 0.0012431954964995384\n",
      "Steps : 43100, \t Total Gen Loss : 28.02366065979004, \t Total Dis Loss : 0.00022136348707135767\n",
      "Steps : 43200, \t Total Gen Loss : 22.81371307373047, \t Total Dis Loss : 0.18393586575984955\n",
      "Steps : 43300, \t Total Gen Loss : 32.19586181640625, \t Total Dis Loss : 0.0019261997658759356\n",
      "Steps : 43400, \t Total Gen Loss : 27.480846405029297, \t Total Dis Loss : 0.0002927432069554925\n",
      "Steps : 43500, \t Total Gen Loss : 29.150920867919922, \t Total Dis Loss : 0.00017570573254488409\n",
      "Steps : 43600, \t Total Gen Loss : 30.11073875427246, \t Total Dis Loss : 0.0005509749753400683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 43700, \t Total Gen Loss : 29.732919692993164, \t Total Dis Loss : 0.00033132312819361687\n",
      "Steps : 43800, \t Total Gen Loss : 29.23515510559082, \t Total Dis Loss : 0.0004702288133557886\n",
      "Steps : 43900, \t Total Gen Loss : 24.971927642822266, \t Total Dis Loss : 0.0010297037661075592\n",
      "Steps : 44000, \t Total Gen Loss : 24.92984390258789, \t Total Dis Loss : 0.0003476319834589958\n",
      "Steps : 44100, \t Total Gen Loss : 25.83761215209961, \t Total Dis Loss : 0.006876570172607899\n",
      "Steps : 44200, \t Total Gen Loss : 25.744537353515625, \t Total Dis Loss : 0.005297509487718344\n",
      "Steps : 44300, \t Total Gen Loss : 30.654672622680664, \t Total Dis Loss : 0.0015495517291128635\n",
      "Steps : 44400, \t Total Gen Loss : 28.418212890625, \t Total Dis Loss : 0.0007111721788533032\n",
      "Steps : 44500, \t Total Gen Loss : 25.850296020507812, \t Total Dis Loss : 0.00025036773877218366\n",
      "Steps : 44600, \t Total Gen Loss : 24.729991912841797, \t Total Dis Loss : 0.011909565888345242\n",
      "Steps : 44700, \t Total Gen Loss : 24.53171157836914, \t Total Dis Loss : 0.00046088488306850195\n",
      "Steps : 44800, \t Total Gen Loss : 24.032215118408203, \t Total Dis Loss : 0.0003524385974742472\n",
      "Steps : 44900, \t Total Gen Loss : 26.420562744140625, \t Total Dis Loss : 0.000230951642151922\n",
      "Steps : 45000, \t Total Gen Loss : 26.478713989257812, \t Total Dis Loss : 0.0005422547692433\n",
      "Time for epoch 8 is 354.41614961624146 sec\n",
      "Steps : 45100, \t Total Gen Loss : 27.5859375, \t Total Dis Loss : 0.00045665953075513244\n",
      "Steps : 45200, \t Total Gen Loss : 23.643924713134766, \t Total Dis Loss : 0.0006202206714078784\n",
      "Steps : 45300, \t Total Gen Loss : 28.955007553100586, \t Total Dis Loss : 0.0330946259200573\n",
      "Steps : 45400, \t Total Gen Loss : 31.79886245727539, \t Total Dis Loss : 0.0001284534955630079\n",
      "Steps : 45500, \t Total Gen Loss : 26.813261032104492, \t Total Dis Loss : 0.003992554731667042\n",
      "Steps : 45600, \t Total Gen Loss : 26.242555618286133, \t Total Dis Loss : 0.0006828806945122778\n",
      "Steps : 45700, \t Total Gen Loss : 22.883773803710938, \t Total Dis Loss : 0.0005883310805074871\n",
      "Steps : 45800, \t Total Gen Loss : 27.428813934326172, \t Total Dis Loss : 0.0013594027841463685\n",
      "Steps : 45900, \t Total Gen Loss : 22.88060188293457, \t Total Dis Loss : 0.0007595268543809652\n",
      "Steps : 46000, \t Total Gen Loss : 27.017908096313477, \t Total Dis Loss : 0.0008100094855763018\n",
      "Steps : 46100, \t Total Gen Loss : 25.853225708007812, \t Total Dis Loss : 0.0006968211382627487\n",
      "Steps : 46200, \t Total Gen Loss : 27.18881607055664, \t Total Dis Loss : 0.002376157557591796\n",
      "Steps : 46300, \t Total Gen Loss : 24.314239501953125, \t Total Dis Loss : 0.02957518771290779\n",
      "Steps : 46400, \t Total Gen Loss : 22.94861602783203, \t Total Dis Loss : 0.00026104217977263033\n",
      "Steps : 46500, \t Total Gen Loss : 26.096391677856445, \t Total Dis Loss : 0.00032477075001224875\n",
      "Steps : 46600, \t Total Gen Loss : 23.84056854248047, \t Total Dis Loss : 0.0008632190874777734\n",
      "Steps : 46700, \t Total Gen Loss : 24.75630760192871, \t Total Dis Loss : 0.0002618999860715121\n",
      "Steps : 46800, \t Total Gen Loss : 26.244653701782227, \t Total Dis Loss : 0.00016462831990793347\n",
      "Steps : 46900, \t Total Gen Loss : 22.454551696777344, \t Total Dis Loss : 0.0009956274880096316\n",
      "Steps : 47000, \t Total Gen Loss : 27.66876220703125, \t Total Dis Loss : 5.5212782172020525e-05\n",
      "Steps : 47100, \t Total Gen Loss : 25.23668098449707, \t Total Dis Loss : 0.0005778884515166283\n",
      "Steps : 47200, \t Total Gen Loss : 25.834426879882812, \t Total Dis Loss : 0.0001988830917980522\n",
      "Steps : 47300, \t Total Gen Loss : 25.757057189941406, \t Total Dis Loss : 0.04714129865169525\n",
      "Steps : 47400, \t Total Gen Loss : 26.878700256347656, \t Total Dis Loss : 0.0002771905274130404\n",
      "Steps : 47500, \t Total Gen Loss : 26.936077117919922, \t Total Dis Loss : 0.0003050113737117499\n",
      "Steps : 47600, \t Total Gen Loss : 29.43671226501465, \t Total Dis Loss : 4.731151420855895e-05\n",
      "Steps : 47700, \t Total Gen Loss : 24.253047943115234, \t Total Dis Loss : 0.0004228304896969348\n",
      "Steps : 47800, \t Total Gen Loss : 24.211496353149414, \t Total Dis Loss : 0.00037117215106263757\n",
      "Steps : 47900, \t Total Gen Loss : 27.016935348510742, \t Total Dis Loss : 0.0007259888225235045\n",
      "Steps : 48000, \t Total Gen Loss : 24.024709701538086, \t Total Dis Loss : 0.00031462477636523545\n",
      "Steps : 48100, \t Total Gen Loss : 29.26125144958496, \t Total Dis Loss : 0.0004916616016998887\n",
      "Steps : 48200, \t Total Gen Loss : 23.593984603881836, \t Total Dis Loss : 0.0004148746083956212\n",
      "Steps : 48300, \t Total Gen Loss : 22.916128158569336, \t Total Dis Loss : 0.0006216344772838056\n",
      "Steps : 48400, \t Total Gen Loss : 22.000896453857422, \t Total Dis Loss : 0.0024123110342770815\n",
      "Steps : 48500, \t Total Gen Loss : 21.811767578125, \t Total Dis Loss : 0.0030635958537459373\n",
      "Steps : 48600, \t Total Gen Loss : 21.813213348388672, \t Total Dis Loss : 0.0036677008029073477\n",
      "Steps : 48700, \t Total Gen Loss : 26.71125030517578, \t Total Dis Loss : 0.0012103508925065398\n",
      "Steps : 48800, \t Total Gen Loss : 26.728832244873047, \t Total Dis Loss : 0.001680054236203432\n",
      "Steps : 48900, \t Total Gen Loss : 22.87910270690918, \t Total Dis Loss : 0.0022489167749881744\n",
      "Steps : 49000, \t Total Gen Loss : 27.310218811035156, \t Total Dis Loss : 0.0002052390918834135\n",
      "Steps : 49100, \t Total Gen Loss : 23.994430541992188, \t Total Dis Loss : 0.0007030518027022481\n",
      "Steps : 49200, \t Total Gen Loss : 25.057645797729492, \t Total Dis Loss : 0.0001192455820273608\n",
      "Steps : 49300, \t Total Gen Loss : 26.123748779296875, \t Total Dis Loss : 0.0006857506814412773\n",
      "Steps : 49400, \t Total Gen Loss : 25.009201049804688, \t Total Dis Loss : 0.0005177825223654509\n",
      "Steps : 49500, \t Total Gen Loss : 23.535179138183594, \t Total Dis Loss : 0.00048799507203511894\n",
      "Steps : 49600, \t Total Gen Loss : 23.98839569091797, \t Total Dis Loss : 0.0004385379725135863\n",
      "Steps : 49700, \t Total Gen Loss : 24.83060073852539, \t Total Dis Loss : 0.00020376201428007334\n",
      "Steps : 49800, \t Total Gen Loss : 25.68441390991211, \t Total Dis Loss : 0.00034906287328340113\n",
      "Steps : 49900, \t Total Gen Loss : 27.710012435913086, \t Total Dis Loss : 0.00037076894659549\n",
      "Steps : 50000, \t Total Gen Loss : 24.076534271240234, \t Total Dis Loss : 0.0007542360108345747\n",
      "Steps : 50100, \t Total Gen Loss : 24.96150016784668, \t Total Dis Loss : 0.0002639888843987137\n",
      "Steps : 50200, \t Total Gen Loss : 24.336559295654297, \t Total Dis Loss : 0.0004001598572358489\n",
      "Steps : 50300, \t Total Gen Loss : 28.052888870239258, \t Total Dis Loss : 0.0002628869842737913\n",
      "Steps : 50400, \t Total Gen Loss : 26.83812713623047, \t Total Dis Loss : 0.00014846734120510519\n",
      "Steps : 50500, \t Total Gen Loss : 26.875850677490234, \t Total Dis Loss : 7.681571878492832e-05\n",
      "Steps : 50600, \t Total Gen Loss : 26.077396392822266, \t Total Dis Loss : 0.0003453892713878304\n",
      "Time for epoch 9 is 347.0223648548126 sec\n",
      "Steps : 50700, \t Total Gen Loss : 25.809486389160156, \t Total Dis Loss : 0.0007332597160711884\n",
      "Steps : 50800, \t Total Gen Loss : 21.419340133666992, \t Total Dis Loss : 0.006719439756125212\n",
      "Steps : 50900, \t Total Gen Loss : 24.256711959838867, \t Total Dis Loss : 0.0014988944167271256\n",
      "Steps : 51000, \t Total Gen Loss : 24.317466735839844, \t Total Dis Loss : 0.0004071355506312102\n",
      "Steps : 51100, \t Total Gen Loss : 22.525697708129883, \t Total Dis Loss : 0.00044902018271386623\n",
      "Steps : 51200, \t Total Gen Loss : 24.92377471923828, \t Total Dis Loss : 0.00033554728724993765\n",
      "Steps : 51300, \t Total Gen Loss : 22.88190460205078, \t Total Dis Loss : 0.000877558602951467\n",
      "Steps : 51400, \t Total Gen Loss : 22.878009796142578, \t Total Dis Loss : 0.0007660973933525383\n",
      "Steps : 51500, \t Total Gen Loss : 23.97723960876465, \t Total Dis Loss : 0.00025987959816120565\n",
      "Steps : 51600, \t Total Gen Loss : 25.407188415527344, \t Total Dis Loss : 0.0006397045799531043\n",
      "Steps : 51700, \t Total Gen Loss : 24.517789840698242, \t Total Dis Loss : 0.00035608996404334903\n",
      "Steps : 51800, \t Total Gen Loss : 25.718700408935547, \t Total Dis Loss : 0.00024127599317580462\n",
      "Steps : 51900, \t Total Gen Loss : 23.218366622924805, \t Total Dis Loss : 0.0002270452823722735\n",
      "Steps : 52000, \t Total Gen Loss : 22.247589111328125, \t Total Dis Loss : 0.000676408177241683\n",
      "Steps : 52100, \t Total Gen Loss : 25.85454750061035, \t Total Dis Loss : 0.00036920164711773396\n",
      "Steps : 52200, \t Total Gen Loss : 25.122100830078125, \t Total Dis Loss : 8.969392365543172e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 52300, \t Total Gen Loss : 24.658222198486328, \t Total Dis Loss : 0.00019957787299063057\n",
      "Steps : 52400, \t Total Gen Loss : 24.385356903076172, \t Total Dis Loss : 0.010450679808855057\n",
      "Steps : 52500, \t Total Gen Loss : 24.834510803222656, \t Total Dis Loss : 0.0020495299249887466\n",
      "Steps : 52600, \t Total Gen Loss : 25.30573272705078, \t Total Dis Loss : 0.0002461056283209473\n",
      "Steps : 52700, \t Total Gen Loss : 24.898208618164062, \t Total Dis Loss : 0.00015822112618479878\n",
      "Steps : 52800, \t Total Gen Loss : 26.805973052978516, \t Total Dis Loss : 0.0007295126561075449\n",
      "Steps : 52900, \t Total Gen Loss : 25.35552978515625, \t Total Dis Loss : 0.0005635962006635964\n",
      "Steps : 53000, \t Total Gen Loss : 27.172821044921875, \t Total Dis Loss : 7.483144872821867e-05\n",
      "Steps : 53100, \t Total Gen Loss : 29.258495330810547, \t Total Dis Loss : 0.0004546573618426919\n",
      "Steps : 53200, \t Total Gen Loss : 24.55353355407715, \t Total Dis Loss : 0.00039432093035429716\n",
      "Steps : 53300, \t Total Gen Loss : 28.415712356567383, \t Total Dis Loss : 4.717355113825761e-05\n",
      "Steps : 53400, \t Total Gen Loss : 28.626535415649414, \t Total Dis Loss : 0.0003244974068365991\n",
      "Steps : 53500, \t Total Gen Loss : 24.892669677734375, \t Total Dis Loss : 0.0007333388202823699\n",
      "Steps : 53600, \t Total Gen Loss : 26.35207176208496, \t Total Dis Loss : 0.000696602975949645\n",
      "Steps : 53700, \t Total Gen Loss : 27.888320922851562, \t Total Dis Loss : 0.005268269684165716\n",
      "Steps : 53800, \t Total Gen Loss : 29.056217193603516, \t Total Dis Loss : 0.0009439628920517862\n",
      "Steps : 53900, \t Total Gen Loss : 30.63054656982422, \t Total Dis Loss : 0.0008568580960854888\n",
      "Steps : 54000, \t Total Gen Loss : 27.144407272338867, \t Total Dis Loss : 0.001146690221503377\n",
      "Steps : 54100, \t Total Gen Loss : 27.213748931884766, \t Total Dis Loss : 0.0006817434914410114\n",
      "Steps : 54200, \t Total Gen Loss : 28.717403411865234, \t Total Dis Loss : 0.0007395653519779444\n",
      "Steps : 54300, \t Total Gen Loss : 29.70783805847168, \t Total Dis Loss : 0.00040797702968120575\n",
      "Steps : 54400, \t Total Gen Loss : 25.855634689331055, \t Total Dis Loss : 0.0016529979184269905\n",
      "Steps : 54500, \t Total Gen Loss : 25.167966842651367, \t Total Dis Loss : 0.0019377124262973666\n",
      "Steps : 54600, \t Total Gen Loss : 27.529897689819336, \t Total Dis Loss : 0.0002225309726782143\n",
      "Steps : 54700, \t Total Gen Loss : 28.664155960083008, \t Total Dis Loss : 0.01382575836032629\n",
      "Steps : 54800, \t Total Gen Loss : 23.873741149902344, \t Total Dis Loss : 0.0002783933887258172\n",
      "Steps : 54900, \t Total Gen Loss : 30.478073120117188, \t Total Dis Loss : 0.00011770326818805188\n",
      "Steps : 55000, \t Total Gen Loss : 25.388090133666992, \t Total Dis Loss : 7.818550511728972e-05\n",
      "Steps : 55100, \t Total Gen Loss : 26.33461570739746, \t Total Dis Loss : 0.0001483509986428544\n",
      "Steps : 55200, \t Total Gen Loss : 27.779464721679688, \t Total Dis Loss : 7.90606572991237e-05\n",
      "Steps : 55300, \t Total Gen Loss : 24.71152114868164, \t Total Dis Loss : 0.0011308338725939393\n",
      "Steps : 55400, \t Total Gen Loss : 28.764650344848633, \t Total Dis Loss : 0.0004728639905806631\n",
      "Steps : 55500, \t Total Gen Loss : 26.939987182617188, \t Total Dis Loss : 0.00025078945327550173\n",
      "Steps : 55600, \t Total Gen Loss : 28.523876190185547, \t Total Dis Loss : 0.001111638848669827\n",
      "Steps : 55700, \t Total Gen Loss : 26.723047256469727, \t Total Dis Loss : 0.00019550079014152288\n",
      "Steps : 55800, \t Total Gen Loss : 27.499988555908203, \t Total Dis Loss : 0.00021766990539617836\n",
      "Steps : 55900, \t Total Gen Loss : 30.043699264526367, \t Total Dis Loss : 0.00013099850912112743\n",
      "Steps : 56000, \t Total Gen Loss : 26.814666748046875, \t Total Dis Loss : 0.0005445925635285676\n",
      "Steps : 56100, \t Total Gen Loss : 23.488231658935547, \t Total Dis Loss : 0.00024238326295744628\n",
      "Steps : 56200, \t Total Gen Loss : 27.26962661743164, \t Total Dis Loss : 0.00010306713375030085\n",
      "Time for epoch 10 is 348.2749481201172 sec\n",
      "Steps : 56300, \t Total Gen Loss : 27.873069763183594, \t Total Dis Loss : 8.07144824648276e-05\n",
      "Steps : 56400, \t Total Gen Loss : 24.68181800842285, \t Total Dis Loss : 0.0002238739252788946\n",
      "Steps : 56500, \t Total Gen Loss : 24.49446678161621, \t Total Dis Loss : 0.0006331170443445444\n",
      "Steps : 56600, \t Total Gen Loss : 27.38170051574707, \t Total Dis Loss : 6.327083247015253e-05\n",
      "Steps : 56700, \t Total Gen Loss : 27.74243927001953, \t Total Dis Loss : 8.453326881863177e-05\n",
      "Steps : 56800, \t Total Gen Loss : 25.104476928710938, \t Total Dis Loss : 0.000245475210249424\n",
      "Steps : 56900, \t Total Gen Loss : 24.022781372070312, \t Total Dis Loss : 0.0005200845189392567\n",
      "Steps : 57000, \t Total Gen Loss : 25.433300018310547, \t Total Dis Loss : 0.00015352631453424692\n",
      "Steps : 57100, \t Total Gen Loss : 23.91646957397461, \t Total Dis Loss : 0.0001473118318244815\n",
      "Steps : 57200, \t Total Gen Loss : 28.897953033447266, \t Total Dis Loss : 0.00027632276760414243\n",
      "Steps : 57300, \t Total Gen Loss : 25.946456909179688, \t Total Dis Loss : 7.97537577454932e-05\n",
      "Steps : 57400, \t Total Gen Loss : 27.519350051879883, \t Total Dis Loss : 0.00012277887435629964\n",
      "Steps : 57500, \t Total Gen Loss : 24.02033042907715, \t Total Dis Loss : 0.00012102194887120277\n",
      "Steps : 57600, \t Total Gen Loss : 28.877731323242188, \t Total Dis Loss : 0.0004789780068676919\n",
      "Steps : 57700, \t Total Gen Loss : 25.872087478637695, \t Total Dis Loss : 0.0002462726552039385\n",
      "Steps : 57800, \t Total Gen Loss : 27.84105110168457, \t Total Dis Loss : 7.7071636042092e-05\n",
      "Steps : 57900, \t Total Gen Loss : 27.319852828979492, \t Total Dis Loss : 5.0662267312873155e-05\n",
      "Steps : 58000, \t Total Gen Loss : 24.94443702697754, \t Total Dis Loss : 0.001172360498458147\n",
      "Steps : 58100, \t Total Gen Loss : 15.855915069580078, \t Total Dis Loss : 0.7791687846183777\n",
      "Steps : 58200, \t Total Gen Loss : 27.487316131591797, \t Total Dis Loss : 0.0009935690322890878\n",
      "Steps : 58300, \t Total Gen Loss : 28.17430877685547, \t Total Dis Loss : 0.00014256440044846386\n",
      "Steps : 58400, \t Total Gen Loss : 27.9851016998291, \t Total Dis Loss : 5.44228169019334e-05\n",
      "Steps : 58500, \t Total Gen Loss : 25.457836151123047, \t Total Dis Loss : 0.00039172082324512303\n",
      "Steps : 58600, \t Total Gen Loss : 30.11143684387207, \t Total Dis Loss : 3.0269231501733884e-05\n",
      "Steps : 58700, \t Total Gen Loss : 26.62106704711914, \t Total Dis Loss : 3.500052844174206e-05\n",
      "Steps : 58800, \t Total Gen Loss : 27.040111541748047, \t Total Dis Loss : 0.00010258851398248225\n",
      "Steps : 58900, \t Total Gen Loss : 24.978166580200195, \t Total Dis Loss : 0.0006116742151789367\n",
      "Steps : 59000, \t Total Gen Loss : 24.75393295288086, \t Total Dis Loss : 0.00020987592870369554\n",
      "Steps : 59100, \t Total Gen Loss : 26.61338233947754, \t Total Dis Loss : 0.0007110051810741425\n",
      "Steps : 59200, \t Total Gen Loss : 25.569807052612305, \t Total Dis Loss : 0.0002561385917942971\n",
      "Steps : 59300, \t Total Gen Loss : 27.547393798828125, \t Total Dis Loss : 0.00015766231808811426\n",
      "Steps : 59400, \t Total Gen Loss : 22.948213577270508, \t Total Dis Loss : 0.0009073681430891156\n",
      "Steps : 59500, \t Total Gen Loss : 27.99715805053711, \t Total Dis Loss : 6.98910589562729e-05\n",
      "Steps : 59600, \t Total Gen Loss : 27.428085327148438, \t Total Dis Loss : 7.808213558746502e-05\n",
      "Steps : 59700, \t Total Gen Loss : 24.52880096435547, \t Total Dis Loss : 0.0004420362238306552\n",
      "Steps : 59800, \t Total Gen Loss : 24.3896427154541, \t Total Dis Loss : 0.00010112208838108927\n",
      "Steps : 59900, \t Total Gen Loss : 25.35790252685547, \t Total Dis Loss : 0.00011104007717221975\n",
      "Steps : 60000, \t Total Gen Loss : 24.636615753173828, \t Total Dis Loss : 0.00021042492880951613\n",
      "Steps : 60100, \t Total Gen Loss : 27.05360984802246, \t Total Dis Loss : 0.0003578346804715693\n",
      "Steps : 60200, \t Total Gen Loss : 26.744266510009766, \t Total Dis Loss : 0.0001683778245933354\n",
      "Steps : 60300, \t Total Gen Loss : 29.110858917236328, \t Total Dis Loss : 0.00021565468341577798\n",
      "Steps : 60400, \t Total Gen Loss : 26.394588470458984, \t Total Dis Loss : 0.0001535453920951113\n",
      "Steps : 60500, \t Total Gen Loss : 25.98896026611328, \t Total Dis Loss : 0.00010654905054252595\n",
      "Steps : 60600, \t Total Gen Loss : 28.037145614624023, \t Total Dis Loss : 0.0001364468043902889\n",
      "Steps : 60700, \t Total Gen Loss : 27.649093627929688, \t Total Dis Loss : 0.00014024383563082665\n",
      "Steps : 60800, \t Total Gen Loss : 27.470048904418945, \t Total Dis Loss : 0.0011155999964103103\n",
      "Steps : 60900, \t Total Gen Loss : 27.25404930114746, \t Total Dis Loss : 0.00015914438699837774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 61000, \t Total Gen Loss : 26.531221389770508, \t Total Dis Loss : 0.0015355389332398772\n",
      "Steps : 61100, \t Total Gen Loss : 23.2108154296875, \t Total Dis Loss : 0.00021423505677375942\n",
      "Steps : 61200, \t Total Gen Loss : 25.581375122070312, \t Total Dis Loss : 0.0009969144593924284\n",
      "Steps : 61300, \t Total Gen Loss : 23.550020217895508, \t Total Dis Loss : 0.0009466881165280938\n",
      "Steps : 61400, \t Total Gen Loss : 26.522096633911133, \t Total Dis Loss : 0.00017157096590381116\n",
      "Steps : 61500, \t Total Gen Loss : 24.73145294189453, \t Total Dis Loss : 0.00018042181909549981\n",
      "Steps : 61600, \t Total Gen Loss : 21.79800033569336, \t Total Dis Loss : 0.0004387536318972707\n",
      "Steps : 61700, \t Total Gen Loss : 23.279762268066406, \t Total Dis Loss : 0.00035292329266667366\n",
      "Steps : 61800, \t Total Gen Loss : 23.029354095458984, \t Total Dis Loss : 0.0001896828762255609\n",
      "Time for epoch 11 is 342.87165880203247 sec\n",
      "Steps : 61900, \t Total Gen Loss : 24.088294982910156, \t Total Dis Loss : 0.0027740211226046085\n",
      "Steps : 62000, \t Total Gen Loss : 21.36248016357422, \t Total Dis Loss : 0.0009917356073856354\n",
      "Steps : 62100, \t Total Gen Loss : 25.136642456054688, \t Total Dis Loss : 0.0011470302706584334\n",
      "Steps : 62200, \t Total Gen Loss : 23.809083938598633, \t Total Dis Loss : 0.0010220418917015195\n",
      "Steps : 62300, \t Total Gen Loss : 25.389720916748047, \t Total Dis Loss : 0.00027198606403544545\n",
      "Steps : 62400, \t Total Gen Loss : 22.284467697143555, \t Total Dis Loss : 0.00026591826463118196\n",
      "Steps : 62500, \t Total Gen Loss : 26.88467788696289, \t Total Dis Loss : 0.00018380819528829306\n",
      "Steps : 62600, \t Total Gen Loss : 27.115947723388672, \t Total Dis Loss : 0.00011095339868916199\n",
      "Steps : 62700, \t Total Gen Loss : 28.517147064208984, \t Total Dis Loss : 0.002489871811121702\n",
      "Steps : 62800, \t Total Gen Loss : 25.295312881469727, \t Total Dis Loss : 0.1366753727197647\n",
      "Steps : 62900, \t Total Gen Loss : 30.01483726501465, \t Total Dis Loss : 0.0003360647533554584\n",
      "Steps : 63000, \t Total Gen Loss : 29.0726375579834, \t Total Dis Loss : 0.006072393152862787\n",
      "Steps : 63100, \t Total Gen Loss : 27.86109733581543, \t Total Dis Loss : 0.00027199232135899365\n",
      "Steps : 63200, \t Total Gen Loss : 27.620378494262695, \t Total Dis Loss : 0.0008301622001454234\n",
      "Steps : 63300, \t Total Gen Loss : 26.618026733398438, \t Total Dis Loss : 0.00028868456138297915\n",
      "Steps : 63400, \t Total Gen Loss : 28.730613708496094, \t Total Dis Loss : 6.85081904521212e-05\n",
      "Steps : 63500, \t Total Gen Loss : 27.44637107849121, \t Total Dis Loss : 0.0009752538171596825\n",
      "Steps : 63600, \t Total Gen Loss : 29.944242477416992, \t Total Dis Loss : 0.0004665702290367335\n",
      "Steps : 63700, \t Total Gen Loss : 27.65602684020996, \t Total Dis Loss : 7.555202319053933e-05\n",
      "Steps : 63800, \t Total Gen Loss : 26.869789123535156, \t Total Dis Loss : 0.00021546815696638077\n",
      "Steps : 63900, \t Total Gen Loss : 27.28275489807129, \t Total Dis Loss : 0.000597249309066683\n",
      "Steps : 64000, \t Total Gen Loss : 33.397499084472656, \t Total Dis Loss : 4.64385339000728e-05\n",
      "Steps : 64100, \t Total Gen Loss : 27.695581436157227, \t Total Dis Loss : 1.933860403369181e-05\n",
      "Steps : 64200, \t Total Gen Loss : 30.255958557128906, \t Total Dis Loss : 0.001385592040605843\n",
      "Steps : 64300, \t Total Gen Loss : 23.98829460144043, \t Total Dis Loss : 0.006006400100886822\n",
      "Steps : 64400, \t Total Gen Loss : 25.024578094482422, \t Total Dis Loss : 0.004151119384914637\n",
      "Steps : 64500, \t Total Gen Loss : 27.532432556152344, \t Total Dis Loss : 0.005383487790822983\n",
      "Steps : 64600, \t Total Gen Loss : 27.76321029663086, \t Total Dis Loss : 0.00012089782831026241\n",
      "Steps : 64700, \t Total Gen Loss : 26.668664932250977, \t Total Dis Loss : 0.00025792483938857913\n",
      "Steps : 64800, \t Total Gen Loss : 29.189651489257812, \t Total Dis Loss : 0.00011738269677152857\n",
      "Steps : 64900, \t Total Gen Loss : 26.155864715576172, \t Total Dis Loss : 8.228157093981281e-05\n",
      "Steps : 65000, \t Total Gen Loss : 24.862346649169922, \t Total Dis Loss : 0.00018243708473164588\n",
      "Steps : 65100, \t Total Gen Loss : 25.64445686340332, \t Total Dis Loss : 0.0002682735794223845\n",
      "Steps : 65200, \t Total Gen Loss : 24.63437271118164, \t Total Dis Loss : 0.00017150190251413733\n",
      "Steps : 65300, \t Total Gen Loss : 27.305522918701172, \t Total Dis Loss : 8.774399611866102e-05\n",
      "Steps : 65400, \t Total Gen Loss : 26.92940902709961, \t Total Dis Loss : 0.00026829520356841385\n",
      "Steps : 65500, \t Total Gen Loss : 26.672119140625, \t Total Dis Loss : 0.00020323999342508614\n",
      "Steps : 65600, \t Total Gen Loss : 26.925973892211914, \t Total Dis Loss : 0.00036963040474802256\n",
      "Steps : 65700, \t Total Gen Loss : 29.261310577392578, \t Total Dis Loss : 8.173493552021682e-05\n",
      "Steps : 65800, \t Total Gen Loss : 24.78487205505371, \t Total Dis Loss : 0.0002122760924976319\n",
      "Steps : 65900, \t Total Gen Loss : 28.208721160888672, \t Total Dis Loss : 0.00016679533291608095\n",
      "Steps : 66000, \t Total Gen Loss : 23.45892333984375, \t Total Dis Loss : 0.0006298309308476746\n",
      "Steps : 66100, \t Total Gen Loss : 26.44614028930664, \t Total Dis Loss : 5.1234168495284393e-05\n",
      "Steps : 66200, \t Total Gen Loss : 24.603931427001953, \t Total Dis Loss : 0.0012289873557165265\n",
      "Steps : 66300, \t Total Gen Loss : 24.125165939331055, \t Total Dis Loss : 0.0008445233106613159\n",
      "Steps : 66400, \t Total Gen Loss : 26.554710388183594, \t Total Dis Loss : 0.00015459058340638876\n",
      "Steps : 66500, \t Total Gen Loss : 25.18413543701172, \t Total Dis Loss : 0.0001763607288012281\n",
      "Steps : 66600, \t Total Gen Loss : 28.540679931640625, \t Total Dis Loss : 1.715537473501172e-05\n",
      "Steps : 66700, \t Total Gen Loss : 30.166473388671875, \t Total Dis Loss : 2.371260416111909e-05\n",
      "Steps : 66800, \t Total Gen Loss : 24.611629486083984, \t Total Dis Loss : 0.001116511644795537\n",
      "Steps : 66900, \t Total Gen Loss : 24.48278045654297, \t Total Dis Loss : 0.00012625126692000777\n",
      "Steps : 67000, \t Total Gen Loss : 23.63275909423828, \t Total Dis Loss : 0.00014207750791683793\n",
      "Steps : 67100, \t Total Gen Loss : 23.888076782226562, \t Total Dis Loss : 0.00021253897284623235\n",
      "Steps : 67200, \t Total Gen Loss : 22.62755584716797, \t Total Dis Loss : 0.0004782472678925842\n",
      "Steps : 67300, \t Total Gen Loss : 30.800796508789062, \t Total Dis Loss : 0.00013818730076309294\n",
      "Steps : 67400, \t Total Gen Loss : 26.503246307373047, \t Total Dis Loss : 0.0004494957101996988\n",
      "Steps : 67500, \t Total Gen Loss : 27.749998092651367, \t Total Dis Loss : 0.000791696016676724\n",
      "Time for epoch 12 is 344.9781401157379 sec\n",
      "Steps : 67600, \t Total Gen Loss : 24.005388259887695, \t Total Dis Loss : 0.0001803024933906272\n",
      "Steps : 67700, \t Total Gen Loss : 25.64885139465332, \t Total Dis Loss : 7.344222103711218e-05\n",
      "Steps : 67800, \t Total Gen Loss : 29.775724411010742, \t Total Dis Loss : 0.00042366620618849993\n",
      "Steps : 67900, \t Total Gen Loss : 25.098617553710938, \t Total Dis Loss : 0.005629583261907101\n",
      "Steps : 68000, \t Total Gen Loss : 24.427034378051758, \t Total Dis Loss : 0.0007481816573999822\n",
      "Steps : 68100, \t Total Gen Loss : 24.772258758544922, \t Total Dis Loss : 0.0001118453437811695\n",
      "Steps : 68200, \t Total Gen Loss : 24.501197814941406, \t Total Dis Loss : 0.00025502475909888744\n",
      "Steps : 68300, \t Total Gen Loss : 27.271638870239258, \t Total Dis Loss : 0.0008275762666016817\n",
      "Steps : 68400, \t Total Gen Loss : 30.195358276367188, \t Total Dis Loss : 0.00016532176232431084\n",
      "Steps : 68500, \t Total Gen Loss : 21.850078582763672, \t Total Dis Loss : 0.0009030897635966539\n",
      "Steps : 68600, \t Total Gen Loss : 26.164514541625977, \t Total Dis Loss : 0.00020311267871875316\n",
      "Steps : 68700, \t Total Gen Loss : 28.25375747680664, \t Total Dis Loss : 0.00025803246535360813\n",
      "Steps : 68800, \t Total Gen Loss : 29.283170700073242, \t Total Dis Loss : 0.00011148313933517784\n",
      "Steps : 68900, \t Total Gen Loss : 23.591476440429688, \t Total Dis Loss : 9.12362738745287e-05\n",
      "Steps : 69000, \t Total Gen Loss : 24.4210205078125, \t Total Dis Loss : 0.00024288885470014066\n",
      "Steps : 69100, \t Total Gen Loss : 23.768836975097656, \t Total Dis Loss : 0.0008851657039485872\n",
      "Steps : 69200, \t Total Gen Loss : 26.240358352661133, \t Total Dis Loss : 9.764413698576391e-05\n",
      "Steps : 69300, \t Total Gen Loss : 23.806163787841797, \t Total Dis Loss : 0.0001118008149205707\n",
      "Steps : 69400, \t Total Gen Loss : 22.981048583984375, \t Total Dis Loss : 0.0006624766392633319\n",
      "Steps : 69500, \t Total Gen Loss : 23.234603881835938, \t Total Dis Loss : 0.0003130441764369607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 69600, \t Total Gen Loss : 25.095138549804688, \t Total Dis Loss : 2.005381611525081e-05\n",
      "Steps : 69700, \t Total Gen Loss : 27.438058853149414, \t Total Dis Loss : 5.158409840078093e-05\n",
      "Steps : 69800, \t Total Gen Loss : 24.652158737182617, \t Total Dis Loss : 7.463717338396236e-05\n",
      "Steps : 69900, \t Total Gen Loss : 25.379165649414062, \t Total Dis Loss : 7.4651965405792e-05\n",
      "Steps : 70000, \t Total Gen Loss : 24.574350357055664, \t Total Dis Loss : 0.0002260303299408406\n",
      "Steps : 70100, \t Total Gen Loss : 25.050832748413086, \t Total Dis Loss : 0.00029624448507092893\n",
      "Steps : 70200, \t Total Gen Loss : 27.469202041625977, \t Total Dis Loss : 3.32648451148998e-05\n",
      "Steps : 70300, \t Total Gen Loss : 23.77355194091797, \t Total Dis Loss : 8.408846770180389e-05\n",
      "Steps : 70400, \t Total Gen Loss : 26.774721145629883, \t Total Dis Loss : 0.00010127506538992748\n",
      "Steps : 70500, \t Total Gen Loss : 28.975008010864258, \t Total Dis Loss : 0.00013408440281637013\n",
      "Steps : 70600, \t Total Gen Loss : 25.45252799987793, \t Total Dis Loss : 0.0003334609209559858\n",
      "Steps : 70700, \t Total Gen Loss : 26.026138305664062, \t Total Dis Loss : 9.349703032057732e-05\n",
      "Steps : 70800, \t Total Gen Loss : 26.250102996826172, \t Total Dis Loss : 0.00011033190821763128\n",
      "Steps : 70900, \t Total Gen Loss : 26.90398406982422, \t Total Dis Loss : 6.066838977858424e-05\n",
      "Steps : 71000, \t Total Gen Loss : 27.982412338256836, \t Total Dis Loss : 0.00021552758698817343\n",
      "Steps : 71100, \t Total Gen Loss : 21.671552658081055, \t Total Dis Loss : 0.002626000205054879\n",
      "Steps : 71200, \t Total Gen Loss : 24.705259323120117, \t Total Dis Loss : 0.0014736211160197854\n",
      "Steps : 71300, \t Total Gen Loss : 22.457931518554688, \t Total Dis Loss : 0.0012863259762525558\n",
      "Steps : 71400, \t Total Gen Loss : 24.97946548461914, \t Total Dis Loss : 0.0008285802905447781\n",
      "Steps : 71500, \t Total Gen Loss : 25.348222732543945, \t Total Dis Loss : 0.00018918057321570814\n",
      "Steps : 71600, \t Total Gen Loss : 26.921627044677734, \t Total Dis Loss : 0.00038936256896704435\n",
      "Steps : 71700, \t Total Gen Loss : 25.3970890045166, \t Total Dis Loss : 5.442267865873873e-05\n",
      "Steps : 71800, \t Total Gen Loss : 23.007871627807617, \t Total Dis Loss : 0.00038422923535108566\n",
      "Steps : 71900, \t Total Gen Loss : 23.838594436645508, \t Total Dis Loss : 0.00015265081310644746\n",
      "Steps : 72000, \t Total Gen Loss : 24.327651977539062, \t Total Dis Loss : 0.007604445796459913\n",
      "Steps : 72100, \t Total Gen Loss : 24.103242874145508, \t Total Dis Loss : 0.000401447934564203\n",
      "Steps : 72200, \t Total Gen Loss : 22.06203842163086, \t Total Dis Loss : 0.0003658154746517539\n",
      "Steps : 72300, \t Total Gen Loss : 23.687320709228516, \t Total Dis Loss : 0.00042334222234785557\n",
      "Steps : 72400, \t Total Gen Loss : 24.371898651123047, \t Total Dis Loss : 0.00019840463937725872\n",
      "Steps : 72500, \t Total Gen Loss : 23.540725708007812, \t Total Dis Loss : 0.00013357753050513566\n",
      "Steps : 72600, \t Total Gen Loss : 24.02956199645996, \t Total Dis Loss : 0.00013017444871366024\n",
      "Steps : 72700, \t Total Gen Loss : 27.437368392944336, \t Total Dis Loss : 0.00011353077570674941\n",
      "Steps : 72800, \t Total Gen Loss : 24.328371047973633, \t Total Dis Loss : 0.00019815370615106076\n",
      "Steps : 72900, \t Total Gen Loss : 26.710237503051758, \t Total Dis Loss : 9.166725794784725e-05\n",
      "Steps : 73000, \t Total Gen Loss : 25.29665184020996, \t Total Dis Loss : 0.00026893772883340716\n",
      "Steps : 73100, \t Total Gen Loss : 25.544910430908203, \t Total Dis Loss : 0.00013091145956423134\n",
      "Time for epoch 13 is 344.54058957099915 sec\n",
      "Steps : 73200, \t Total Gen Loss : 23.575035095214844, \t Total Dis Loss : 0.00010391630348749459\n",
      "Steps : 73300, \t Total Gen Loss : 26.42911148071289, \t Total Dis Loss : 6.481462332885712e-05\n",
      "Steps : 73400, \t Total Gen Loss : 25.087514877319336, \t Total Dis Loss : 0.0006546010263264179\n",
      "Steps : 73500, \t Total Gen Loss : 23.711872100830078, \t Total Dis Loss : 0.00018116101273335516\n",
      "Steps : 73600, \t Total Gen Loss : 24.180238723754883, \t Total Dis Loss : 9.431732178200036e-05\n",
      "Steps : 73700, \t Total Gen Loss : 24.459774017333984, \t Total Dis Loss : 0.00013315519026946276\n",
      "Steps : 73800, \t Total Gen Loss : 25.117990493774414, \t Total Dis Loss : 0.00014547618047799915\n",
      "Steps : 73900, \t Total Gen Loss : 26.324399948120117, \t Total Dis Loss : 0.00017626458429731429\n",
      "Steps : 74000, \t Total Gen Loss : 23.737464904785156, \t Total Dis Loss : 0.0002994299284182489\n",
      "Steps : 74100, \t Total Gen Loss : 27.138715744018555, \t Total Dis Loss : 0.00010491645662114024\n",
      "Steps : 74200, \t Total Gen Loss : 24.446325302124023, \t Total Dis Loss : 0.0001503724924987182\n",
      "Steps : 74300, \t Total Gen Loss : 25.582122802734375, \t Total Dis Loss : 0.00021715034381486475\n",
      "Steps : 74400, \t Total Gen Loss : 21.707944869995117, \t Total Dis Loss : 0.0004404489300213754\n",
      "Steps : 74500, \t Total Gen Loss : 25.877735137939453, \t Total Dis Loss : 9.125783253693953e-05\n",
      "Steps : 74600, \t Total Gen Loss : 24.83926010131836, \t Total Dis Loss : 9.349103493150324e-05\n",
      "Steps : 74700, \t Total Gen Loss : 24.49478530883789, \t Total Dis Loss : 0.09100354462862015\n",
      "Steps : 74800, \t Total Gen Loss : 28.67344856262207, \t Total Dis Loss : 2.2244405045057647e-05\n",
      "Steps : 74900, \t Total Gen Loss : 29.932756423950195, \t Total Dis Loss : 0.00066491833422333\n",
      "Steps : 75000, \t Total Gen Loss : 27.53888702392578, \t Total Dis Loss : 0.0012281257659196854\n",
      "Steps : 75100, \t Total Gen Loss : 25.644502639770508, \t Total Dis Loss : 0.0028738274704664946\n",
      "Steps : 75200, \t Total Gen Loss : 24.791969299316406, \t Total Dis Loss : 0.0006719664088450372\n",
      "Steps : 75300, \t Total Gen Loss : 24.37298583984375, \t Total Dis Loss : 0.005429633893072605\n",
      "Steps : 75400, \t Total Gen Loss : 24.10016632080078, \t Total Dis Loss : 0.000982181285507977\n",
      "Steps : 75500, \t Total Gen Loss : 27.508697509765625, \t Total Dis Loss : 0.00030734913889318705\n",
      "Steps : 75600, \t Total Gen Loss : 29.3971004486084, \t Total Dis Loss : 0.0006471552769653499\n",
      "Steps : 75700, \t Total Gen Loss : 26.63594627380371, \t Total Dis Loss : 0.0004474963352549821\n",
      "Steps : 75800, \t Total Gen Loss : 27.608705520629883, \t Total Dis Loss : 0.0002781249932013452\n",
      "Steps : 75900, \t Total Gen Loss : 27.344707489013672, \t Total Dis Loss : 0.0004977507051080465\n",
      "Steps : 76000, \t Total Gen Loss : 29.979175567626953, \t Total Dis Loss : 0.00260331341996789\n",
      "Steps : 76100, \t Total Gen Loss : 30.47173500061035, \t Total Dis Loss : 0.2808612883090973\n",
      "Steps : 76200, \t Total Gen Loss : 25.646236419677734, \t Total Dis Loss : 0.0002359735080972314\n",
      "Steps : 76300, \t Total Gen Loss : 29.890487670898438, \t Total Dis Loss : 0.00019283413712400943\n",
      "Steps : 76400, \t Total Gen Loss : 28.469724655151367, \t Total Dis Loss : 0.000122256635222584\n",
      "Steps : 76500, \t Total Gen Loss : 27.01432228088379, \t Total Dis Loss : 6.15255266893655e-05\n",
      "Steps : 76600, \t Total Gen Loss : 29.34117889404297, \t Total Dis Loss : 8.169424836523831e-05\n",
      "Steps : 76700, \t Total Gen Loss : 26.76141357421875, \t Total Dis Loss : 0.00046431145165115595\n",
      "Steps : 76800, \t Total Gen Loss : 25.049970626831055, \t Total Dis Loss : 0.0006505906349048018\n",
      "Steps : 76900, \t Total Gen Loss : 26.428691864013672, \t Total Dis Loss : 0.0010693867225199938\n",
      "Steps : 77000, \t Total Gen Loss : 27.62575340270996, \t Total Dis Loss : 0.00019623784464783967\n",
      "Steps : 77100, \t Total Gen Loss : 25.821674346923828, \t Total Dis Loss : 0.001632637926377356\n",
      "Steps : 77200, \t Total Gen Loss : 26.045700073242188, \t Total Dis Loss : 0.0005439169472083449\n",
      "Steps : 77300, \t Total Gen Loss : 26.6583251953125, \t Total Dis Loss : 0.0002828099823091179\n",
      "Steps : 77400, \t Total Gen Loss : 26.59461784362793, \t Total Dis Loss : 0.00014307707897387445\n",
      "Steps : 77500, \t Total Gen Loss : 25.293685913085938, \t Total Dis Loss : 0.0014349347911775112\n",
      "Steps : 77600, \t Total Gen Loss : 22.164867401123047, \t Total Dis Loss : 0.011014355346560478\n",
      "Steps : 77700, \t Total Gen Loss : 28.844894409179688, \t Total Dis Loss : 0.00020653718092944473\n",
      "Steps : 77800, \t Total Gen Loss : 24.54017448425293, \t Total Dis Loss : 0.0003196785692125559\n",
      "Steps : 77900, \t Total Gen Loss : 21.03267478942871, \t Total Dis Loss : 0.0033834988716989756\n",
      "Steps : 78000, \t Total Gen Loss : 25.736814498901367, \t Total Dis Loss : 0.00016029478865675628\n",
      "Steps : 78100, \t Total Gen Loss : 27.833351135253906, \t Total Dis Loss : 0.00025711936177685857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 78200, \t Total Gen Loss : 24.52930450439453, \t Total Dis Loss : 0.0002585811889730394\n",
      "Steps : 78300, \t Total Gen Loss : 24.31979751586914, \t Total Dis Loss : 0.0003197219339199364\n",
      "Steps : 78400, \t Total Gen Loss : 26.943225860595703, \t Total Dis Loss : 0.00010592068429104984\n",
      "Steps : 78500, \t Total Gen Loss : 25.167339324951172, \t Total Dis Loss : 0.00021436679526232183\n",
      "Steps : 78600, \t Total Gen Loss : 23.828323364257812, \t Total Dis Loss : 0.004881726577877998\n",
      "Steps : 78700, \t Total Gen Loss : 25.484737396240234, \t Total Dis Loss : 5.82191496505402e-05\n",
      "Time for epoch 14 is 343.56625986099243 sec\n",
      "Steps : 78800, \t Total Gen Loss : 25.038820266723633, \t Total Dis Loss : 0.00012709348811767995\n",
      "Steps : 78900, \t Total Gen Loss : 30.78399085998535, \t Total Dis Loss : 0.0009425440221093595\n",
      "Steps : 79000, \t Total Gen Loss : 25.370532989501953, \t Total Dis Loss : 8.681588951731101e-05\n",
      "Steps : 79100, \t Total Gen Loss : 25.39352798461914, \t Total Dis Loss : 0.00022072572028264403\n",
      "Steps : 79200, \t Total Gen Loss : 26.329599380493164, \t Total Dis Loss : 0.0001278813579119742\n",
      "Steps : 79300, \t Total Gen Loss : 26.40739631652832, \t Total Dis Loss : 6.426883192034438e-05\n",
      "Steps : 79400, \t Total Gen Loss : 23.92247200012207, \t Total Dis Loss : 0.0007007842650637031\n",
      "Steps : 79500, \t Total Gen Loss : 27.076997756958008, \t Total Dis Loss : 0.00014003648539073765\n",
      "Steps : 79600, \t Total Gen Loss : 28.13553237915039, \t Total Dis Loss : 6.580467015737668e-05\n",
      "Steps : 79700, \t Total Gen Loss : 25.577775955200195, \t Total Dis Loss : 0.0001755703560775146\n",
      "Steps : 79800, \t Total Gen Loss : 28.60939598083496, \t Total Dis Loss : 6.34943789918907e-05\n",
      "Steps : 79900, \t Total Gen Loss : 29.296478271484375, \t Total Dis Loss : 5.1708044338738546e-05\n",
      "Steps : 80000, \t Total Gen Loss : 27.523967742919922, \t Total Dis Loss : 0.00014726606605108827\n",
      "Steps : 80100, \t Total Gen Loss : 26.302675247192383, \t Total Dis Loss : 0.00017620685684960335\n",
      "Steps : 80200, \t Total Gen Loss : 23.502201080322266, \t Total Dis Loss : 0.002754413289949298\n",
      "Steps : 80300, \t Total Gen Loss : 28.956947326660156, \t Total Dis Loss : 0.00015566448564641178\n",
      "Steps : 80400, \t Total Gen Loss : 26.80335235595703, \t Total Dis Loss : 6.347449379973114e-05\n",
      "Steps : 80500, \t Total Gen Loss : 28.497102737426758, \t Total Dis Loss : 0.0003887669590767473\n",
      "Steps : 80600, \t Total Gen Loss : 26.59549331665039, \t Total Dis Loss : 0.00011240113235544413\n",
      "Steps : 80700, \t Total Gen Loss : 28.77337646484375, \t Total Dis Loss : 3.5154447687091306e-05\n",
      "Steps : 80800, \t Total Gen Loss : 29.524282455444336, \t Total Dis Loss : 3.391120117157698e-05\n",
      "Steps : 80900, \t Total Gen Loss : 29.08945655822754, \t Total Dis Loss : 0.00039901540731079876\n",
      "Steps : 81000, \t Total Gen Loss : 24.30734634399414, \t Total Dis Loss : 0.00026042209356091917\n",
      "Steps : 81100, \t Total Gen Loss : 25.545198440551758, \t Total Dis Loss : 0.00022828592045698315\n",
      "Steps : 81200, \t Total Gen Loss : 27.091833114624023, \t Total Dis Loss : 0.0014265074860304594\n",
      "Steps : 81300, \t Total Gen Loss : 23.4859619140625, \t Total Dis Loss : 0.00032305484637618065\n",
      "Steps : 81400, \t Total Gen Loss : 26.902772903442383, \t Total Dis Loss : 0.0005658349837176502\n",
      "Steps : 81500, \t Total Gen Loss : 24.877676010131836, \t Total Dis Loss : 0.00023952231276780367\n",
      "Steps : 81600, \t Total Gen Loss : 25.188209533691406, \t Total Dis Loss : 0.00015605030057486147\n",
      "Steps : 81700, \t Total Gen Loss : 23.021427154541016, \t Total Dis Loss : 0.000344516069162637\n",
      "Steps : 81800, \t Total Gen Loss : 25.464618682861328, \t Total Dis Loss : 0.00021414569346234202\n",
      "Steps : 81900, \t Total Gen Loss : 26.058664321899414, \t Total Dis Loss : 3.191243013134226e-05\n",
      "Steps : 82000, \t Total Gen Loss : 29.11586570739746, \t Total Dis Loss : 2.1774903871119022e-05\n",
      "Steps : 82100, \t Total Gen Loss : 25.220481872558594, \t Total Dis Loss : 7.760513108223677e-05\n",
      "Steps : 82200, \t Total Gen Loss : 24.864473342895508, \t Total Dis Loss : 0.00042001428664661944\n",
      "Steps : 82300, \t Total Gen Loss : 24.073287963867188, \t Total Dis Loss : 8.233566768467426e-05\n",
      "Steps : 82400, \t Total Gen Loss : 25.607511520385742, \t Total Dis Loss : 9.700609371066093e-05\n",
      "Steps : 82500, \t Total Gen Loss : 23.91707992553711, \t Total Dis Loss : 4.4818283640779555e-05\n",
      "Steps : 82600, \t Total Gen Loss : 24.834741592407227, \t Total Dis Loss : 0.00052978890016675\n",
      "Steps : 82700, \t Total Gen Loss : 28.30088233947754, \t Total Dis Loss : 0.00012349280586931854\n",
      "Steps : 82800, \t Total Gen Loss : 25.4141788482666, \t Total Dis Loss : 6.729466986143962e-05\n",
      "Steps : 82900, \t Total Gen Loss : 24.356876373291016, \t Total Dis Loss : 0.00015480865840800107\n",
      "Steps : 83000, \t Total Gen Loss : 27.391992568969727, \t Total Dis Loss : 8.09158300398849e-05\n",
      "Steps : 83100, \t Total Gen Loss : 26.176227569580078, \t Total Dis Loss : 3.245322295697406e-05\n",
      "Steps : 83200, \t Total Gen Loss : 25.724472045898438, \t Total Dis Loss : 6.965270586078987e-05\n",
      "Steps : 83300, \t Total Gen Loss : 24.749528884887695, \t Total Dis Loss : 0.00016006731311790645\n",
      "Steps : 83400, \t Total Gen Loss : 23.514188766479492, \t Total Dis Loss : 0.0003157715837005526\n",
      "Steps : 83500, \t Total Gen Loss : 29.767337799072266, \t Total Dis Loss : 0.00010048106196336448\n",
      "Steps : 83600, \t Total Gen Loss : 24.442312240600586, \t Total Dis Loss : 0.0003094966523349285\n",
      "Steps : 83700, \t Total Gen Loss : 26.326528549194336, \t Total Dis Loss : 0.0005367789999581873\n",
      "Steps : 83800, \t Total Gen Loss : 23.610998153686523, \t Total Dis Loss : 0.0027155173011124134\n",
      "Steps : 83900, \t Total Gen Loss : 25.538175582885742, \t Total Dis Loss : 4.219885158818215e-05\n",
      "Steps : 84000, \t Total Gen Loss : 26.8981876373291, \t Total Dis Loss : 0.0005257240263745189\n",
      "Steps : 84100, \t Total Gen Loss : 24.901220321655273, \t Total Dis Loss : 9.460798173677176e-05\n",
      "Steps : 84200, \t Total Gen Loss : 25.445392608642578, \t Total Dis Loss : 0.0001591128238942474\n",
      "Steps : 84300, \t Total Gen Loss : 27.553903579711914, \t Total Dis Loss : 0.0006049937219358981\n",
      "Time for epoch 15 is 345.7389018535614 sec\n",
      "Steps : 84400, \t Total Gen Loss : 25.48743438720703, \t Total Dis Loss : 0.00022228635498322546\n",
      "Steps : 84500, \t Total Gen Loss : 28.90595817565918, \t Total Dis Loss : 0.00013176799984648824\n",
      "Steps : 84600, \t Total Gen Loss : 26.592859268188477, \t Total Dis Loss : 8.951815107138827e-05\n",
      "Steps : 84700, \t Total Gen Loss : 26.599016189575195, \t Total Dis Loss : 1.711868026177399e-05\n",
      "Steps : 84800, \t Total Gen Loss : 23.736976623535156, \t Total Dis Loss : 5.964971205685288e-05\n",
      "Steps : 84900, \t Total Gen Loss : 26.979671478271484, \t Total Dis Loss : 1.3456174201564863e-05\n",
      "Steps : 85000, \t Total Gen Loss : 26.336118698120117, \t Total Dis Loss : 2.652140028658323e-05\n",
      "Steps : 85100, \t Total Gen Loss : 28.138578414916992, \t Total Dis Loss : 4.054393866681494e-05\n",
      "Steps : 85200, \t Total Gen Loss : 28.11238670349121, \t Total Dis Loss : 2.822799615387339e-05\n",
      "Steps : 85300, \t Total Gen Loss : 25.006139755249023, \t Total Dis Loss : 0.00022154203907120973\n",
      "Steps : 85400, \t Total Gen Loss : 27.96615982055664, \t Total Dis Loss : 4.2843174014706165e-05\n",
      "Steps : 85500, \t Total Gen Loss : 25.58175277709961, \t Total Dis Loss : 2.356313052587211e-05\n",
      "Steps : 85600, \t Total Gen Loss : 25.24092674255371, \t Total Dis Loss : 0.0009866973850876093\n",
      "Steps : 85700, \t Total Gen Loss : 26.835805892944336, \t Total Dis Loss : 9.987545490730554e-05\n",
      "Steps : 85800, \t Total Gen Loss : 24.957427978515625, \t Total Dis Loss : 0.0001356616849079728\n",
      "Steps : 85900, \t Total Gen Loss : 27.3106746673584, \t Total Dis Loss : 0.000415947288274765\n",
      "Steps : 86000, \t Total Gen Loss : 26.088449478149414, \t Total Dis Loss : 9.709028381621465e-05\n",
      "Steps : 86100, \t Total Gen Loss : 27.806163787841797, \t Total Dis Loss : 0.00010264810407534242\n",
      "Steps : 86200, \t Total Gen Loss : 27.221248626708984, \t Total Dis Loss : 6.495464913314208e-05\n",
      "Steps : 86300, \t Total Gen Loss : 26.11647605895996, \t Total Dis Loss : 0.00016212594346143305\n",
      "Steps : 86400, \t Total Gen Loss : 26.794330596923828, \t Total Dis Loss : 0.00034871030948124826\n",
      "Steps : 86500, \t Total Gen Loss : 21.521717071533203, \t Total Dis Loss : 0.0004707917105406523\n",
      "Steps : 86600, \t Total Gen Loss : 23.83031463623047, \t Total Dis Loss : 0.0008115890086628497\n",
      "Steps : 86700, \t Total Gen Loss : 25.146202087402344, \t Total Dis Loss : 3.2409334380645305e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 86800, \t Total Gen Loss : 28.235275268554688, \t Total Dis Loss : 0.00017575608217157423\n",
      "Steps : 86900, \t Total Gen Loss : 28.438404083251953, \t Total Dis Loss : 0.00010915666643995792\n",
      "Steps : 87000, \t Total Gen Loss : 24.40714454650879, \t Total Dis Loss : 0.0005983402952551842\n",
      "Steps : 87100, \t Total Gen Loss : 24.698148727416992, \t Total Dis Loss : 0.00013981290976516902\n",
      "Steps : 87200, \t Total Gen Loss : 24.48383140563965, \t Total Dis Loss : 0.0001523272949270904\n",
      "Steps : 87300, \t Total Gen Loss : 24.826515197753906, \t Total Dis Loss : 0.00017223550821654499\n",
      "Steps : 87400, \t Total Gen Loss : 24.70992660522461, \t Total Dis Loss : 0.00018898978305514902\n",
      "Steps : 87500, \t Total Gen Loss : 28.931072235107422, \t Total Dis Loss : 7.701209688093513e-05\n",
      "Steps : 87600, \t Total Gen Loss : 28.85407829284668, \t Total Dis Loss : 4.623600034392439e-05\n",
      "Steps : 87700, \t Total Gen Loss : 26.235668182373047, \t Total Dis Loss : 0.002698916010558605\n",
      "Steps : 87800, \t Total Gen Loss : 28.839242935180664, \t Total Dis Loss : 0.00038628093898296356\n",
      "Steps : 87900, \t Total Gen Loss : 31.130172729492188, \t Total Dis Loss : 1.0464493243489414e-05\n",
      "Steps : 88000, \t Total Gen Loss : 26.942419052124023, \t Total Dis Loss : 0.00011703257041517645\n",
      "Steps : 88100, \t Total Gen Loss : 28.49191665649414, \t Total Dis Loss : 0.00012062592577422038\n",
      "Steps : 88200, \t Total Gen Loss : 28.491785049438477, \t Total Dis Loss : 0.00011475505016278476\n",
      "Steps : 88300, \t Total Gen Loss : 25.060157775878906, \t Total Dis Loss : 0.0002740848285611719\n",
      "Steps : 88400, \t Total Gen Loss : 27.640323638916016, \t Total Dis Loss : 7.849900430301204e-05\n",
      "Steps : 88500, \t Total Gen Loss : 28.88626480102539, \t Total Dis Loss : 6.844157178420573e-05\n",
      "Steps : 88600, \t Total Gen Loss : 24.766935348510742, \t Total Dis Loss : 1.8984424968948588e-05\n",
      "Steps : 88700, \t Total Gen Loss : 27.174137115478516, \t Total Dis Loss : 0.0001567211584188044\n",
      "Steps : 88800, \t Total Gen Loss : 25.86669158935547, \t Total Dis Loss : 0.00046629164717160165\n",
      "Steps : 88900, \t Total Gen Loss : 24.542078018188477, \t Total Dis Loss : 0.00015929315122775733\n",
      "Steps : 89000, \t Total Gen Loss : 24.961273193359375, \t Total Dis Loss : 0.00022967212134972215\n",
      "Steps : 89100, \t Total Gen Loss : 28.040817260742188, \t Total Dis Loss : 0.00015015876851975918\n",
      "Steps : 89200, \t Total Gen Loss : 26.53125762939453, \t Total Dis Loss : 7.557608478236943e-05\n",
      "Steps : 89300, \t Total Gen Loss : 24.347064971923828, \t Total Dis Loss : 0.0003497473371680826\n",
      "Steps : 89400, \t Total Gen Loss : 26.52939224243164, \t Total Dis Loss : 0.00027986671193502843\n",
      "Steps : 89500, \t Total Gen Loss : 29.266984939575195, \t Total Dis Loss : 0.0004993489128537476\n",
      "Steps : 89600, \t Total Gen Loss : 27.45598602294922, \t Total Dis Loss : 0.00012190634151920676\n",
      "Steps : 89700, \t Total Gen Loss : 24.052536010742188, \t Total Dis Loss : 0.00013234259677119553\n",
      "Steps : 89800, \t Total Gen Loss : 27.245559692382812, \t Total Dis Loss : 0.0001245145540451631\n",
      "Steps : 89900, \t Total Gen Loss : 24.39811897277832, \t Total Dis Loss : 0.001706777373328805\n",
      "Steps : 90000, \t Total Gen Loss : 27.239011764526367, \t Total Dis Loss : 5.49090800632257e-05\n",
      "Time for epoch 16 is 364.29757738113403 sec\n",
      "Steps : 90100, \t Total Gen Loss : 25.14359474182129, \t Total Dis Loss : 0.00021011604985687882\n",
      "Steps : 90200, \t Total Gen Loss : 27.31439781188965, \t Total Dis Loss : 8.21017601992935e-05\n",
      "Steps : 90300, \t Total Gen Loss : 27.20866584777832, \t Total Dis Loss : 0.0002175543922930956\n",
      "Steps : 90400, \t Total Gen Loss : 28.68821144104004, \t Total Dis Loss : 0.00018497277051210403\n",
      "Steps : 90500, \t Total Gen Loss : 28.742656707763672, \t Total Dis Loss : 6.167358515085652e-05\n",
      "Steps : 90600, \t Total Gen Loss : 26.366792678833008, \t Total Dis Loss : 0.00010604351700749248\n",
      "Steps : 90700, \t Total Gen Loss : 28.569690704345703, \t Total Dis Loss : 0.00025621263193897903\n",
      "Steps : 90800, \t Total Gen Loss : 25.237152099609375, \t Total Dis Loss : 0.003050413215532899\n",
      "Steps : 90900, \t Total Gen Loss : 25.86332893371582, \t Total Dis Loss : 0.005080830305814743\n",
      "Steps : 91000, \t Total Gen Loss : 26.533348083496094, \t Total Dis Loss : 0.0003618505143094808\n",
      "Steps : 91100, \t Total Gen Loss : 24.645559310913086, \t Total Dis Loss : 0.0017013216856867075\n",
      "Steps : 91200, \t Total Gen Loss : 27.20124053955078, \t Total Dis Loss : 0.00022896856535226107\n",
      "Steps : 91300, \t Total Gen Loss : 21.577436447143555, \t Total Dis Loss : 0.001435060054063797\n",
      "Steps : 91400, \t Total Gen Loss : 28.803577423095703, \t Total Dis Loss : 0.0002957255346700549\n",
      "Steps : 91500, \t Total Gen Loss : 27.52582550048828, \t Total Dis Loss : 0.00048258667811751366\n",
      "Steps : 91600, \t Total Gen Loss : 26.021718978881836, \t Total Dis Loss : 0.00018505578918848187\n",
      "Steps : 91700, \t Total Gen Loss : 19.48137664794922, \t Total Dis Loss : 0.003601617645472288\n",
      "Steps : 91800, \t Total Gen Loss : 25.277502059936523, \t Total Dis Loss : 0.0003903961624018848\n",
      "Steps : 91900, \t Total Gen Loss : 27.785568237304688, \t Total Dis Loss : 0.0001884381490526721\n",
      "Steps : 92000, \t Total Gen Loss : 26.725950241088867, \t Total Dis Loss : 0.0019946114625781775\n",
      "Steps : 92100, \t Total Gen Loss : 28.863937377929688, \t Total Dis Loss : 0.00019045181397814304\n",
      "Steps : 92200, \t Total Gen Loss : 31.893428802490234, \t Total Dis Loss : 0.0005150490324012935\n",
      "Steps : 92300, \t Total Gen Loss : 26.675762176513672, \t Total Dis Loss : 0.0004448397667147219\n",
      "Steps : 92400, \t Total Gen Loss : 27.952810287475586, \t Total Dis Loss : 0.00010000915062846616\n",
      "Steps : 92500, \t Total Gen Loss : 29.80282974243164, \t Total Dis Loss : 1.72150776052149e-05\n",
      "Steps : 92600, \t Total Gen Loss : 29.601512908935547, \t Total Dis Loss : 3.392418147996068e-05\n",
      "Steps : 92700, \t Total Gen Loss : 28.326419830322266, \t Total Dis Loss : 3.930219099856913e-05\n",
      "Steps : 92800, \t Total Gen Loss : 28.624637603759766, \t Total Dis Loss : 3.1655265047447756e-05\n",
      "Steps : 92900, \t Total Gen Loss : 25.404443740844727, \t Total Dis Loss : 0.0008178562275134027\n",
      "Steps : 93000, \t Total Gen Loss : 26.7981014251709, \t Total Dis Loss : 0.0004393395211081952\n",
      "Steps : 93100, \t Total Gen Loss : 28.463472366333008, \t Total Dis Loss : 0.00012447463814169168\n",
      "Steps : 93200, \t Total Gen Loss : 28.404863357543945, \t Total Dis Loss : 2.4174447389668785e-05\n",
      "Steps : 93300, \t Total Gen Loss : 26.95621681213379, \t Total Dis Loss : 5.029978638049215e-05\n",
      "Steps : 93400, \t Total Gen Loss : 26.75499725341797, \t Total Dis Loss : 0.00016161207167897373\n",
      "Steps : 93500, \t Total Gen Loss : 29.873165130615234, \t Total Dis Loss : 3.53402501787059e-05\n",
      "Steps : 93600, \t Total Gen Loss : 24.853300094604492, \t Total Dis Loss : 0.000341765204211697\n",
      "Steps : 93700, \t Total Gen Loss : 26.7960205078125, \t Total Dis Loss : 6.859145651105791e-05\n",
      "Steps : 93800, \t Total Gen Loss : 27.87540054321289, \t Total Dis Loss : 3.438219937379472e-05\n",
      "Steps : 93900, \t Total Gen Loss : 28.66022300720215, \t Total Dis Loss : 0.00013041665079072118\n",
      "Steps : 94000, \t Total Gen Loss : 27.77383804321289, \t Total Dis Loss : 0.00010948932322207838\n",
      "Steps : 94100, \t Total Gen Loss : 27.979660034179688, \t Total Dis Loss : 7.340183219639584e-05\n",
      "Steps : 94200, \t Total Gen Loss : 30.779373168945312, \t Total Dis Loss : 0.00024590615066699684\n",
      "Steps : 94300, \t Total Gen Loss : 26.068443298339844, \t Total Dis Loss : 7.83267169026658e-05\n",
      "Steps : 94400, \t Total Gen Loss : 25.88458251953125, \t Total Dis Loss : 0.0002185123594244942\n",
      "Steps : 94500, \t Total Gen Loss : 26.858413696289062, \t Total Dis Loss : 0.00012720109953079373\n",
      "Steps : 94600, \t Total Gen Loss : 25.782400131225586, \t Total Dis Loss : 8.701673505129293e-05\n",
      "Steps : 94700, \t Total Gen Loss : 28.16448211669922, \t Total Dis Loss : 7.900764467194676e-05\n",
      "Steps : 94800, \t Total Gen Loss : 29.240140914916992, \t Total Dis Loss : 0.00011081565753556788\n",
      "Steps : 94900, \t Total Gen Loss : 26.861234664916992, \t Total Dis Loss : 7.436475425492972e-05\n",
      "Steps : 95000, \t Total Gen Loss : 24.218854904174805, \t Total Dis Loss : 0.0009512753458693624\n",
      "Steps : 95100, \t Total Gen Loss : 29.379913330078125, \t Total Dis Loss : 0.0038622592110186815\n",
      "Steps : 95200, \t Total Gen Loss : 24.935176849365234, \t Total Dis Loss : 8.904677088139579e-05\n",
      "Steps : 95300, \t Total Gen Loss : 25.422605514526367, \t Total Dis Loss : 0.00017178250709548593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 95400, \t Total Gen Loss : 30.35726547241211, \t Total Dis Loss : 0.0002418088261038065\n",
      "Steps : 95500, \t Total Gen Loss : 27.813045501708984, \t Total Dis Loss : 0.0006991370464675128\n",
      "Steps : 95600, \t Total Gen Loss : 27.321531295776367, \t Total Dis Loss : 0.00010301650036126375\n",
      "Time for epoch 17 is 360.4349727630615 sec\n",
      "Steps : 95700, \t Total Gen Loss : 26.163894653320312, \t Total Dis Loss : 4.565360359265469e-05\n",
      "Steps : 95800, \t Total Gen Loss : 25.04644775390625, \t Total Dis Loss : 0.003888343460857868\n",
      "Steps : 95900, \t Total Gen Loss : 27.964332580566406, \t Total Dis Loss : 4.411456757225096e-05\n",
      "Steps : 96000, \t Total Gen Loss : 28.89838409423828, \t Total Dis Loss : 5.4674543207511306e-05\n",
      "Steps : 96100, \t Total Gen Loss : 30.439910888671875, \t Total Dis Loss : 2.124222919519525e-05\n",
      "Steps : 96200, \t Total Gen Loss : 29.118717193603516, \t Total Dis Loss : 1.477107434766367e-05\n",
      "Steps : 96300, \t Total Gen Loss : 28.759531021118164, \t Total Dis Loss : 5.849904118804261e-05\n",
      "Steps : 96400, \t Total Gen Loss : 25.73643684387207, \t Total Dis Loss : 3.9260205085156485e-05\n",
      "Steps : 96500, \t Total Gen Loss : 29.513370513916016, \t Total Dis Loss : 2.2195919882506132e-05\n",
      "Steps : 96600, \t Total Gen Loss : 29.991432189941406, \t Total Dis Loss : 3.510147871566005e-05\n",
      "Steps : 96700, \t Total Gen Loss : 30.781831741333008, \t Total Dis Loss : 2.209513695561327e-05\n",
      "Steps : 96800, \t Total Gen Loss : 28.276805877685547, \t Total Dis Loss : 8.969147165771574e-05\n",
      "Steps : 96900, \t Total Gen Loss : 28.733192443847656, \t Total Dis Loss : 0.002493074629455805\n",
      "Steps : 97000, \t Total Gen Loss : 27.372304916381836, \t Total Dis Loss : 7.888641266617924e-05\n",
      "Steps : 97100, \t Total Gen Loss : 29.91179656982422, \t Total Dis Loss : 0.0003057708381675184\n",
      "Steps : 97200, \t Total Gen Loss : 27.93714141845703, \t Total Dis Loss : 4.273362719686702e-05\n",
      "Steps : 97300, \t Total Gen Loss : 30.164928436279297, \t Total Dis Loss : 0.0004714002425316721\n",
      "Steps : 97400, \t Total Gen Loss : 31.402029037475586, \t Total Dis Loss : 0.00024128949735313654\n",
      "Steps : 97500, \t Total Gen Loss : 25.931909561157227, \t Total Dis Loss : 0.00017610314534977078\n",
      "Steps : 97600, \t Total Gen Loss : 28.903066635131836, \t Total Dis Loss : 0.0003992763231508434\n",
      "Steps : 97700, \t Total Gen Loss : 29.01137924194336, \t Total Dis Loss : 7.899090269347653e-05\n",
      "Steps : 97800, \t Total Gen Loss : 26.876422882080078, \t Total Dis Loss : 5.0800368626369163e-05\n",
      "Steps : 97900, \t Total Gen Loss : 29.277124404907227, \t Total Dis Loss : 5.215621422394179e-05\n",
      "Steps : 98000, \t Total Gen Loss : 26.632152557373047, \t Total Dis Loss : 0.00029747941880486906\n",
      "Steps : 98100, \t Total Gen Loss : 30.11067771911621, \t Total Dis Loss : 0.00018506137712392956\n",
      "Steps : 98200, \t Total Gen Loss : 26.51835823059082, \t Total Dis Loss : 7.531104347435758e-05\n",
      "Steps : 98300, \t Total Gen Loss : 28.957897186279297, \t Total Dis Loss : 0.00024865023442544043\n",
      "Steps : 98400, \t Total Gen Loss : 28.379085540771484, \t Total Dis Loss : 5.24870483786799e-05\n",
      "Steps : 98500, \t Total Gen Loss : 25.681241989135742, \t Total Dis Loss : 7.414982246700674e-05\n",
      "Steps : 98600, \t Total Gen Loss : 27.44898223876953, \t Total Dis Loss : 6.876454426674172e-05\n",
      "Steps : 98700, \t Total Gen Loss : 25.598682403564453, \t Total Dis Loss : 3.668487988761626e-05\n",
      "Steps : 98800, \t Total Gen Loss : 26.93229866027832, \t Total Dis Loss : 5.269748726277612e-05\n",
      "Steps : 98900, \t Total Gen Loss : 24.476863861083984, \t Total Dis Loss : 0.00010459659824846312\n",
      "Steps : 99000, \t Total Gen Loss : 31.16469383239746, \t Total Dis Loss : 1.8555983842816204e-05\n",
      "Steps : 99100, \t Total Gen Loss : 26.210121154785156, \t Total Dis Loss : 4.689398701884784e-05\n",
      "Steps : 99200, \t Total Gen Loss : 27.254131317138672, \t Total Dis Loss : 2.8387421480147168e-05\n",
      "Steps : 99300, \t Total Gen Loss : 26.433349609375, \t Total Dis Loss : 7.696521606703755e-06\n",
      "Steps : 99400, \t Total Gen Loss : 25.02540397644043, \t Total Dis Loss : 0.00012372812489047647\n",
      "Steps : 99500, \t Total Gen Loss : 26.82257652282715, \t Total Dis Loss : 7.813272532075644e-05\n",
      "Steps : 99600, \t Total Gen Loss : 25.81777572631836, \t Total Dis Loss : 7.409363752231002e-05\n",
      "Steps : 99700, \t Total Gen Loss : 25.27212142944336, \t Total Dis Loss : 9.237873018719256e-05\n",
      "Steps : 99800, \t Total Gen Loss : 31.220420837402344, \t Total Dis Loss : 1.5005707609816454e-05\n",
      "Steps : 99900, \t Total Gen Loss : 28.65949249267578, \t Total Dis Loss : 0.00048198457807302475\n",
      "Steps : 100000, \t Total Gen Loss : 26.66900062561035, \t Total Dis Loss : 0.00016564520774409175\n",
      "Steps : 100100, \t Total Gen Loss : 24.318775177001953, \t Total Dis Loss : 0.003580177202820778\n",
      "Steps : 100200, \t Total Gen Loss : 28.797592163085938, \t Total Dis Loss : 3.456039848970249e-05\n",
      "Steps : 100300, \t Total Gen Loss : 29.870445251464844, \t Total Dis Loss : 2.7852212951984257e-05\n",
      "Steps : 100400, \t Total Gen Loss : 28.18096923828125, \t Total Dis Loss : 8.054264617385343e-05\n",
      "Steps : 100500, \t Total Gen Loss : 26.81092071533203, \t Total Dis Loss : 5.241537110123318e-06\n",
      "Steps : 100600, \t Total Gen Loss : 28.114986419677734, \t Total Dis Loss : 5.400705776992254e-05\n",
      "Steps : 100700, \t Total Gen Loss : 30.491058349609375, \t Total Dis Loss : 2.4383271011174656e-05\n",
      "Steps : 100800, \t Total Gen Loss : 27.030290603637695, \t Total Dis Loss : 0.00023461246746592224\n",
      "Steps : 100900, \t Total Gen Loss : 28.615751266479492, \t Total Dis Loss : 2.154877620341722e-05\n",
      "Steps : 101000, \t Total Gen Loss : 28.872112274169922, \t Total Dis Loss : 0.00017647181812208146\n",
      "Steps : 101100, \t Total Gen Loss : 25.286319732666016, \t Total Dis Loss : 0.00047140795504674315\n",
      "Steps : 101200, \t Total Gen Loss : 23.542131423950195, \t Total Dis Loss : 0.00039281000499613583\n",
      "Time for epoch 18 is 363.99281787872314 sec\n",
      "Steps : 101300, \t Total Gen Loss : 27.383020401000977, \t Total Dis Loss : 0.0002850840683095157\n",
      "Steps : 101400, \t Total Gen Loss : 25.25228500366211, \t Total Dis Loss : 8.945501031121239e-05\n",
      "Steps : 101500, \t Total Gen Loss : 25.847671508789062, \t Total Dis Loss : 0.0013264493318274617\n",
      "Steps : 101600, \t Total Gen Loss : 25.64557456970215, \t Total Dis Loss : 7.796076533850282e-05\n",
      "Steps : 101700, \t Total Gen Loss : 24.71025848388672, \t Total Dis Loss : 0.00012034134124405682\n",
      "Steps : 101800, \t Total Gen Loss : 25.573347091674805, \t Total Dis Loss : 5.18430388183333e-05\n",
      "Steps : 101900, \t Total Gen Loss : 27.014602661132812, \t Total Dis Loss : 1.76807698153425e-05\n",
      "Steps : 102000, \t Total Gen Loss : 26.314502716064453, \t Total Dis Loss : 0.0006869600038044155\n",
      "Steps : 102100, \t Total Gen Loss : 26.613069534301758, \t Total Dis Loss : 0.00010194376955041662\n",
      "Steps : 102200, \t Total Gen Loss : 28.07959747314453, \t Total Dis Loss : 0.00011938126408495009\n",
      "Steps : 102300, \t Total Gen Loss : 27.3916015625, \t Total Dis Loss : 0.012005077674984932\n",
      "Steps : 102400, \t Total Gen Loss : 30.321136474609375, \t Total Dis Loss : 8.107683970592916e-05\n",
      "Steps : 102500, \t Total Gen Loss : 29.60181427001953, \t Total Dis Loss : 6.032605961081572e-05\n",
      "Steps : 102600, \t Total Gen Loss : 28.417814254760742, \t Total Dis Loss : 0.0007525627734139562\n",
      "Steps : 102700, \t Total Gen Loss : 26.809236526489258, \t Total Dis Loss : 7.576248026452959e-05\n",
      "Steps : 102800, \t Total Gen Loss : 29.57193374633789, \t Total Dis Loss : 1.4700264728162438e-05\n",
      "Steps : 102900, \t Total Gen Loss : 28.673503875732422, \t Total Dis Loss : 2.0538620447041467e-05\n",
      "Steps : 103000, \t Total Gen Loss : 30.530914306640625, \t Total Dis Loss : 6.62078891764395e-05\n",
      "Steps : 103100, \t Total Gen Loss : 25.77895164489746, \t Total Dis Loss : 0.00017457472858950496\n",
      "Steps : 103200, \t Total Gen Loss : 27.789352416992188, \t Total Dis Loss : 1.98492307390552e-05\n",
      "Steps : 103300, \t Total Gen Loss : 26.953956604003906, \t Total Dis Loss : 5.8687059208750725e-05\n",
      "Steps : 103400, \t Total Gen Loss : 27.97046661376953, \t Total Dis Loss : 2.930477785412222e-05\n",
      "Steps : 103500, \t Total Gen Loss : 27.847824096679688, \t Total Dis Loss : 4.117132630199194e-05\n",
      "Steps : 103600, \t Total Gen Loss : 28.158466339111328, \t Total Dis Loss : 7.025679951766506e-05\n",
      "Steps : 103700, \t Total Gen Loss : 26.460704803466797, \t Total Dis Loss : 0.0001956563937710598\n",
      "Steps : 103800, \t Total Gen Loss : 25.71823501586914, \t Total Dis Loss : 9.412995132151991e-05\n",
      "Steps : 103900, \t Total Gen Loss : 26.8940486907959, \t Total Dis Loss : 4.3968346290057525e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 104000, \t Total Gen Loss : 29.680212020874023, \t Total Dis Loss : 3.488262154860422e-05\n",
      "Steps : 104100, \t Total Gen Loss : 26.282386779785156, \t Total Dis Loss : 0.0008645540219731629\n",
      "Steps : 104200, \t Total Gen Loss : 28.11974334716797, \t Total Dis Loss : 7.840291800675914e-05\n",
      "Steps : 104300, \t Total Gen Loss : 26.798851013183594, \t Total Dis Loss : 9.636938193580136e-05\n",
      "Steps : 104400, \t Total Gen Loss : 27.135576248168945, \t Total Dis Loss : 3.517898221616633e-05\n",
      "Steps : 104500, \t Total Gen Loss : 26.637895584106445, \t Total Dis Loss : 4.812503175344318e-05\n",
      "Steps : 104600, \t Total Gen Loss : 27.042865753173828, \t Total Dis Loss : 0.00014431832823902369\n",
      "Steps : 104700, \t Total Gen Loss : 24.34684181213379, \t Total Dis Loss : 4.3980031477985904e-05\n",
      "Steps : 104800, \t Total Gen Loss : 25.61467742919922, \t Total Dis Loss : 7.246407039929181e-05\n",
      "Steps : 104900, \t Total Gen Loss : 27.644182205200195, \t Total Dis Loss : 7.55882792873308e-05\n",
      "Steps : 105000, \t Total Gen Loss : 24.24321937561035, \t Total Dis Loss : 5.977190812700428e-05\n",
      "Steps : 105100, \t Total Gen Loss : 26.22240447998047, \t Total Dis Loss : 0.00022755069949198514\n",
      "Steps : 105200, \t Total Gen Loss : 25.333723068237305, \t Total Dis Loss : 0.00011033675400540233\n",
      "Steps : 105300, \t Total Gen Loss : 29.13111686706543, \t Total Dis Loss : 4.079196514794603e-05\n",
      "Steps : 105400, \t Total Gen Loss : 28.134197235107422, \t Total Dis Loss : 5.7567329349694774e-05\n",
      "Steps : 105500, \t Total Gen Loss : 26.73595428466797, \t Total Dis Loss : 8.354589226655662e-05\n",
      "Steps : 105600, \t Total Gen Loss : 29.970592498779297, \t Total Dis Loss : 0.0001259193813893944\n",
      "Steps : 105700, \t Total Gen Loss : 27.606666564941406, \t Total Dis Loss : 4.623424320016056e-05\n",
      "Steps : 105800, \t Total Gen Loss : 24.733139038085938, \t Total Dis Loss : 6.293772457865998e-05\n",
      "Steps : 105900, \t Total Gen Loss : 27.015668869018555, \t Total Dis Loss : 1.3798848158330657e-05\n",
      "Steps : 106000, \t Total Gen Loss : 29.55735969543457, \t Total Dis Loss : 4.0829723729984835e-05\n",
      "Steps : 106100, \t Total Gen Loss : 26.44659996032715, \t Total Dis Loss : 2.0489660528255627e-05\n",
      "Steps : 106200, \t Total Gen Loss : 30.536945343017578, \t Total Dis Loss : 6.220910290721804e-05\n",
      "Steps : 106300, \t Total Gen Loss : 26.72679328918457, \t Total Dis Loss : 0.0005863271653652191\n",
      "Steps : 106400, \t Total Gen Loss : 24.642898559570312, \t Total Dis Loss : 6.64654653519392e-05\n",
      "Steps : 106500, \t Total Gen Loss : 27.235965728759766, \t Total Dis Loss : 3.384281080798246e-05\n",
      "Steps : 106600, \t Total Gen Loss : 27.851116180419922, \t Total Dis Loss : 0.00023695318668615073\n",
      "Steps : 106700, \t Total Gen Loss : 27.83641815185547, \t Total Dis Loss : 3.44611398759298e-05\n",
      "Steps : 106800, \t Total Gen Loss : 26.492956161499023, \t Total Dis Loss : 5.23500275448896e-05\n",
      "Time for epoch 19 is 390.01387190818787 sec\n",
      "Steps : 106900, \t Total Gen Loss : 28.629106521606445, \t Total Dis Loss : 8.332876313943416e-05\n",
      "Steps : 107000, \t Total Gen Loss : 27.895509719848633, \t Total Dis Loss : 5.9420901379780844e-05\n",
      "Steps : 107100, \t Total Gen Loss : 26.319353103637695, \t Total Dis Loss : 8.495918882545084e-05\n",
      "Steps : 107200, \t Total Gen Loss : 26.81058692932129, \t Total Dis Loss : 0.0001659930858295411\n",
      "Steps : 107300, \t Total Gen Loss : 26.134334564208984, \t Total Dis Loss : 0.0054808109998703\n",
      "Steps : 107400, \t Total Gen Loss : 26.572656631469727, \t Total Dis Loss : 0.00042384833795949817\n",
      "Steps : 107500, \t Total Gen Loss : 30.137775421142578, \t Total Dis Loss : 5.6089600548148155e-05\n",
      "Steps : 107600, \t Total Gen Loss : 28.231576919555664, \t Total Dis Loss : 1.9103739759884775e-05\n",
      "Steps : 107700, \t Total Gen Loss : 24.96695327758789, \t Total Dis Loss : 0.00042267184471711516\n",
      "Steps : 107800, \t Total Gen Loss : 24.689645767211914, \t Total Dis Loss : 0.00022647612786386162\n",
      "Steps : 107900, \t Total Gen Loss : 29.51705551147461, \t Total Dis Loss : 2.0009330910397694e-05\n",
      "Steps : 108000, \t Total Gen Loss : 27.875003814697266, \t Total Dis Loss : 0.00357244024053216\n",
      "Steps : 108100, \t Total Gen Loss : 29.86520767211914, \t Total Dis Loss : 0.0003799655241891742\n",
      "Steps : 108200, \t Total Gen Loss : 29.877168655395508, \t Total Dis Loss : 0.00010266902972944081\n",
      "Steps : 108300, \t Total Gen Loss : 29.47935676574707, \t Total Dis Loss : 0.0005741469212807715\n",
      "Steps : 108400, \t Total Gen Loss : 24.704927444458008, \t Total Dis Loss : 0.0008983022416941822\n",
      "Steps : 108500, \t Total Gen Loss : 29.122005462646484, \t Total Dis Loss : 0.000214676110772416\n",
      "Steps : 108600, \t Total Gen Loss : 24.948423385620117, \t Total Dis Loss : 0.0013570451410487294\n",
      "Steps : 108700, \t Total Gen Loss : 26.656112670898438, \t Total Dis Loss : 0.00023459845397155732\n",
      "Steps : 108800, \t Total Gen Loss : 29.6298885345459, \t Total Dis Loss : 0.0005858616787008941\n",
      "Steps : 108900, \t Total Gen Loss : 25.822643280029297, \t Total Dis Loss : 0.0006773025961592793\n",
      "Steps : 109000, \t Total Gen Loss : 28.74873924255371, \t Total Dis Loss : 0.0002107348555000499\n",
      "Steps : 109100, \t Total Gen Loss : 28.405258178710938, \t Total Dis Loss : 0.00013495757593773305\n",
      "Steps : 109200, \t Total Gen Loss : 26.394638061523438, \t Total Dis Loss : 0.00028375303372740746\n",
      "Steps : 109300, \t Total Gen Loss : 25.86712646484375, \t Total Dis Loss : 0.00018653410370461643\n",
      "Steps : 109400, \t Total Gen Loss : 25.905574798583984, \t Total Dis Loss : 0.00039157053106464446\n",
      "Steps : 109500, \t Total Gen Loss : 23.120302200317383, \t Total Dis Loss : 9.70973342191428e-05\n",
      "Steps : 109600, \t Total Gen Loss : 25.808103561401367, \t Total Dis Loss : 0.0003259884542785585\n",
      "Steps : 109700, \t Total Gen Loss : 28.308147430419922, \t Total Dis Loss : 9.338270319858566e-05\n",
      "Steps : 109800, \t Total Gen Loss : 24.130189895629883, \t Total Dis Loss : 0.000122176599688828\n",
      "Steps : 109900, \t Total Gen Loss : 28.644227981567383, \t Total Dis Loss : 8.704759238753468e-05\n",
      "Steps : 110000, \t Total Gen Loss : 25.586009979248047, \t Total Dis Loss : 5.236580909695476e-05\n",
      "Steps : 110100, \t Total Gen Loss : 24.24950408935547, \t Total Dis Loss : 0.9063043594360352\n",
      "Steps : 110200, \t Total Gen Loss : 22.48898696899414, \t Total Dis Loss : 0.02179344743490219\n",
      "Steps : 110300, \t Total Gen Loss : 27.218889236450195, \t Total Dis Loss : 0.0001510905713075772\n",
      "Steps : 110400, \t Total Gen Loss : 26.278276443481445, \t Total Dis Loss : 0.00015840836567804217\n",
      "Steps : 110500, \t Total Gen Loss : 28.463258743286133, \t Total Dis Loss : 8.396222983719781e-05\n",
      "Steps : 110600, \t Total Gen Loss : 25.16837501525879, \t Total Dis Loss : 0.0002086818276438862\n",
      "Steps : 110700, \t Total Gen Loss : 30.187206268310547, \t Total Dis Loss : 0.00010392988042440265\n",
      "Steps : 110800, \t Total Gen Loss : 28.629470825195312, \t Total Dis Loss : 2.888713061111048e-05\n",
      "Steps : 110900, \t Total Gen Loss : 25.90011215209961, \t Total Dis Loss : 5.063419303041883e-05\n",
      "Steps : 111000, \t Total Gen Loss : 28.031688690185547, \t Total Dis Loss : 0.00011774326412705705\n",
      "Steps : 111100, \t Total Gen Loss : 24.812496185302734, \t Total Dis Loss : 0.00011804993118857965\n",
      "Steps : 111200, \t Total Gen Loss : 26.84947967529297, \t Total Dis Loss : 8.128248737193644e-05\n",
      "Steps : 111300, \t Total Gen Loss : 27.933429718017578, \t Total Dis Loss : 3.490407470962964e-05\n",
      "Steps : 111400, \t Total Gen Loss : 25.895496368408203, \t Total Dis Loss : 7.062087388476357e-05\n",
      "Steps : 111500, \t Total Gen Loss : 25.147802352905273, \t Total Dis Loss : 4.51263076683972e-05\n",
      "Steps : 111600, \t Total Gen Loss : 28.753816604614258, \t Total Dis Loss : 0.00019903649808838964\n",
      "Steps : 111700, \t Total Gen Loss : 26.286052703857422, \t Total Dis Loss : 0.0002540324639994651\n",
      "Steps : 111800, \t Total Gen Loss : 29.36687660217285, \t Total Dis Loss : 0.0001745347399264574\n",
      "Steps : 111900, \t Total Gen Loss : 28.335506439208984, \t Total Dis Loss : 0.00028420824673958123\n",
      "Steps : 112000, \t Total Gen Loss : 29.01378631591797, \t Total Dis Loss : 0.0001459209161112085\n",
      "Steps : 112100, \t Total Gen Loss : 26.24008560180664, \t Total Dis Loss : 0.000168477141414769\n",
      "Steps : 112200, \t Total Gen Loss : 22.98059844970703, \t Total Dis Loss : 0.00011167643970111385\n",
      "Steps : 112300, \t Total Gen Loss : 28.74360466003418, \t Total Dis Loss : 0.000142568678711541\n",
      "Steps : 112400, \t Total Gen Loss : 30.067794799804688, \t Total Dis Loss : 0.0001120431479648687\n",
      "Steps : 112500, \t Total Gen Loss : 29.1942081451416, \t Total Dis Loss : 3.310831016278826e-05\n",
      "Time for epoch 20 is 369.45240688323975 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 112600, \t Total Gen Loss : 30.279233932495117, \t Total Dis Loss : 7.610543252667412e-05\n",
      "Steps : 112700, \t Total Gen Loss : 29.210214614868164, \t Total Dis Loss : 0.00011593282397370785\n",
      "Steps : 112800, \t Total Gen Loss : 29.150279998779297, \t Total Dis Loss : 2.221885006292723e-05\n",
      "Steps : 112900, \t Total Gen Loss : 28.578189849853516, \t Total Dis Loss : 0.0003090090467594564\n",
      "Steps : 113000, \t Total Gen Loss : 30.029626846313477, \t Total Dis Loss : 0.0013088597916066647\n",
      "Steps : 113100, \t Total Gen Loss : 32.41983413696289, \t Total Dis Loss : 0.00013927866530139\n",
      "Steps : 113200, \t Total Gen Loss : 30.226001739501953, \t Total Dis Loss : 4.026219903607853e-05\n",
      "Steps : 113300, \t Total Gen Loss : 32.169898986816406, \t Total Dis Loss : 6.148919055704027e-05\n",
      "Steps : 113400, \t Total Gen Loss : 24.302698135375977, \t Total Dis Loss : 0.00032107243896462023\n",
      "Steps : 113500, \t Total Gen Loss : 28.99905776977539, \t Total Dis Loss : 5.0578633818076923e-05\n",
      "Steps : 113600, \t Total Gen Loss : 29.397308349609375, \t Total Dis Loss : 6.037369166733697e-05\n",
      "Steps : 113700, \t Total Gen Loss : 30.30475425720215, \t Total Dis Loss : 4.4639680709224194e-05\n",
      "Steps : 113800, \t Total Gen Loss : 29.89365577697754, \t Total Dis Loss : 0.00130254658870399\n",
      "Steps : 113900, \t Total Gen Loss : 30.685440063476562, \t Total Dis Loss : 3.464017572696321e-05\n",
      "Steps : 114000, \t Total Gen Loss : 27.73992919921875, \t Total Dis Loss : 0.0002978866104967892\n",
      "Steps : 114100, \t Total Gen Loss : 30.52181625366211, \t Total Dis Loss : 7.484605885110795e-05\n",
      "Steps : 114200, \t Total Gen Loss : 28.60483741760254, \t Total Dis Loss : 0.0004797367437276989\n",
      "Steps : 114300, \t Total Gen Loss : 30.619853973388672, \t Total Dis Loss : 0.0015426748432219028\n",
      "Steps : 114400, \t Total Gen Loss : 32.249385833740234, \t Total Dis Loss : 0.00036808039294555783\n",
      "Steps : 114500, \t Total Gen Loss : 30.340124130249023, \t Total Dis Loss : 9.710055019240826e-05\n",
      "Steps : 114600, \t Total Gen Loss : 32.114723205566406, \t Total Dis Loss : 0.00034459750168025494\n",
      "Steps : 114700, \t Total Gen Loss : 30.51193618774414, \t Total Dis Loss : 0.04864181950688362\n",
      "Steps : 114800, \t Total Gen Loss : 31.853389739990234, \t Total Dis Loss : 0.00014822804951108992\n",
      "Steps : 114900, \t Total Gen Loss : 30.859533309936523, \t Total Dis Loss : 0.005041028838604689\n",
      "Steps : 115000, \t Total Gen Loss : 22.938175201416016, \t Total Dis Loss : 0.0054825046099722385\n",
      "Steps : 115100, \t Total Gen Loss : 29.511131286621094, \t Total Dis Loss : 0.00014576211106032133\n",
      "Steps : 115200, \t Total Gen Loss : 31.804723739624023, \t Total Dis Loss : 0.00041929204599000514\n",
      "Steps : 115300, \t Total Gen Loss : 31.240188598632812, \t Total Dis Loss : 0.00011270333197899163\n",
      "Steps : 115400, \t Total Gen Loss : 29.100522994995117, \t Total Dis Loss : 0.0002833740727510303\n",
      "Steps : 115500, \t Total Gen Loss : 35.32712173461914, \t Total Dis Loss : 0.0001589574385434389\n",
      "Steps : 115600, \t Total Gen Loss : 28.36411476135254, \t Total Dis Loss : 0.0004260324640199542\n",
      "Steps : 115700, \t Total Gen Loss : 25.577011108398438, \t Total Dis Loss : 0.0018727973802015185\n",
      "Steps : 115800, \t Total Gen Loss : 27.157480239868164, \t Total Dis Loss : 0.000400818680645898\n",
      "Steps : 115900, \t Total Gen Loss : 30.392366409301758, \t Total Dis Loss : 0.0001430124102625996\n",
      "Steps : 116000, \t Total Gen Loss : 28.64706802368164, \t Total Dis Loss : 0.17582589387893677\n",
      "Steps : 116100, \t Total Gen Loss : 32.1167106628418, \t Total Dis Loss : 6.4067033235915e-05\n",
      "Steps : 116200, \t Total Gen Loss : 30.187646865844727, \t Total Dis Loss : 0.00013073423178866506\n",
      "Steps : 116300, \t Total Gen Loss : 24.00057601928711, \t Total Dis Loss : 0.0004002362838946283\n",
      "Steps : 116400, \t Total Gen Loss : 28.654767990112305, \t Total Dis Loss : 8.648479706607759e-05\n",
      "Steps : 116500, \t Total Gen Loss : 29.658950805664062, \t Total Dis Loss : 0.00011313067807350308\n",
      "Steps : 116600, \t Total Gen Loss : 28.87455177307129, \t Total Dis Loss : 0.008839968591928482\n",
      "Steps : 116700, \t Total Gen Loss : 29.026168823242188, \t Total Dis Loss : 0.022436952218413353\n",
      "Steps : 116800, \t Total Gen Loss : 31.024410247802734, \t Total Dis Loss : 0.0001017702161334455\n",
      "Steps : 116900, \t Total Gen Loss : 33.36366653442383, \t Total Dis Loss : 2.6224735847790726e-05\n",
      "Steps : 117000, \t Total Gen Loss : 31.92241668701172, \t Total Dis Loss : 0.00024959261645562947\n",
      "Steps : 117100, \t Total Gen Loss : 29.411714553833008, \t Total Dis Loss : 0.00013205314462538809\n",
      "Steps : 117200, \t Total Gen Loss : 29.29499626159668, \t Total Dis Loss : 8.261794573627412e-05\n",
      "Steps : 117300, \t Total Gen Loss : 27.78217315673828, \t Total Dis Loss : 0.0005612241802737117\n",
      "Steps : 117400, \t Total Gen Loss : 30.23923110961914, \t Total Dis Loss : 0.0008676654542796314\n",
      "Steps : 117500, \t Total Gen Loss : 26.542766571044922, \t Total Dis Loss : 0.0008330239215865731\n",
      "Steps : 117600, \t Total Gen Loss : 28.16234588623047, \t Total Dis Loss : 0.0002525825402699411\n",
      "Steps : 117700, \t Total Gen Loss : 30.74785804748535, \t Total Dis Loss : 1.9178127331542782e-05\n",
      "Steps : 117800, \t Total Gen Loss : 27.259403228759766, \t Total Dis Loss : 0.000717785325832665\n",
      "Steps : 117900, \t Total Gen Loss : 26.821392059326172, \t Total Dis Loss : 0.00016144986147992313\n",
      "Steps : 118000, \t Total Gen Loss : 29.601932525634766, \t Total Dis Loss : 8.268406963907182e-05\n",
      "Steps : 118100, \t Total Gen Loss : 26.56401824951172, \t Total Dis Loss : 0.00032758430461399257\n",
      "Time for epoch 21 is 361.90345644950867 sec\n",
      "Steps : 118200, \t Total Gen Loss : 25.682296752929688, \t Total Dis Loss : 0.0003111352270934731\n",
      "Steps : 118300, \t Total Gen Loss : 23.351356506347656, \t Total Dis Loss : 0.00014731631381437182\n",
      "Steps : 118400, \t Total Gen Loss : 27.366294860839844, \t Total Dis Loss : 0.00015600270126014948\n",
      "Steps : 118500, \t Total Gen Loss : 24.72730827331543, \t Total Dis Loss : 5.02285074617248e-05\n",
      "Steps : 118600, \t Total Gen Loss : 31.525405883789062, \t Total Dis Loss : 7.158637890825048e-05\n",
      "Steps : 118700, \t Total Gen Loss : 27.351337432861328, \t Total Dis Loss : 6.2638457166031e-05\n",
      "Steps : 118800, \t Total Gen Loss : 27.403472900390625, \t Total Dis Loss : 0.00012870690261479467\n",
      "Steps : 118900, \t Total Gen Loss : 27.997074127197266, \t Total Dis Loss : 6.270605808822438e-05\n",
      "Steps : 119000, \t Total Gen Loss : 28.508668899536133, \t Total Dis Loss : 0.0003699310473166406\n",
      "Steps : 119100, \t Total Gen Loss : 31.24519157409668, \t Total Dis Loss : 9.163666982203722e-05\n",
      "Steps : 119200, \t Total Gen Loss : 25.071992874145508, \t Total Dis Loss : 0.0003535557771101594\n",
      "Steps : 119300, \t Total Gen Loss : 24.13782501220703, \t Total Dis Loss : 0.00037091143894940615\n",
      "Steps : 119400, \t Total Gen Loss : 29.021596908569336, \t Total Dis Loss : 5.1872801122954115e-05\n",
      "Steps : 119500, \t Total Gen Loss : 29.023210525512695, \t Total Dis Loss : 2.8108424885431305e-05\n",
      "Steps : 119600, \t Total Gen Loss : 32.49130630493164, \t Total Dis Loss : 0.0001631338964216411\n",
      "Steps : 119700, \t Total Gen Loss : 28.195165634155273, \t Total Dis Loss : 3.7531674024648964e-05\n",
      "Steps : 119800, \t Total Gen Loss : 26.791522979736328, \t Total Dis Loss : 5.26939365954604e-05\n",
      "Steps : 119900, \t Total Gen Loss : 25.336227416992188, \t Total Dis Loss : 0.00012813576904591173\n",
      "Steps : 120000, \t Total Gen Loss : 26.58134651184082, \t Total Dis Loss : 0.00014901437680236995\n",
      "Steps : 120100, \t Total Gen Loss : 24.986536026000977, \t Total Dis Loss : 0.0009744265698827803\n",
      "Steps : 120200, \t Total Gen Loss : 22.494075775146484, \t Total Dis Loss : 0.001551173161715269\n",
      "Steps : 120300, \t Total Gen Loss : 25.533599853515625, \t Total Dis Loss : 0.0002679848112165928\n",
      "Steps : 120400, \t Total Gen Loss : 25.036767959594727, \t Total Dis Loss : 0.00010829127131728455\n",
      "Steps : 120500, \t Total Gen Loss : 26.096593856811523, \t Total Dis Loss : 0.0001366760698147118\n",
      "Steps : 120600, \t Total Gen Loss : 23.927595138549805, \t Total Dis Loss : 0.0007926884572952986\n",
      "Steps : 120700, \t Total Gen Loss : 26.433544158935547, \t Total Dis Loss : 0.000172456813743338\n",
      "Steps : 120800, \t Total Gen Loss : 25.68794059753418, \t Total Dis Loss : 0.00010455146548338234\n",
      "Steps : 120900, \t Total Gen Loss : 25.269832611083984, \t Total Dis Loss : 7.704148447373882e-05\n",
      "Steps : 121000, \t Total Gen Loss : 29.24532127380371, \t Total Dis Loss : 2.492732892278582e-05\n",
      "Steps : 121100, \t Total Gen Loss : 29.269973754882812, \t Total Dis Loss : 7.65612639952451e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 121200, \t Total Gen Loss : 28.7222957611084, \t Total Dis Loss : 0.00012265364057384431\n",
      "Steps : 121300, \t Total Gen Loss : 25.649389266967773, \t Total Dis Loss : 0.0001263809681404382\n",
      "Steps : 121400, \t Total Gen Loss : 26.074434280395508, \t Total Dis Loss : 6.146614032331854e-05\n",
      "Steps : 121500, \t Total Gen Loss : 24.68320655822754, \t Total Dis Loss : 0.0001195345539599657\n",
      "Steps : 121600, \t Total Gen Loss : 26.815343856811523, \t Total Dis Loss : 6.370907067321241e-05\n",
      "Steps : 121700, \t Total Gen Loss : 24.821725845336914, \t Total Dis Loss : 4.529911166173406e-05\n",
      "Steps : 121800, \t Total Gen Loss : 26.079425811767578, \t Total Dis Loss : 7.00697346474044e-05\n",
      "Steps : 121900, \t Total Gen Loss : 24.94619369506836, \t Total Dis Loss : 5.549376510316506e-05\n",
      "Steps : 122000, \t Total Gen Loss : 26.50994110107422, \t Total Dis Loss : 6.499301525764167e-05\n",
      "Steps : 122100, \t Total Gen Loss : 27.789852142333984, \t Total Dis Loss : 2.7586658688960597e-05\n",
      "Steps : 122200, \t Total Gen Loss : 29.12137222290039, \t Total Dis Loss : 7.901909702923149e-05\n",
      "Steps : 122300, \t Total Gen Loss : 27.81871795654297, \t Total Dis Loss : 0.00035789888352155685\n",
      "Steps : 122400, \t Total Gen Loss : 28.222206115722656, \t Total Dis Loss : 2.7404508728068322e-05\n",
      "Steps : 122500, \t Total Gen Loss : 19.446805953979492, \t Total Dis Loss : 0.008313650265336037\n",
      "Steps : 122600, \t Total Gen Loss : 25.870756149291992, \t Total Dis Loss : 0.0004833657294511795\n",
      "Steps : 122700, \t Total Gen Loss : 27.318077087402344, \t Total Dis Loss : 0.006252327933907509\n",
      "Steps : 122800, \t Total Gen Loss : 23.532779693603516, \t Total Dis Loss : 0.00013563661195803434\n",
      "Steps : 122900, \t Total Gen Loss : 28.047687530517578, \t Total Dis Loss : 0.00015857104153838009\n",
      "Steps : 123000, \t Total Gen Loss : 23.561960220336914, \t Total Dis Loss : 0.0001373746490571648\n",
      "Steps : 123100, \t Total Gen Loss : 26.09600067138672, \t Total Dis Loss : 3.979623215855099e-05\n",
      "Steps : 123200, \t Total Gen Loss : 26.963594436645508, \t Total Dis Loss : 0.00012448959751054645\n",
      "Steps : 123300, \t Total Gen Loss : 27.166576385498047, \t Total Dis Loss : 0.00023499346571043134\n",
      "Steps : 123400, \t Total Gen Loss : 26.396469116210938, \t Total Dis Loss : 0.00023504205455537885\n",
      "Steps : 123500, \t Total Gen Loss : 27.192001342773438, \t Total Dis Loss : 0.0003248153952881694\n",
      "Steps : 123600, \t Total Gen Loss : 25.869930267333984, \t Total Dis Loss : 0.0001503818784840405\n",
      "Steps : 123700, \t Total Gen Loss : 23.392641067504883, \t Total Dis Loss : 0.00026583930593915284\n",
      "Time for epoch 22 is 325.5504529476166 sec\n",
      "Steps : 123800, \t Total Gen Loss : 23.7408447265625, \t Total Dis Loss : 0.001013662782497704\n",
      "Steps : 123900, \t Total Gen Loss : 26.619966506958008, \t Total Dis Loss : 0.0001242759171873331\n",
      "Steps : 124000, \t Total Gen Loss : 24.26715850830078, \t Total Dis Loss : 0.0004085685941390693\n",
      "Steps : 124100, \t Total Gen Loss : 24.491823196411133, \t Total Dis Loss : 0.00013058945478405803\n",
      "Steps : 124200, \t Total Gen Loss : 29.237869262695312, \t Total Dis Loss : 0.00012196290481369942\n",
      "Steps : 124300, \t Total Gen Loss : 29.352924346923828, \t Total Dis Loss : 5.364880053093657e-05\n",
      "Steps : 124400, \t Total Gen Loss : 31.190624237060547, \t Total Dis Loss : 7.941371586639434e-05\n",
      "Steps : 124500, \t Total Gen Loss : 28.303665161132812, \t Total Dis Loss : 0.00018722085223998874\n",
      "Steps : 124600, \t Total Gen Loss : 26.602712631225586, \t Total Dis Loss : 0.0007421093760058284\n",
      "Steps : 124700, \t Total Gen Loss : 25.168134689331055, \t Total Dis Loss : 3.298132651252672e-05\n",
      "Steps : 124800, \t Total Gen Loss : 27.476346969604492, \t Total Dis Loss : 3.151738928863779e-05\n",
      "Steps : 124900, \t Total Gen Loss : 25.292184829711914, \t Total Dis Loss : 3.4535041777417064e-05\n",
      "Steps : 125000, \t Total Gen Loss : 24.271703720092773, \t Total Dis Loss : 0.0001936015032697469\n",
      "Steps : 125100, \t Total Gen Loss : 25.63262176513672, \t Total Dis Loss : 7.533362077083439e-05\n",
      "Steps : 125200, \t Total Gen Loss : 26.135528564453125, \t Total Dis Loss : 0.00021416846720967442\n",
      "Steps : 125300, \t Total Gen Loss : 25.742822647094727, \t Total Dis Loss : 5.2833929657936096e-05\n",
      "Steps : 125400, \t Total Gen Loss : 23.21416473388672, \t Total Dis Loss : 0.00018091783567797393\n",
      "Steps : 125500, \t Total Gen Loss : 24.20952033996582, \t Total Dis Loss : 0.0004249623161740601\n",
      "Steps : 125600, \t Total Gen Loss : 26.76519775390625, \t Total Dis Loss : 0.00017747018137015402\n",
      "Steps : 125700, \t Total Gen Loss : 26.36655616760254, \t Total Dis Loss : 5.914697976550087e-05\n",
      "Steps : 125800, \t Total Gen Loss : 30.281475067138672, \t Total Dis Loss : 0.0007259388803504407\n",
      "Steps : 125900, \t Total Gen Loss : 27.681509017944336, \t Total Dis Loss : 0.08718151599168777\n",
      "Steps : 126000, \t Total Gen Loss : 30.727367401123047, \t Total Dis Loss : 7.682990690227598e-05\n",
      "Steps : 126100, \t Total Gen Loss : 28.279296875, \t Total Dis Loss : 3.088967423536815e-05\n",
      "Steps : 126200, \t Total Gen Loss : 25.8312931060791, \t Total Dis Loss : 3.069367812713608e-05\n",
      "Steps : 126300, \t Total Gen Loss : 29.20384979248047, \t Total Dis Loss : 2.4946908524725586e-05\n",
      "Steps : 126400, \t Total Gen Loss : 29.338253021240234, \t Total Dis Loss : 0.0001427709066774696\n",
      "Steps : 126500, \t Total Gen Loss : 33.06355667114258, \t Total Dis Loss : 1.7252841644221917e-05\n",
      "Steps : 126600, \t Total Gen Loss : 29.421070098876953, \t Total Dis Loss : 3.273735273978673e-05\n",
      "Steps : 126700, \t Total Gen Loss : 27.261394500732422, \t Total Dis Loss : 0.00010170302266487852\n",
      "Steps : 126800, \t Total Gen Loss : 28.545682907104492, \t Total Dis Loss : 0.0001162820408353582\n",
      "Steps : 126900, \t Total Gen Loss : 30.289974212646484, \t Total Dis Loss : 8.144223102135584e-05\n",
      "Steps : 127000, \t Total Gen Loss : 25.38165283203125, \t Total Dis Loss : 0.005055435933172703\n",
      "Steps : 127100, \t Total Gen Loss : 25.723546981811523, \t Total Dis Loss : 5.61996603209991e-05\n",
      "Steps : 127200, \t Total Gen Loss : 29.103282928466797, \t Total Dis Loss : 0.00016481558850500733\n",
      "Steps : 127300, \t Total Gen Loss : 29.027416229248047, \t Total Dis Loss : 0.0013644620776176453\n",
      "Steps : 127400, \t Total Gen Loss : 30.551395416259766, \t Total Dis Loss : 0.0006849413039162755\n",
      "Steps : 127500, \t Total Gen Loss : 29.250022888183594, \t Total Dis Loss : 0.0005005128332413733\n",
      "Steps : 127600, \t Total Gen Loss : 25.18097686767578, \t Total Dis Loss : 0.0004452946886885911\n",
      "Steps : 127700, \t Total Gen Loss : 29.08586883544922, \t Total Dis Loss : 6.268674769671634e-05\n",
      "Steps : 127800, \t Total Gen Loss : 26.298383712768555, \t Total Dis Loss : 0.0001290890504606068\n",
      "Steps : 127900, \t Total Gen Loss : 29.126394271850586, \t Total Dis Loss : 5.9811754908878356e-05\n",
      "Steps : 128000, \t Total Gen Loss : 28.375043869018555, \t Total Dis Loss : 0.000376236712327227\n",
      "Steps : 128100, \t Total Gen Loss : 26.72025489807129, \t Total Dis Loss : 3.443440073169768e-05\n",
      "Steps : 128200, \t Total Gen Loss : 27.799367904663086, \t Total Dis Loss : 5.6123073591152206e-05\n",
      "Steps : 128300, \t Total Gen Loss : 27.76345443725586, \t Total Dis Loss : 0.00011400686344131827\n",
      "Steps : 128400, \t Total Gen Loss : 26.120346069335938, \t Total Dis Loss : 9.151759877568111e-05\n",
      "Steps : 128500, \t Total Gen Loss : 26.302106857299805, \t Total Dis Loss : 0.000211221631616354\n",
      "Steps : 128600, \t Total Gen Loss : 25.816137313842773, \t Total Dis Loss : 7.911722059361637e-05\n",
      "Steps : 128700, \t Total Gen Loss : 25.502620697021484, \t Total Dis Loss : 0.0007071393774822354\n",
      "Steps : 128800, \t Total Gen Loss : 30.689376831054688, \t Total Dis Loss : 2.1518553694477305e-05\n",
      "Steps : 128900, \t Total Gen Loss : 27.013710021972656, \t Total Dis Loss : 8.837244240567088e-05\n",
      "Steps : 129000, \t Total Gen Loss : 23.81500244140625, \t Total Dis Loss : 0.00010898908658418804\n",
      "Steps : 129100, \t Total Gen Loss : 23.968759536743164, \t Total Dis Loss : 0.000103379825304728\n",
      "Steps : 129200, \t Total Gen Loss : 25.432863235473633, \t Total Dis Loss : 0.0005788160488009453\n",
      "Steps : 129300, \t Total Gen Loss : 29.1159725189209, \t Total Dis Loss : 1.9700590200955048e-05\n",
      "Time for epoch 23 is 319.5708820819855 sec\n",
      "Steps : 129400, \t Total Gen Loss : 28.847320556640625, \t Total Dis Loss : 0.00010265676974086091\n",
      "Steps : 129500, \t Total Gen Loss : 25.297225952148438, \t Total Dis Loss : 0.0002374265022808686\n",
      "Steps : 129600, \t Total Gen Loss : 29.16452980041504, \t Total Dis Loss : 6.124095671111718e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 129700, \t Total Gen Loss : 27.876903533935547, \t Total Dis Loss : 0.00012193220754852518\n",
      "Steps : 129800, \t Total Gen Loss : 24.589340209960938, \t Total Dis Loss : 0.00013226503506302834\n",
      "Steps : 129900, \t Total Gen Loss : 27.72205924987793, \t Total Dis Loss : 5.124569361214526e-05\n",
      "Steps : 130000, \t Total Gen Loss : 29.0106143951416, \t Total Dis Loss : 6.744437996530905e-05\n",
      "Steps : 130100, \t Total Gen Loss : 27.384052276611328, \t Total Dis Loss : 9.174359001917765e-05\n",
      "Steps : 130200, \t Total Gen Loss : 22.83837890625, \t Total Dis Loss : 0.007761551067233086\n",
      "Steps : 130300, \t Total Gen Loss : 23.71360206604004, \t Total Dis Loss : 0.000684799684677273\n",
      "Steps : 130400, \t Total Gen Loss : 25.816978454589844, \t Total Dis Loss : 0.0002457588561810553\n",
      "Steps : 130500, \t Total Gen Loss : 27.168489456176758, \t Total Dis Loss : 6.300592212937772e-05\n",
      "Steps : 130600, \t Total Gen Loss : 25.55554962158203, \t Total Dis Loss : 0.00016645746654830873\n",
      "Steps : 130700, \t Total Gen Loss : 28.085357666015625, \t Total Dis Loss : 2.547579788370058e-05\n",
      "Steps : 130800, \t Total Gen Loss : 25.472440719604492, \t Total Dis Loss : 8.166478073690087e-05\n",
      "Steps : 130900, \t Total Gen Loss : 24.52217674255371, \t Total Dis Loss : 0.00028771342476829886\n",
      "Steps : 131000, \t Total Gen Loss : 24.790821075439453, \t Total Dis Loss : 8.85813424247317e-05\n",
      "Steps : 131100, \t Total Gen Loss : 23.93023681640625, \t Total Dis Loss : 0.00022805877961218357\n",
      "Steps : 131200, \t Total Gen Loss : 26.595569610595703, \t Total Dis Loss : 0.00013376353308558464\n",
      "Steps : 131300, \t Total Gen Loss : 26.228090286254883, \t Total Dis Loss : 5.9738566051237285e-05\n",
      "Steps : 131400, \t Total Gen Loss : 27.631166458129883, \t Total Dis Loss : 0.00015505700139328837\n",
      "Steps : 131500, \t Total Gen Loss : 26.88897705078125, \t Total Dis Loss : 6.516763824038208e-05\n",
      "Steps : 131600, \t Total Gen Loss : 26.281307220458984, \t Total Dis Loss : 0.0035899383947253227\n",
      "Steps : 131700, \t Total Gen Loss : 30.77235984802246, \t Total Dis Loss : 6.686500273644924e-05\n",
      "Steps : 131800, \t Total Gen Loss : 28.616527557373047, \t Total Dis Loss : 7.705981988692656e-05\n",
      "Steps : 131900, \t Total Gen Loss : 24.322118759155273, \t Total Dis Loss : 0.0002732077264226973\n",
      "Steps : 132000, \t Total Gen Loss : 25.719160079956055, \t Total Dis Loss : 0.0001778214646037668\n",
      "Steps : 132100, \t Total Gen Loss : 29.38627815246582, \t Total Dis Loss : 3.026213016710244e-05\n",
      "Steps : 132200, \t Total Gen Loss : 25.295551300048828, \t Total Dis Loss : 9.882900485536084e-05\n",
      "Steps : 132300, \t Total Gen Loss : 27.582496643066406, \t Total Dis Loss : 6.024852336850017e-05\n",
      "Steps : 132400, \t Total Gen Loss : 26.61141586303711, \t Total Dis Loss : 0.0016523855738341808\n",
      "Steps : 132500, \t Total Gen Loss : 25.969072341918945, \t Total Dis Loss : 0.00010730679787229747\n",
      "Steps : 132600, \t Total Gen Loss : 22.355396270751953, \t Total Dis Loss : 0.0025110263377428055\n",
      "Steps : 132700, \t Total Gen Loss : 27.271102905273438, \t Total Dis Loss : 3.480303712422028e-05\n",
      "Steps : 132800, \t Total Gen Loss : 26.442428588867188, \t Total Dis Loss : 2.1573998310486786e-05\n",
      "Steps : 132900, \t Total Gen Loss : 27.944076538085938, \t Total Dis Loss : 2.8664639103226364e-05\n",
      "Steps : 133000, \t Total Gen Loss : 26.252553939819336, \t Total Dis Loss : 4.0604711102787405e-05\n",
      "Steps : 133100, \t Total Gen Loss : 29.68204116821289, \t Total Dis Loss : 0.02336994931101799\n",
      "Steps : 133200, \t Total Gen Loss : 27.589771270751953, \t Total Dis Loss : 7.720768917351961e-05\n",
      "Steps : 133300, \t Total Gen Loss : 24.773149490356445, \t Total Dis Loss : 0.0003842397127300501\n",
      "Steps : 133400, \t Total Gen Loss : 22.97117042541504, \t Total Dis Loss : 7.456369348801672e-05\n",
      "Steps : 133500, \t Total Gen Loss : 25.833633422851562, \t Total Dis Loss : 0.00016582119860686362\n",
      "Steps : 133600, \t Total Gen Loss : 27.961748123168945, \t Total Dis Loss : 0.00015847687609493732\n",
      "Steps : 133700, \t Total Gen Loss : 25.634111404418945, \t Total Dis Loss : 3.802263381658122e-05\n",
      "Steps : 133800, \t Total Gen Loss : 25.668859481811523, \t Total Dis Loss : 0.00016604851407464594\n",
      "Steps : 133900, \t Total Gen Loss : 27.880176544189453, \t Total Dis Loss : 0.00010488973202882335\n",
      "Steps : 134000, \t Total Gen Loss : 28.962757110595703, \t Total Dis Loss : 0.00010701065184548497\n",
      "Steps : 134100, \t Total Gen Loss : 28.46303939819336, \t Total Dis Loss : 0.00014796463074162602\n",
      "Steps : 134200, \t Total Gen Loss : 29.107210159301758, \t Total Dis Loss : 0.0007109855650924146\n",
      "Steps : 134300, \t Total Gen Loss : 28.999797821044922, \t Total Dis Loss : 0.0002612435491755605\n",
      "Steps : 134400, \t Total Gen Loss : 30.565296173095703, \t Total Dis Loss : 1.1433250620029867e-05\n",
      "Steps : 134500, \t Total Gen Loss : 29.160442352294922, \t Total Dis Loss : 0.0001274026435567066\n",
      "Steps : 134600, \t Total Gen Loss : 27.817060470581055, \t Total Dis Loss : 6.642616062890738e-05\n",
      "Steps : 134700, \t Total Gen Loss : 30.470279693603516, \t Total Dis Loss : 4.806196739082225e-05\n",
      "Steps : 134800, \t Total Gen Loss : 25.95709228515625, \t Total Dis Loss : 0.00011884085688507184\n",
      "Steps : 134900, \t Total Gen Loss : 27.82962417602539, \t Total Dis Loss : 0.00012278079520910978\n",
      "Steps : 135000, \t Total Gen Loss : 26.58733367919922, \t Total Dis Loss : 9.186654642689973e-05\n",
      "Time for epoch 24 is 333.43009519577026 sec\n",
      "Steps : 135100, \t Total Gen Loss : 29.962644577026367, \t Total Dis Loss : 2.619734368636273e-05\n",
      "Steps : 135200, \t Total Gen Loss : 28.503629684448242, \t Total Dis Loss : 2.8518104954855517e-05\n",
      "Steps : 135300, \t Total Gen Loss : 31.100341796875, \t Total Dis Loss : 3.323594864923507e-05\n",
      "Steps : 135400, \t Total Gen Loss : 26.552291870117188, \t Total Dis Loss : 0.0024748235009610653\n",
      "Steps : 135500, \t Total Gen Loss : 29.968887329101562, \t Total Dis Loss : 4.757239366881549e-06\n",
      "Steps : 135600, \t Total Gen Loss : 31.035900115966797, \t Total Dis Loss : 4.61568515675026e-06\n",
      "Steps : 135700, \t Total Gen Loss : 26.700590133666992, \t Total Dis Loss : 9.791235788725317e-05\n",
      "Steps : 135800, \t Total Gen Loss : 28.80023193359375, \t Total Dis Loss : 2.1987678337609395e-05\n",
      "Steps : 135900, \t Total Gen Loss : 25.441478729248047, \t Total Dis Loss : 3.136119630653411e-05\n",
      "Steps : 136000, \t Total Gen Loss : 26.421382904052734, \t Total Dis Loss : 0.00013207872689235955\n",
      "Steps : 136100, \t Total Gen Loss : 28.465614318847656, \t Total Dis Loss : 0.00011634414113359526\n",
      "Steps : 136200, \t Total Gen Loss : 24.370723724365234, \t Total Dis Loss : 0.0009780412074178457\n",
      "Steps : 136300, \t Total Gen Loss : 25.343799591064453, \t Total Dis Loss : 0.00021642261708620936\n",
      "Steps : 136400, \t Total Gen Loss : 31.555959701538086, \t Total Dis Loss : 1.9510003767209128e-05\n",
      "Steps : 136500, \t Total Gen Loss : 27.32552719116211, \t Total Dis Loss : 8.594156679464504e-05\n",
      "Steps : 136600, \t Total Gen Loss : 25.09016227722168, \t Total Dis Loss : 0.000655328156426549\n",
      "Steps : 136700, \t Total Gen Loss : 26.77557945251465, \t Total Dis Loss : 0.00011167592310812324\n",
      "Steps : 136800, \t Total Gen Loss : 37.69507598876953, \t Total Dis Loss : 6.402842700481415e-05\n",
      "Steps : 136900, \t Total Gen Loss : 30.627033233642578, \t Total Dis Loss : 0.0002264982322230935\n",
      "Steps : 137000, \t Total Gen Loss : 28.764020919799805, \t Total Dis Loss : 0.004231515806168318\n",
      "Steps : 137100, \t Total Gen Loss : 31.014366149902344, \t Total Dis Loss : 3.111537080258131e-05\n",
      "Steps : 137200, \t Total Gen Loss : 26.582216262817383, \t Total Dis Loss : 0.00136516650673002\n",
      "Steps : 137300, \t Total Gen Loss : 29.96117401123047, \t Total Dis Loss : 0.00018387807358521968\n",
      "Steps : 137400, \t Total Gen Loss : 27.984582901000977, \t Total Dis Loss : 0.0001771031820680946\n",
      "Steps : 137500, \t Total Gen Loss : 27.121387481689453, \t Total Dis Loss : 0.00011451746104285121\n",
      "Steps : 137600, \t Total Gen Loss : 28.736783981323242, \t Total Dis Loss : 0.00013635787763632834\n",
      "Steps : 137700, \t Total Gen Loss : 26.746273040771484, \t Total Dis Loss : 0.0001535364572191611\n",
      "Steps : 137800, \t Total Gen Loss : 29.12594985961914, \t Total Dis Loss : 3.952930273953825e-05\n",
      "Steps : 137900, \t Total Gen Loss : 29.238967895507812, \t Total Dis Loss : 0.00010714752716012299\n",
      "Steps : 138000, \t Total Gen Loss : 26.970458984375, \t Total Dis Loss : 0.00033873756183311343\n",
      "Steps : 138100, \t Total Gen Loss : 25.49669647216797, \t Total Dis Loss : 0.00012812182831112295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 138200, \t Total Gen Loss : 26.482406616210938, \t Total Dis Loss : 9.770475298864767e-05\n",
      "Steps : 138300, \t Total Gen Loss : 25.78363800048828, \t Total Dis Loss : 0.0006137649179436266\n",
      "Steps : 138400, \t Total Gen Loss : 23.864137649536133, \t Total Dis Loss : 0.00011878243822138757\n",
      "Steps : 138500, \t Total Gen Loss : 25.507266998291016, \t Total Dis Loss : 0.00018748128786683083\n",
      "Steps : 138600, \t Total Gen Loss : 26.97260284423828, \t Total Dis Loss : 0.0007385098142549396\n",
      "Steps : 138700, \t Total Gen Loss : 29.88854217529297, \t Total Dis Loss : 0.00016872186097316444\n",
      "Steps : 138800, \t Total Gen Loss : 28.90036392211914, \t Total Dis Loss : 2.679358840396162e-05\n",
      "Steps : 138900, \t Total Gen Loss : 26.584815979003906, \t Total Dis Loss : 0.00018976580759044737\n",
      "Steps : 139000, \t Total Gen Loss : 25.726959228515625, \t Total Dis Loss : 0.00012490974040701985\n",
      "Steps : 139100, \t Total Gen Loss : 21.037137985229492, \t Total Dis Loss : 0.011985791847109795\n",
      "Steps : 139200, \t Total Gen Loss : 27.924034118652344, \t Total Dis Loss : 9.635486640036106e-05\n",
      "Steps : 139300, \t Total Gen Loss : 27.40906524658203, \t Total Dis Loss : 0.00010525262041483074\n",
      "Steps : 139400, \t Total Gen Loss : 27.499780654907227, \t Total Dis Loss : 4.3173637095605955e-05\n",
      "Steps : 139500, \t Total Gen Loss : 31.06113624572754, \t Total Dis Loss : 3.3052649087039754e-05\n",
      "Steps : 139600, \t Total Gen Loss : 28.07512664794922, \t Total Dis Loss : 3.9461701817344874e-05\n",
      "Steps : 139700, \t Total Gen Loss : 30.751651763916016, \t Total Dis Loss : 3.4131575375795364e-05\n",
      "Steps : 139800, \t Total Gen Loss : 23.719406127929688, \t Total Dis Loss : 0.0003071035898756236\n",
      "Steps : 139900, \t Total Gen Loss : 25.47496223449707, \t Total Dis Loss : 9.310537279816344e-05\n",
      "Steps : 140000, \t Total Gen Loss : 23.30021858215332, \t Total Dis Loss : 0.00020995084196329117\n",
      "Steps : 140100, \t Total Gen Loss : 23.534881591796875, \t Total Dis Loss : 0.00015618758334312588\n",
      "Steps : 140200, \t Total Gen Loss : 24.834976196289062, \t Total Dis Loss : 0.00043648522114381194\n",
      "Steps : 140300, \t Total Gen Loss : 28.0158748626709, \t Total Dis Loss : 0.00014627931523136795\n",
      "Steps : 140400, \t Total Gen Loss : 27.61552619934082, \t Total Dis Loss : 6.452786328736693e-05\n",
      "Steps : 140500, \t Total Gen Loss : 26.031620025634766, \t Total Dis Loss : 9.04172848095186e-05\n",
      "Steps : 140600, \t Total Gen Loss : 25.751178741455078, \t Total Dis Loss : 0.0006654023309238255\n",
      "Time for epoch 25 is 337.29620265960693 sec\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 25\n",
    "steps = 0\n",
    "checkpoint_prefix = os.path.join(checkpoint_path, \"ckpt\")\n",
    "for epoch in range(max_epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for images, labels in train_dataset:\n",
    "        steps += 1\n",
    "        gen_loss, disc_loss = train_step(images)\n",
    "        \n",
    "        if steps % 100 == 0:\n",
    "            print ('Steps : {}, \\t Total Gen Loss : {}, \\t Total Dis Loss : {}'.format(steps, gen_loss.numpy(), disc_loss.numpy()))\n",
    "        \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        \n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f5514e5ba10>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate(test_dataset, set_lambda=0.9):\n",
    "    an_scores = []\n",
    "    gt_labels = []\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(test_dataset):\n",
    "        generated_images = generator(x_batch_train, training=True)\n",
    "        _, feat_real = discriminator(x_batch_train, training=True)\n",
    "        _, feat_fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        generated_images, feat_real, feat_fake = generated_images.numpy(), feat_real.numpy(), feat_fake.numpy()        \n",
    "\n",
    "        rec = abs(x_batch_train - generated_images)\n",
    "        lat = (feat_real - feat_fake) ** 2\n",
    "\n",
    "        rec = tf.reduce_sum(rec, [1,2,3])\n",
    "        lat = tf.reduce_sum(lat, [1,2,3])\n",
    "        \n",
    "        error = (set_lambda * tf.cast(rec, tf.float32)) + ((1 - set_lambda) * tf.cast(lat, tf.float32))\n",
    "        \n",
    "        an_scores.append(error)\n",
    "        gt_labels.append(y_batch_train)\n",
    "        \n",
    "    an_scores = np.concatenate(an_scores, axis=0).reshape([-1])\n",
    "    gt_labels = np.concatenate(gt_labels, axis=0).reshape([-1])\n",
    "    \n",
    "    an_scores = (an_scores - np.amin(an_scores)) / (np.amax(an_scores) - np.amin(an_scores))\n",
    "    \n",
    "    return an_scores, gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000 15000\n"
     ]
    }
   ],
   "source": [
    "an_scores, gt_labels = _evaluate(test_dataset)\n",
    "\n",
    "print(len(an_scores), len(gt_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000,)\n",
      "(6000,)\n"
     ]
    }
   ],
   "source": [
    "normal = []\n",
    "anormaly = []\n",
    "for score, label in zip(an_scores, gt_labels):\n",
    "    if label == 0:\n",
    "        anormaly.append(score)\n",
    "    else:\n",
    "        normal.append(score)\n",
    "\n",
    "normal = np.array(normal)\n",
    "print(normal.shape)\n",
    "anormaly = np.array(anormaly)\n",
    "print(anormaly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(normal, bins=np.linspace(0.0, 1.0, num=100))\n",
    "plt.hist(anormaly, bins=np.linspace(0.0, 1.0, num=100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33164513 0.31041336\n",
      "0.1282906 0.124317914\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(normal, norm.pdf(normal, np.mean(normal), np.std(normal)), 'o')\n",
    "plt.plot(anormaly, norm.pdf(anormaly, np.mean(anormaly), np.std(anormaly)), 'o')\n",
    "\n",
    "print(np.mean(normal), np.mean(anormaly))\n",
    "print(np.std(normal), np.std(anormaly))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
