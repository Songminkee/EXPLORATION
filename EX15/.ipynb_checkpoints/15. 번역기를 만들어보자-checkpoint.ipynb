{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문자수준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 178009\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>I'm naked.</td>\n",
       "      <td>Je suis nue.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143111</th>\n",
       "      <td>I want to know how you feel about this.</td>\n",
       "      <td>Je veux savoir ce que tu ressens à ce sujet.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125163</th>\n",
       "      <td>A lot of guys around here hate you.</td>\n",
       "      <td>Beaucoup de gens du coin te détestent.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161125</th>\n",
       "      <td>He had scarcely escaped when he was recaptured.</td>\n",
       "      <td>À peine s'était-il échappé qu'il fut recapturé.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171644</th>\n",
       "      <td>Jobs are hard to come by with so many people o...</td>\n",
       "      <td>Les emplois sont difficiles à trouver avec tan...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      eng  \\\n",
       "760                                            I'm naked.   \n",
       "143111            I want to know how you feel about this.   \n",
       "125163                A lot of guys around here hate you.   \n",
       "161125    He had scarcely escaped when he was recaptured.   \n",
       "171644  Jobs are hard to come by with so many people o...   \n",
       "\n",
       "                                                      fra  \\\n",
       "760                                          Je suis nue.   \n",
       "143111       Je veux savoir ce que tu ressens à ce sujet.   \n",
       "125163             Beaucoup de gens du coin te détestent.   \n",
       "161125    À peine s'était-il échappé qu'il fut recapturé.   \n",
       "171644  Les emplois sont difficiles à trouver avec tan...   \n",
       "\n",
       "                                                       cc  \n",
       "760     CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "143111  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "125163  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "161125  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "171644  CC-BY 2.0 (France) Attribution: tatoeba.org #5...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5) #샘플 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30351</th>\n",
       "      <td>I wonder who she is.</td>\n",
       "      <td>Je me demande qui elle est.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12097</th>\n",
       "      <td>Look who's here.</td>\n",
       "      <td>Regarde qui est là.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33367</th>\n",
       "      <td>You can't stay here.</td>\n",
       "      <td>Tu ne peux pas rester ici.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11098</th>\n",
       "      <td>I know your son.</td>\n",
       "      <td>Je connais ton fils.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12015</th>\n",
       "      <td>Keep on smiling.</td>\n",
       "      <td>Reste souriante.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        eng                          fra\n",
       "30351  I wonder who she is.  Je me demande qui elle est.\n",
       "12097      Look who's here.          Regarde qui est là.\n",
       "33367  You can't stay here.   Tu ne peux pas rester ici.\n",
       "11098      I know your son.         Je connais ton fils.\n",
       "12015      Keep on smiling.             Reste souriante."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[['eng', 'fra']][:50000] # 5만개 샘플 사용\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43009</th>\n",
       "      <td>I'm happy you're here.</td>\n",
       "      <td>\\t Je suis heureux que vous soyez là. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45102</th>\n",
       "      <td>Tom is an unusual kid.</td>\n",
       "      <td>\\t Tom est un enfant inhabituel. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44173</th>\n",
       "      <td>She's too old for you.</td>\n",
       "      <td>\\t Elle est trop vieille pour toi. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18011</th>\n",
       "      <td>Am I welcome here?</td>\n",
       "      <td>\\t Suis-je le bienvenu ici ? \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39287</th>\n",
       "      <td>What's the good news?</td>\n",
       "      <td>\\t Quelle est la bonne nouvelle ? \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          eng                                       fra\n",
       "43009  I'm happy you're here.  \\t Je suis heureux que vous soyez là. \\n\n",
       "45102  Tom is an unusual kid.       \\t Tom est un enfant inhabituel. \\n\n",
       "44173  She's too old for you.     \\t Elle est trop vieille pour toi. \\n\n",
       "18011      Am I welcome here?           \\t Suis-je le bienvenu ici ? \\n\n",
       "39287   What's the good news?      \\t Quelle est la bonne nouvelle ? \\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰 추가\n",
    "sos_token = '\\t'\n",
    "eos_token = '\\n'\n",
    "lines.fra = lines.fra.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[19, 3, 8], [10, 5, 8], [10, 5, 8]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tokenizer = Tokenizer(char_level=True)   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "eng_tokenizer.fit_on_texts(lines.eng)               # 50000개의 행을 가진 eng의 각 행에 토큰화를 수행\n",
    "input_text = eng_tokenizer.texts_to_sequences(lines.eng)    # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "input_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[11, 1, 19, 4, 1, 33, 1, 12],\n",
       " [11, 1, 3, 4, 13, 7, 5, 1, 33, 1, 12],\n",
       " [11, 1, 3, 4, 13, 7, 5, 14, 1, 12]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fra_tokenizer = Tokenizer(char_level=True)   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "fra_tokenizer.fit_on_texts(lines.fra)                 # 50000개의 행을 가진 fra의 각 행에 토큰화를 수행\n",
    "target_text = fra_tokenizer.texts_to_sequences(lines.fra)     # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "target_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장의 크기 : 51\n",
      "프랑스어 단어장의 크기 : 73\n"
     ]
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 시퀀스의 최대 길이 23\n",
      "프랑스어 시퀀스의 최대 길이 76\n"
     ]
    }
   ],
   "source": [
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 50000\n",
      "영어 단어장의 크기 : 51\n",
      "프랑스어 단어장의 크기 : 73\n",
      "영어 시퀀스의 최대 길이 23\n",
      "프랑스어 시퀀스의 최대 길이 76\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = input_text\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [[ char for char in line if char != fra_tokenizer.word_index[eos_token] ] for line in target_text] \n",
    "# 시작 토큰 제거\n",
    "decoder_target = [[ char for char in line if char != fra_tokenizer.word_index[sos_token] ] for line in target_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11, 1, 19, 4, 1, 33, 1], [11, 1, 3, 4, 13, 7, 5, 1, 33, 1], [11, 1, 3, 4, 13, 7, 5, 14, 1]]\n",
      "[[1, 19, 4, 1, 33, 1, 12], [1, 3, 4, 13, 7, 5, 1, 33, 1, 12], [1, 3, 4, 13, 7, 5, 14, 1, 12]]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input[:3])\n",
    "print(decoder_target[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (50000, 23)\n",
      "프랑스어 입력데이터의 크기(shape) : (50000, 76)\n",
      "프랑스어 출력데이터의 크기(shape) : (50000, 76)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19  3  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (50000, 23, 51)\n",
      "프랑스어 입력데이터의 크기(shape) : (50000, 76, 73)\n",
      "프랑스어 출력데이터의 크기(shape) : (50000, 76, 73)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 학습데이터의 크기(shape) : (50000, 23, 51)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (50000, 76, 73)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (50000, 76, 73)\n"
     ]
    }
   ],
   "source": [
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "print('⏳')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 텐서 생성.\n",
    "encoder_inputs = Input(shape=(None, eng_vocab_size))\n",
    "# hidden size가 256인 인코더의 LSTM 셀 생성\n",
    "encoder_lstm = LSTM(units = 256, return_state = True)\n",
    "# 디코더로 전달할 hidden state, cell state를 리턴. encoder_outputs은 여기서는 불필요.\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# hidden state와 cell state를 다음 time step으로 전달하기 위해서 별도 저장.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 텐서 생성.\n",
    "decoder_inputs = Input(shape=(None, fra_vocab_size))\n",
    "# hidden size가 256인 인코더의 LSTM 셀 생성\n",
    "decoder_lstm = LSTM(units = 256, return_sequences = True, return_state=True)\n",
    "# decoder_outputs는 모든 time step의 hidden state\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state = encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 51)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 73)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 315392      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  337920      input_2[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 73)     18761       lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 672,073\n",
      "Trainable params: 672,073\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.9296 - val_loss: 0.7754\n",
      "Epoch 2/50\n",
      "368/368 [==============================] - 5s 15ms/step - loss: 0.5606 - val_loss: 0.6547\n",
      "Epoch 3/50\n",
      "368/368 [==============================] - 6s 15ms/step - loss: 0.4658 - val_loss: 0.5699\n",
      "Epoch 4/50\n",
      "368/368 [==============================] - 5s 15ms/step - loss: 0.4101 - val_loss: 0.5126\n",
      "Epoch 5/50\n",
      "368/368 [==============================] - 5s 15ms/step - loss: 0.3730 - val_loss: 0.4796\n",
      "Epoch 6/50\n",
      "368/368 [==============================] - 5s 15ms/step - loss: 0.3451 - val_loss: 0.4491\n",
      "Epoch 7/50\n",
      "368/368 [==============================] - 5s 15ms/step - loss: 0.3245 - val_loss: 0.4370\n",
      "Epoch 8/50\n",
      "368/368 [==============================] - 6s 15ms/step - loss: 0.3083 - val_loss: 0.4177\n",
      "Epoch 9/50\n",
      "368/368 [==============================] - 5s 15ms/step - loss: 0.2947 - val_loss: 0.4099\n",
      "Epoch 10/50\n",
      "368/368 [==============================] - 5s 15ms/step - loss: 0.2834 - val_loss: 0.4100\n",
      "Epoch 11/50\n",
      "368/368 [==============================] - 5s 15ms/step - loss: 0.2734 - val_loss: 0.3991\n",
      "Epoch 12/50\n",
      "368/368 [==============================] - 5s 15ms/step - loss: 0.2649 - val_loss: 0.3896\n",
      "Epoch 13/50\n",
      "368/368 [==============================] - 6s 15ms/step - loss: 0.2574 - val_loss: 0.3770\n",
      "Epoch 14/50\n",
      "368/368 [==============================] - 5s 15ms/step - loss: 0.2506 - val_loss: 0.3765\n",
      "Epoch 15/50\n",
      "368/368 [==============================] - 5s 15ms/step - loss: 0.2443 - val_loss: 0.3745\n",
      "Epoch 16/50\n",
      "368/368 [==============================] - 5s 15ms/step - loss: 0.2386 - val_loss: 0.3725\n",
      "Epoch 17/50\n",
      "368/368 [==============================] - 5s 15ms/step - loss: 0.2335 - val_loss: 0.3707\n",
      "Epoch 18/50\n",
      "368/368 [==============================] - 6s 15ms/step - loss: 0.2287 - val_loss: 0.3655\n",
      "Epoch 19/50\n",
      "368/368 [==============================] - 6s 15ms/step - loss: 0.2241 - val_loss: 0.3644\n",
      "Epoch 20/50\n",
      "368/368 [==============================] - 6s 15ms/step - loss: 0.2198 - val_loss: 0.3649\n",
      "Epoch 21/50\n",
      "368/368 [==============================] - 5s 15ms/step - loss: 0.2158 - val_loss: 0.3677\n",
      "Epoch 22/50\n",
      "368/368 [==============================] - 5s 15ms/step - loss: 0.2119 - val_loss: 0.3623\n",
      "Epoch 23/50\n",
      "368/368 [==============================] - 5s 15ms/step - loss: 0.2083 - val_loss: 0.3640\n",
      "Epoch 24/50\n",
      "368/368 [==============================] - 5s 15ms/step - loss: 0.2047 - val_loss: 0.3641\n",
      "Epoch 25/50\n",
      "368/368 [==============================] - 6s 15ms/step - loss: 0.2015 - val_loss: 0.3723\n",
      "Epoch 26/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1983 - val_loss: 0.3628\n",
      "Epoch 27/50\n",
      "368/368 [==============================] - 6s 15ms/step - loss: 0.1954 - val_loss: 0.3651\n",
      "Epoch 28/50\n",
      "368/368 [==============================] - 5s 15ms/step - loss: 0.1924 - val_loss: 0.3653\n",
      "Epoch 29/50\n",
      "368/368 [==============================] - 5s 15ms/step - loss: 0.1896 - val_loss: 0.3733\n",
      "Epoch 30/50\n",
      "368/368 [==============================] - 5s 15ms/step - loss: 0.1869 - val_loss: 0.3727\n",
      "Epoch 31/50\n",
      "368/368 [==============================] - 5s 15ms/step - loss: 0.1844 - val_loss: 0.3721\n",
      "Epoch 32/50\n",
      "368/368 [==============================] - 5s 15ms/step - loss: 0.1819 - val_loss: 0.3714\n",
      "Epoch 33/50\n",
      "368/368 [==============================] - 6s 15ms/step - loss: 0.1796 - val_loss: 0.3733\n",
      "Epoch 34/50\n",
      "368/368 [==============================] - 5s 15ms/step - loss: 0.1772 - val_loss: 0.3706\n",
      "Epoch 35/50\n",
      "368/368 [==============================] - 6s 15ms/step - loss: 0.1749 - val_loss: 0.3737\n",
      "Epoch 36/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1726 - val_loss: 0.3739\n",
      "Epoch 37/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1705 - val_loss: 0.3826\n",
      "Epoch 38/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1684 - val_loss: 0.3817\n",
      "Epoch 39/50\n",
      "368/368 [==============================] - 5s 15ms/step - loss: 0.1665 - val_loss: 0.3842\n",
      "Epoch 40/50\n",
      "368/368 [==============================] - 5s 15ms/step - loss: 0.1650 - val_loss: 0.3816\n",
      "Epoch 41/50\n",
      "368/368 [==============================] - 6s 15ms/step - loss: 0.1625 - val_loss: 0.3901\n",
      "Epoch 42/50\n",
      "368/368 [==============================] - 6s 15ms/step - loss: 0.1608 - val_loss: 0.3888\n",
      "Epoch 43/50\n",
      "368/368 [==============================] - 5s 15ms/step - loss: 0.1590 - val_loss: 0.3861\n",
      "Epoch 44/50\n",
      "368/368 [==============================] - 6s 15ms/step - loss: 0.1572 - val_loss: 0.3920\n",
      "Epoch 45/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1555 - val_loss: 0.4006\n",
      "Epoch 46/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1539 - val_loss: 0.3984\n",
      "Epoch 47/50\n",
      "368/368 [==============================] - 6s 15ms/step - loss: 0.1523 - val_loss: 0.4006\n",
      "Epoch 48/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1507 - val_loss: 0.4031\n",
      "Epoch 49/50\n",
      "368/368 [==============================] - 5s 15ms/step - loss: 0.1493 - val_loss: 0.4046\n",
      "Epoch 50/50\n",
      "368/368 [==============================] - 5s 15ms/step - loss: 0.1477 - val_loss: 0.4040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efbffa5f4d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, 51)]        0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 256), (None, 256) 315392    \n",
      "=================================================================\n",
      "Total params: 315,392\n",
      "Trainable params: 315,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 time step의 hidden state를 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "# 이전 time step의 cell state를 저장하는 텐서\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "# 이전 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# decoder_states_inputs를 현재 time step의 초기 상태로 사용.\n",
    "# 구체적인 동작 자체는 def decode_sequence()에 구현.\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state = decoder_states_inputs)\n",
    "# 현재 time step의 hidden state와 cell state를 하나의 변수에 저장.\n",
    "decoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None, 73)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  337920      input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 73)     18761       lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 356,681\n",
      "Trainable params: 356,681\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "    target_seq[0, 0, fra2idx['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: Run!\n",
      "정답 문장:  Cours ! \n",
      "번역기가 번역한 문장:  courez-vous ! \n",
      "-----------------------------------\n",
      "입력 문장: I left.\n",
      "정답 문장:  Je suis partie. \n",
      "번역기가 번역한 문장:  je suis partie. \n",
      "-----------------------------------\n",
      "입력 문장: Call us.\n",
      "정답 문장:  Appelez-nous ! \n",
      "번역기가 번역한 문장:  appelle-nous ! \n",
      "-----------------------------------\n",
      "입력 문장: How nice!\n",
      "정답 문장:  Comme c'est gentil ! \n",
      "번역기가 번역한 문장:  comme c'est ignortai ! \n",
      "-----------------------------------\n",
      "입력 문장: Turn left.\n",
      "정답 문장:  Tourne à gauche. \n",
      "번역기가 번역한 문장:  tourne la porte ! \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스 (자유롭게 선택해 보세요)\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.eng[seq_index])\n",
    "    print('정답 문장:', lines.fra[seq_index][1:len(lines.fra[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어 수준 - 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 178009\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101617</th>\n",
       "      <td>What do you have in your hand?</td>\n",
       "      <td>Qu'as-tu dans la main ?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132520</th>\n",
       "      <td>She believes her son is still alive.</td>\n",
       "      <td>Elle croit que son fils est encore vivant.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101760</th>\n",
       "      <td>When did you build your house?</td>\n",
       "      <td>Quand avez-vous construit votre maison ?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85085</th>\n",
       "      <td>I would like some envelopes.</td>\n",
       "      <td>Je voudrais quelques enveloppes.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128767</th>\n",
       "      <td>This movie is highly controversial.</td>\n",
       "      <td>Ce film est très polémique.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         eng  \\\n",
       "101617        What do you have in your hand?   \n",
       "132520  She believes her son is still alive.   \n",
       "101760        When did you build your house?   \n",
       "85085           I would like some envelopes.   \n",
       "128767   This movie is highly controversial.   \n",
       "\n",
       "                                               fra  \\\n",
       "101617                     Qu'as-tu dans la main ?   \n",
       "132520  Elle croit que son fils est encore vivant.   \n",
       "101760    Quand avez-vous construit votre maison ?   \n",
       "85085             Je voudrais quelques enveloppes.   \n",
       "128767                 Ce film est très polémique.   \n",
       "\n",
       "                                                       cc  \n",
       "101617  CC-BY 2.0 (France) Attribution: tatoeba.org #6...  \n",
       "132520  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "101760  CC-BY 2.0 (France) Attribution: tatoeba.org #6...  \n",
       "85085   CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
       "128767  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5) #샘플 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22872</th>\n",
       "      <td>Your face is pale.</td>\n",
       "      <td>Ton visage est pâle.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10428</th>\n",
       "      <td>Don't run risks.</td>\n",
       "      <td>Ne courez pas de risques.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37929</th>\n",
       "      <td>That's not how it is.</td>\n",
       "      <td>Ce n'est pas ainsi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23655</th>\n",
       "      <td>He is about my age.</td>\n",
       "      <td>Il a environ mon âge.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7810</th>\n",
       "      <td>How did you do?</td>\n",
       "      <td>Comment avez-vous fait ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         eng                        fra\n",
       "22872     Your face is pale.       Ton visage est pâle.\n",
       "10428       Don't run risks.  Ne courez pas de risques.\n",
       "37929  That's not how it is.        Ce n'est pas ainsi.\n",
       "23655    He is about my age.      Il a environ mon âge.\n",
       "7810         How did you do?   Comment avez-vous fait ?"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[['eng', 'fra']][:50000] # 5만개 샘플 사용\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "lines.eng = lines.eng.apply(lambda x: x.lower())\n",
    "lines.fra = lines.fra.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22549</th>\n",
       "      <td>[you, may, go, in, now, .]</td>\n",
       "      <td>vous pouvez entrer, maintenant.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12224</th>\n",
       "      <td>[open, your, mouth, !]</td>\n",
       "      <td>ouvrez la bouche !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3343</th>\n",
       "      <td>[come, help, me, .]</td>\n",
       "      <td>viens me donner un coup de main.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26360</th>\n",
       "      <td>[there, was, a, winner, .]</td>\n",
       "      <td>il y a eu un vainqueur.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47902</th>\n",
       "      <td>[how, are, things, at, home, ?]</td>\n",
       "      <td>comment vont les choses, à la maison ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   eng                                     fra\n",
       "22549       [you, may, go, in, now, .]         vous pouvez entrer, maintenant.\n",
       "12224           [open, your, mouth, !]                      ouvrez la bouche !\n",
       "3343               [come, help, me, .]        viens me donner un coup de main.\n",
       "26360       [there, was, a, winner, .]                 il y a eu un vainqueur.\n",
       "47902  [how, are, things, at, home, ?]  comment vont les choses, à la maison ?"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.eng = lines.eng.apply(lambda x: re.findall(r\"[\\w']+|[.,!?;]\", x))\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7755</th>\n",
       "      <td>[he, was, stunned, .]</td>\n",
       "      <td>[&lt;sos&gt;, il, a, été, paralysé, ., &lt;eos&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39861</th>\n",
       "      <td>[you're, a, filthy, liar, !]</td>\n",
       "      <td>[&lt;sos&gt;, tu, es, un, sale, menteur, !, &lt;eos&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2102</th>\n",
       "      <td>[he, helps, us, .]</td>\n",
       "      <td>[&lt;sos&gt;, il, nous, aide, ., &lt;eos&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33290</th>\n",
       "      <td>[why, worry, about, tom, ?]</td>\n",
       "      <td>[&lt;sos&gt;, pourquoi, s'inquiéter, pour, tom, ?, &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17252</th>\n",
       "      <td>[we've, got, enough, .]</td>\n",
       "      <td>[&lt;sos&gt;, nous, en, avons, suffisamment, ., &lt;eos&gt;]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                eng  \\\n",
       "7755          [he, was, stunned, .]   \n",
       "39861  [you're, a, filthy, liar, !]   \n",
       "2102             [he, helps, us, .]   \n",
       "33290   [why, worry, about, tom, ?]   \n",
       "17252       [we've, got, enough, .]   \n",
       "\n",
       "                                                     fra  \n",
       "7755             [<sos>, il, a, été, paralysé, ., <eos>]  \n",
       "39861       [<sos>, tu, es, un, sale, menteur, !, <eos>]  \n",
       "2102                   [<sos>, il, nous, aide, ., <eos>]  \n",
       "33290  [<sos>, pourquoi, s'inquiéter, pour, tom, ?, <...  \n",
       "17252   [<sos>, nous, en, avons, suffisamment, ., <eos>]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰 추가\n",
    "lines.fra = lines.fra.apply(lambda x: ['<sos>']+re.findall(r\"[\\w']+|[.,!?;]\", x)+['<eos>'])\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[30, 1], [1298, 1], [1298, 1]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tokenizer = Tokenizer(char_level=False)   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "eng_tokenizer.fit_on_texts(lines.eng)               # 50000개의 행을 가진 eng의 각 행에 토큰화를 수행\n",
    "input_text = eng_tokenizer.texts_to_sequences(lines.eng)    # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "input_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 101, 15, 2], [1, 1238, 15, 2], [1, 1238, 3, 2]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fra_tokenizer = Tokenizer(char_level=False)   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "fra_tokenizer.fit_on_texts(lines.fra)                 # 50000개의 행을 가진 fra의 각 행에 토큰화를 수행\n",
    "target_text = fra_tokenizer.texts_to_sequences(lines.fra)     # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "target_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장의 크기 : 6026\n",
      "프랑스어 단어장의 크기 : 11534\n"
     ]
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 시퀀스의 최대 길이 9\n",
      "프랑스어 시퀀스의 최대 길이 18\n"
     ]
    }
   ],
   "source": [
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_to_index = eng_tokenizer.word_index\n",
    "index_to_src = eng_tokenizer.index_word\n",
    "\n",
    "tar_to_index = fra_tokenizer.word_index\n",
    "index_to_tar = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 50000\n",
      "영어 단어장의 크기 : 6026\n",
      "프랑스어 단어장의 크기 : 11534\n",
      "영어 시퀀스의 최대 길이 9\n",
      "프랑스어 시퀀스의 최대 길이 18\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = input_text\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [line[:-1] for line in target_text]\n",
    "# 시작 토큰 제거\n",
    "decoder_target = [line[1:] for line in target_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 101, 15], [1, 1238, 15], [1, 1238, 3]]\n",
      "[[101, 15, 2], [1238, 15, 2], [1238, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input[:3])\n",
    "print(decoder_target[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (50000, 9)\n",
      "프랑스어 입력데이터의 크기(shape) : (50000, 18)\n",
      "프랑스어 출력데이터의 크기(shape) : (50000, 18)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 학습데이터의 크기(shape) : (47000, 9)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (47000, 18)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (3000, 18)\n"
     ]
    }
   ],
   "source": [
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input_train))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input_train))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어 수준 - 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense,Masking\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "print('⏳')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(eng_vocab_size, 256)(encoder_inputs)\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb)\n",
    "encoder_lstm = LSTM(256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# 입력 텐서 생성.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "# hidden size가 256인 인코더의 LSTM 셀 생성\n",
    "dec_emb_layer = Embedding(fra_vocab_size, 256) \n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "\n",
    "decoder_lstm = LSTM(units = 256, return_sequences = True, return_state=True)\n",
    "# decoder_outputs는 모든 time step의 hidden state\n",
    "decoder_outputs, _, _= decoder_lstm(dec_masking, initial_state = encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    1542656     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 256)    2952704     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 256)    0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "masking_2 (Masking)             (None, None, 256)    0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 256), (None, 525312      masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 256),  525312      masking_2[0][0]                  \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 11534)  2964238     lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 8,510,222\n",
      "Trainable params: 8,510,222\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\",metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "368/368 [==============================] - 24s 66ms/step - loss: 1.7140 - acc: 0.7469 - val_loss: 1.7121 - val_acc: 0.7390\n",
      "Epoch 2/50\n",
      "368/368 [==============================] - 24s 64ms/step - loss: 1.2205 - acc: 0.8048 - val_loss: 1.4551 - val_acc: 0.7747\n",
      "Epoch 3/50\n",
      "368/368 [==============================] - 22s 60ms/step - loss: 1.0321 - acc: 0.8326 - val_loss: 1.3401 - val_acc: 0.7893\n",
      "Epoch 4/50\n",
      "368/368 [==============================] - 22s 61ms/step - loss: 0.9242 - acc: 0.8464 - val_loss: 1.2778 - val_acc: 0.7977\n",
      "Epoch 5/50\n",
      "368/368 [==============================] - 22s 61ms/step - loss: 0.8466 - acc: 0.8564 - val_loss: 1.2062 - val_acc: 0.8082\n",
      "Epoch 6/50\n",
      "368/368 [==============================] - 22s 61ms/step - loss: 0.7826 - acc: 0.8652 - val_loss: 1.1672 - val_acc: 0.8136\n",
      "Epoch 7/50\n",
      "368/368 [==============================] - 23s 63ms/step - loss: 0.7269 - acc: 0.8730 - val_loss: 1.1217 - val_acc: 0.8184\n",
      "Epoch 8/50\n",
      "368/368 [==============================] - 23s 62ms/step - loss: 0.6795 - acc: 0.8805 - val_loss: 1.0961 - val_acc: 0.8242\n",
      "Epoch 9/50\n",
      "368/368 [==============================] - 22s 60ms/step - loss: 0.6381 - acc: 0.8873 - val_loss: 1.0641 - val_acc: 0.8287\n",
      "Epoch 10/50\n",
      "368/368 [==============================] - 23s 62ms/step - loss: 0.6026 - acc: 0.8937 - val_loss: 1.0472 - val_acc: 0.8310\n",
      "Epoch 11/50\n",
      "368/368 [==============================] - 23s 62ms/step - loss: 0.5705 - acc: 0.8998 - val_loss: 1.0387 - val_acc: 0.8340\n",
      "Epoch 12/50\n",
      "368/368 [==============================] - 23s 62ms/step - loss: 0.5421 - acc: 0.9049 - val_loss: 1.0241 - val_acc: 0.8369\n",
      "Epoch 13/50\n",
      "368/368 [==============================] - 24s 65ms/step - loss: 0.5173 - acc: 0.9096 - val_loss: 1.0183 - val_acc: 0.8380\n",
      "Epoch 14/50\n",
      "368/368 [==============================] - 23s 62ms/step - loss: 0.4937 - acc: 0.9139 - val_loss: 1.0140 - val_acc: 0.8385\n",
      "Epoch 15/50\n",
      "368/368 [==============================] - 24s 64ms/step - loss: 0.4724 - acc: 0.9176 - val_loss: 1.0076 - val_acc: 0.8403\n",
      "Epoch 16/50\n",
      "368/368 [==============================] - 23s 63ms/step - loss: 0.4526 - acc: 0.9212 - val_loss: 1.0070 - val_acc: 0.8413\n",
      "Epoch 17/50\n",
      "368/368 [==============================] - 23s 62ms/step - loss: 0.4345 - acc: 0.9245 - val_loss: 1.0083 - val_acc: 0.8423\n",
      "Epoch 18/50\n",
      "368/368 [==============================] - 23s 62ms/step - loss: 0.4190 - acc: 0.9271 - val_loss: 1.0075 - val_acc: 0.8430\n",
      "Epoch 19/50\n",
      "368/368 [==============================] - 23s 62ms/step - loss: 0.4053 - acc: 0.9297 - val_loss: 1.0105 - val_acc: 0.8425\n",
      "Epoch 20/50\n",
      "368/368 [==============================] - 23s 62ms/step - loss: 0.3938 - acc: 0.9320 - val_loss: 1.0142 - val_acc: 0.8425\n",
      "Epoch 21/50\n",
      "368/368 [==============================] - 23s 62ms/step - loss: 0.3832 - acc: 0.9340 - val_loss: 1.0151 - val_acc: 0.8441\n",
      "Epoch 22/50\n",
      "368/368 [==============================] - 23s 62ms/step - loss: 0.3725 - acc: 0.9361 - val_loss: 1.0218 - val_acc: 0.8436\n",
      "Epoch 23/50\n",
      "368/368 [==============================] - 23s 62ms/step - loss: 0.3627 - acc: 0.9378 - val_loss: 1.0222 - val_acc: 0.8429\n",
      "Epoch 24/50\n",
      "368/368 [==============================] - 23s 63ms/step - loss: 0.3539 - acc: 0.9394 - val_loss: 1.0319 - val_acc: 0.8427\n",
      "Epoch 25/50\n",
      "368/368 [==============================] - 23s 62ms/step - loss: 0.3456 - acc: 0.9408 - val_loss: 1.0301 - val_acc: 0.8415\n",
      "Epoch 26/50\n",
      "368/368 [==============================] - 23s 63ms/step - loss: 0.3384 - acc: 0.9420 - val_loss: 1.0363 - val_acc: 0.8410\n",
      "Epoch 27/50\n",
      "368/368 [==============================] - 23s 64ms/step - loss: 0.3310 - acc: 0.9432 - val_loss: 1.0395 - val_acc: 0.8430\n",
      "Epoch 28/50\n",
      "368/368 [==============================] - 23s 62ms/step - loss: 0.3243 - acc: 0.9443 - val_loss: 1.0431 - val_acc: 0.8422\n",
      "Epoch 29/50\n",
      "368/368 [==============================] - 24s 64ms/step - loss: 0.3173 - acc: 0.9452 - val_loss: 1.0456 - val_acc: 0.8409\n",
      "Epoch 30/50\n",
      "368/368 [==============================] - 23s 62ms/step - loss: 0.3100 - acc: 0.9461 - val_loss: 1.0461 - val_acc: 0.8412\n",
      "Epoch 31/50\n",
      "368/368 [==============================] - 23s 63ms/step - loss: 0.3026 - acc: 0.9468 - val_loss: 1.0468 - val_acc: 0.8421\n",
      "Epoch 32/50\n",
      "368/368 [==============================] - 23s 63ms/step - loss: 0.2974 - acc: 0.9481 - val_loss: 1.0507 - val_acc: 0.8419\n",
      "Epoch 33/50\n",
      "368/368 [==============================] - 23s 62ms/step - loss: 0.2932 - acc: 0.9487 - val_loss: 1.0553 - val_acc: 0.8418\n",
      "Epoch 34/50\n",
      "368/368 [==============================] - 23s 62ms/step - loss: 0.2891 - acc: 0.9493 - val_loss: 1.0575 - val_acc: 0.8396\n",
      "Epoch 35/50\n",
      "368/368 [==============================] - 23s 62ms/step - loss: 0.2849 - acc: 0.9498 - val_loss: 1.0617 - val_acc: 0.8407\n",
      "Epoch 36/50\n",
      "368/368 [==============================] - 23s 62ms/step - loss: 0.2807 - acc: 0.9503 - val_loss: 1.0673 - val_acc: 0.8384\n",
      "Epoch 37/50\n",
      "368/368 [==============================] - 22s 61ms/step - loss: 0.2765 - acc: 0.9510 - val_loss: 1.0680 - val_acc: 0.8405\n",
      "Epoch 38/50\n",
      "368/368 [==============================] - 23s 62ms/step - loss: 0.2725 - acc: 0.9515 - val_loss: 1.0677 - val_acc: 0.8420\n",
      "Epoch 39/50\n",
      "368/368 [==============================] - 22s 60ms/step - loss: 0.2688 - acc: 0.9519 - val_loss: 1.0706 - val_acc: 0.8408\n",
      "Epoch 40/50\n",
      "368/368 [==============================] - 22s 61ms/step - loss: 0.2651 - acc: 0.9524 - val_loss: 1.0722 - val_acc: 0.8395\n",
      "Epoch 41/50\n",
      "368/368 [==============================] - 22s 61ms/step - loss: 0.2618 - acc: 0.9528 - val_loss: 1.0783 - val_acc: 0.8406\n",
      "Epoch 42/50\n",
      "368/368 [==============================] - 22s 60ms/step - loss: 0.2588 - acc: 0.9533 - val_loss: 1.0822 - val_acc: 0.8402\n",
      "Epoch 43/50\n",
      "368/368 [==============================] - 22s 61ms/step - loss: 0.2560 - acc: 0.9534 - val_loss: 1.0811 - val_acc: 0.8392\n",
      "Epoch 44/50\n",
      "368/368 [==============================] - 22s 60ms/step - loss: 0.2537 - acc: 0.9536 - val_loss: 1.0814 - val_acc: 0.8399\n",
      "Epoch 45/50\n",
      "368/368 [==============================] - 22s 60ms/step - loss: 0.2513 - acc: 0.9541 - val_loss: 1.0853 - val_acc: 0.8388\n",
      "Epoch 46/50\n",
      "368/368 [==============================] - 22s 61ms/step - loss: 0.2494 - acc: 0.9543 - val_loss: 1.0908 - val_acc: 0.8386\n",
      "Epoch 47/50\n",
      "368/368 [==============================] - 24s 65ms/step - loss: 0.2471 - acc: 0.9546 - val_loss: 1.0917 - val_acc: 0.8393\n",
      "Epoch 48/50\n",
      "368/368 [==============================] - 23s 62ms/step - loss: 0.2447 - acc: 0.9547 - val_loss: 1.0937 - val_acc: 0.8383\n",
      "Epoch 49/50\n",
      "368/368 [==============================] - 23s 62ms/step - loss: 0.2428 - acc: 0.9550 - val_loss: 1.0938 - val_acc: 0.8390\n",
      "Epoch 50/50\n",
      "368/368 [==============================] - 23s 63ms/step - loss: 0.2402 - acc: 0.9552 - val_loss: 1.0956 - val_acc: 0.8394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f631d790550>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2= dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_outputs2)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 정수 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_to_index['<sos>']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 단어로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "         # 현재 시점의 예측 단어를 예측 문장에 추가\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n",
    "        if (sampled_char == '<eos>' or\n",
    "           len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2src(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            temp = temp + index_to_src[i]+' '\n",
    "    return temp\n",
    "\n",
    "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2tar(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=tar_to_index['<sos>']) and i!=tar_to_index['<eos>']):\n",
    "            temp = temp + index_to_tar[i] + ' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 :  run ! \n",
      "번역문 : cours ! \n",
      "예측문 :  cours ! \n",
      "\n",
      "\n",
      "원문 :  i left . \n",
      "번역문 : je suis partie . \n",
      "예측문 :  je suis parti . \n",
      "\n",
      "\n",
      "원문 :  call us . \n",
      "번역문 : appelez nous ! \n",
      "예측문 :  appelle nous ! \n",
      "\n",
      "\n",
      "원문 :  how nice ! \n",
      "번역문 : comme c'est gentil ! \n",
      "예측문 :  comme c'est chouette ! \n",
      "\n",
      "\n",
      "원문 :  turn left . \n",
      "번역문 : tourne à gauche . \n",
      "예측문 :  tourne à gauche . \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3,50,100,300,1001]:\n",
    "    input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(\"원문 : \",seq2src(encoder_input_train[seq_index]))\n",
    "    print(\"번역문 :\",seq2tar(decoder_input_train[seq_index]))\n",
    "    print(\"예측문 :\",decoded_sentence[:-5])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 단어가 조금씩 안맞는 것 같지만 대부분은 번역이 잘 된다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
